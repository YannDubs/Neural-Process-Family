{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Criteria for LNPF\n",
    "\n",
    "In this notebook we will investigate the inpact of using the ML or the ELBO objective for training members of LNPF.\n",
    "We will also investigate the effect and/or need of using a lower bound for the standard deviation of the the latent variable and the posterior predictive.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import logging\n",
    "import os\n",
    "import warnings\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "\n",
    "os.chdir(\"../..\")\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "warnings.simplefilter(\"ignore\")\n",
    "logging.disable(logging.ERROR)\n",
    "\n",
    "N_THREADS = 8\n",
    "IS_FORCE_CPU = False  # Nota Bene : notebooks don't deallocate GPU memory\n",
    "\n",
    "if IS_FORCE_CPU:\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\"\n",
    "\n",
    "torch.set_num_threads(N_THREADS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialization\n",
    "\n",
    "Let's load the data, here we will only be working with Gaussian Processes from a single underlying kernel. For more details, see the {doc}`data <Datasets>` notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.ntbks_helpers import get_datasets_single_gp\n",
    "\n",
    "# DATASET\n",
    "gp_datasets, gp_test_datasets, gp_valid_datasets = get_datasets_single_gp()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from npf.utils.datasplit import CntxtTrgtGetter, GetRandomIndcs\n",
    "from utils.data import cntxt_trgt_collate\n",
    "\n",
    "# CONTEXT TARGET SPLIT\n",
    "get_cntxt_trgt_1d = cntxt_trgt_collate(\n",
    "    CntxtTrgtGetter(contexts_getter=GetRandomIndcs(a=0.0, b=50))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now make the model. We will make make one model for every member of LNPF. For each we will train them with both losses, with or without lower bound on the the std of the latent distribution, and with or without lower bound on the std of the predictive distribution.\n",
    "This is a total of 24 models, so we will do in a loop. Note that besides training, the same models are used as in other notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "from npf import LNP,ConvLNP, AttnLNP\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from npf.architectures import (\n",
    "    CNN,\n",
    "    MLP,\n",
    "    ResConvBlock,\n",
    "    SetConv,\n",
    "    discard_ith_arg,\n",
    "    merge_flat_input,\n",
    ")\n",
    "from utils.helpers import count_parameters\n",
    "\n",
    "R_DIM = 128\n",
    "KWARGS = dict(\n",
    "    XEncoder=partial(MLP, n_hidden_layers=1, hidden_size=R_DIM),\n",
    "    Decoder=merge_flat_input(  # MLP takes single input but we give x and R so merge them\n",
    "        partial(MLP, n_hidden_layers=4, hidden_size=R_DIM), is_sum_merge=True,\n",
    "    ),\n",
    "    r_dim=R_DIM,\n",
    ")\n",
    "\n",
    "\n",
    "def get_std_processing_kwargs(min_sigma_pred=0.01, min_lat=None):\n",
    "    \"\"\"Function returning kwarhs for processing std\"\"\"\n",
    "    kwargs = dict(\n",
    "        p_y_scale_transformer=lambda y_scale: min_sigma_pred\n",
    "        + (1 - min_sigma_pred) * F.softplus(y_scale)\n",
    "    )\n",
    "\n",
    "    if min_lat is not None:\n",
    "        kwargs[\"q_z_scale_transformer\"] = lambda y_scale: min_lat + (\n",
    "            1 - min_lat\n",
    "        ) * F.softplus(y_scale)\n",
    "\n",
    "    return kwargs\n",
    "\n",
    "\n",
    "def get_lnp(\n",
    "    is_mle=True, min_sigma_pred=0.01, min_lat=None,\n",
    "):\n",
    "\n",
    "    KWARGS = dict(\n",
    "        is_q_zCct=not is_mle,  # use MLE instead of ELBO\n",
    "        n_z_samples_train=32 if is_mle else 1,  # going to be more expensive\n",
    "        n_z_samples_test=32,\n",
    "        XEncoder=partial(MLP, n_hidden_layers=1, hidden_size=R_DIM),\n",
    "        Decoder=merge_flat_input(  # MLP takes single input but we give x and R so merge them\n",
    "            partial(MLP, n_hidden_layers=4, hidden_size=R_DIM), is_sum_merge=True,\n",
    "        ),\n",
    "        r_dim=R_DIM,\n",
    "        **get_std_processing_kwargs(min_sigma_pred=min_sigma_pred, min_lat=min_lat),\n",
    "    )\n",
    "\n",
    "    # 1D case\n",
    "    model_1d = partial(\n",
    "        LNP,\n",
    "        x_dim=1,\n",
    "        y_dim=1,\n",
    "        XYEncoder=merge_flat_input(  # MLP takes single input but we give x and y so merge them\n",
    "            partial(MLP, n_hidden_layers=2, hidden_size=R_DIM * 2), is_sum_merge=True,\n",
    "        ),\n",
    "        **KWARGS,\n",
    "    )\n",
    "    \n",
    "    return model_1d\n",
    "\n",
    "\n",
    "def get_attnlnp(\n",
    "    is_mle=True, min_sigma_pred=0.01, min_lat=None,\n",
    "):\n",
    "\n",
    "    KWARGS = dict(\n",
    "        is_q_zCct=not is_mle,  # use MLE instead of ELBO\n",
    "        n_z_samples_train=8 if is_mle else 1,  # going to be more expensive\n",
    "        n_z_samples_test=8,\n",
    "        r_dim=R_DIM,\n",
    "        attention=\"transformer\",\n",
    "        **get_std_processing_kwargs(min_sigma_pred=min_sigma_pred, min_lat=min_lat),\n",
    "    )\n",
    "\n",
    "    # 1D case\n",
    "    model_1d = partial(\n",
    "        AttnLNP,\n",
    "        x_dim=1,\n",
    "        y_dim=1,\n",
    "        XYEncoder=merge_flat_input(  # MLP takes single input but we give x and y so merge them\n",
    "            partial(MLP, n_hidden_layers=2, hidden_size=R_DIM), is_sum_merge=True,\n",
    "        ),\n",
    "        is_self_attn=False,\n",
    "        **KWARGS,\n",
    "    )\n",
    "\n",
    "    return model_1d\n",
    "\n",
    "\n",
    "def get_convlnp(\n",
    "    is_mle=True, min_sigma_pred=0.01, min_lat=None, z_dim=None\n",
    "):\n",
    "    KWARGS = dict(\n",
    "        is_q_zCct=not is_mle,  # use MLE instead of ELBO\n",
    "        n_z_samples_train=16 if is_mle else 1, # going to be more expensive\n",
    "        n_z_samples_test=16, #! DEV\n",
    "        r_dim=R_DIM,\n",
    "        Decoder=discard_ith_arg(\n",
    "            torch.nn.Linear, i=0\n",
    "        ),  # use small decoder because already went through CNN\n",
    "        z_dim=16, #! NPVI requires smaller number of latent channels due to the KL\n",
    "        **get_std_processing_kwargs(min_sigma_pred=min_sigma_pred, min_lat=min_lat),\n",
    "    )\n",
    "\n",
    "    CNN_KWARGS = dict(\n",
    "        ConvBlock=ResConvBlock,\n",
    "        is_chan_last=True,  # all computations are done with channel last in our code\n",
    "        n_conv_layers=2,\n",
    "        n_blocks=4,\n",
    "    )\n",
    "\n",
    "    # 1D case\n",
    "    model_1d = partial(\n",
    "        ConvLNP,\n",
    "        x_dim=1,\n",
    "        y_dim=1,\n",
    "        CNN=partial(\n",
    "            CNN,\n",
    "            Conv=torch.nn.Conv1d,\n",
    "            Normalization=torch.nn.BatchNorm1d,\n",
    "            kernel_size=19,\n",
    "            **CNN_KWARGS,\n",
    "        ),\n",
    "        density_induced=64,  # size of discretization\n",
    "        is_global=True, \n",
    "        **KWARGS,\n",
    "    )\n",
    "\n",
    "    return model_1d\n",
    "\n",
    "\n",
    "lnpf_getters = dict(LNP=get_lnp, AttnLNP=get_attnlnp, ConvLNP=get_convlnp)\n",
    "\n",
    "\n",
    "def get_name(lnpf, is_elbo, is_lat_LB, is_sigma_LB):\n",
    "    return f\"{lnpf}_ELBO{str(is_elbo)}_LatLB{str(is_lat_LB)}_SigLB{str(is_sigma_LB)}\"\n",
    "\n",
    "models = {\n",
    "    get_name(lnpf, is_elbo, is_lat_LB, is_sigma_LB): lnpf_getters[\n",
    "        lnpf\n",
    "    ](\n",
    "        is_mle=not is_elbo,\n",
    "        min_sigma_pred=0.01 if is_sigma_LB else 1e-4,\n",
    "        min_lat=None if is_lat_LB else 1e-4,\n",
    "    )\n",
    "    for lnpf in [\"ConvLNP\"] \n",
    "    for is_elbo in [True, False]\n",
    "    for is_sigma_LB in [True, False]\n",
    "    for is_lat_LB in [True, False]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(models)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training\n",
    "\n",
    "The main function for training is `train_models` which trains a dictionary of models on a dictionary of datasets and returns all the trained models.\n",
    "See its docstring for possible parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-output"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Training RBF_Kernel/ConvLNP_ELBOTrue_LatLBTrue_SigLBTrue/run_0 ---\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ebea1f549b864edb88406de566b01e3b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1563.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import skorch\n",
    "from npf import NLLLossLNPF, ELBOLossLNPF\n",
    "from utils.ntbks_helpers import add_y_dim\n",
    "from utils.train import train_models\n",
    "\n",
    "KWARGS = dict(\n",
    "    is_retrain=True, # whether to load precomputed model or retrain\n",
    "    chckpnt_dirname=\"results/pretrained/\",\n",
    "    device=None,  # use GPU if available\n",
    "    batch_size=32,\n",
    "    lr=1e-3,\n",
    "    decay_lr=10,  # decrease learning rate by 10 during training\n",
    "    seed=123,\n",
    "    test_datasets=gp_test_datasets,\n",
    "    train_split=None,  # No need for validation as the training data is generated on the fly\n",
    "    iterator_train__collate_fn=get_cntxt_trgt_1d,\n",
    "    iterator_valid__collate_fn=get_cntxt_trgt_1d,\n",
    "    max_epochs=100,\n",
    ")\n",
    "\n",
    "# NPVI\n",
    "trainers_1d_NPVI = train_models(\n",
    "    gp_datasets,\n",
    "    {k:v for k,v in models.items() if \"ELBOTrue\" in k},\n",
    "    criterion=ELBOLossLNPF,  # NPVI\n",
    "    **KWARGS\n",
    ")\n",
    "\n",
    "#NPML\n",
    "trainers_1d_NPML = train_models(\n",
    "    gp_datasets,\n",
    "    {k:v for k,v in models.items() if \"ELBOTrue\" not in k},\n",
    "    criterion=NLLLossLNPF,  # NPML\n",
    "    **KWARGS\n",
    ")\n",
    "\n",
    "trainers_1d = {**trainers_1d_NPML, **trainers_1d_NPVI}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plots\n",
    "\n",
    "Let's visualize how well the model performs in different settings.\n",
    "\n",
    "#### GPs Dataset\n",
    "\n",
    "Let's define a plotting function that we will use in this section. We'll reuse the same function defined in {doc}`CNP notebook <CNP>`, but will use `n_samples = 20` to plot multiple posterior predictives conditioned on different latent samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.ntbks_helpers import PRETTY_RENAMER, plot_multi_posterior_samples_1d\n",
    "from utils.visualize import giffify\n",
    "\n",
    "\n",
    "def multi_posterior_gp_gif(filename, trainers, datasets, seed=123, **kwargs):\n",
    "    giffify(\n",
    "        save_filename=f\"jupyter/gifs/{filename}.gif\",\n",
    "        gen_single_fig=plot_multi_posterior_samples_1d,  # core plotting\n",
    "        sweep_parameter=\"n_cntxt\",  # param over which to sweep\n",
    "        sweep_values=[0, 2, 5, 7, 10, 15, 20, 30, 50, 100],\n",
    "        fps=0.5,  # gif speed\n",
    "        # PLOTTING KWARGS\n",
    "        trainers=trainers,\n",
    "        datasets=datasets,\n",
    "        is_plot_generator=True,  # plot underlying GP\n",
    "        is_plot_real=False,  # don't plot sampled / underlying function\n",
    "        is_plot_std=True,  # plot the predictive std\n",
    "        is_fill_generator_std=False,  # do not fill predictive of GP\n",
    "        pretty_renamer=PRETTY_RENAMER,  # pretiffy names of modulte + data\n",
    "        # Fix formatting for coherent GIF\n",
    "        plot_config_kwargs=dict(\n",
    "            set_kwargs=dict(ylim=[-3, 3]), rc={\"legend.loc\": \"upper right\"}\n",
    "        ),\n",
    "        seed=seed,\n",
    "        **kwargs,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us visualize the CNP when it is trained on samples from a single GP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Number of rows must be a positive integer, not 0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-2152364a43b7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mis_sigma_LB\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mis_lat_LB\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m             multi_posterior_gp_gif(\n\u001b[0m\u001b[1;32m      9\u001b[0m                 \u001b[0;34mf\"test_singlegp_{lnpf}_LatLB{str(is_lat_LB)}_SigLB{str(is_sigma_LB)}\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m                 \u001b[0mtrainers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfilter_npf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainers_1d\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlnpf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_elbo\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_lat_LB\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mis_lat_LB\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_sigma_LB\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mis_sigma_LB\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-12-07b8e46c7ab1>\u001b[0m in \u001b[0;36mmulti_posterior_gp_gif\u001b[0;34m(filename, trainers, datasets, seed, **kwargs)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmulti_posterior_gp_gif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m123\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     giffify(\n\u001b[0m\u001b[1;32m      7\u001b[0m         \u001b[0msave_filename\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mf\"jupyter/gifs/{filename}.gif\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mgen_single_fig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mplot_multi_posterior_samples_1d\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# core plotting\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Neural-Process-Family/utils/visualize/helpers.py\u001b[0m in \u001b[0;36mgiffify\u001b[0;34m(save_filename, gen_single_fig, sweep_parameter, sweep_values, fps, quality, is_transparent, **kwargs)\u001b[0m\n\u001b[1;32m    132\u001b[0m     \u001b[0mfigs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msweep_values\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 134\u001b[0;31m         \u001b[0mfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgen_single_fig\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0msweep_parameter\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    135\u001b[0m         \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m         \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfig2img\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_transparent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mis_transparent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Neural-Process-Family/utils/ntbks_helpers.py\u001b[0m in \u001b[0;36mplot_multi_posterior_samples_1d\u001b[0;34m(trainers, datasets, n_cntxt, trainers_compare, plot_config_kwargs, title, left_extrap, right_extrap, pretty_renamer, is_plot_generator, imgsize, **kwargs)\u001b[0m\n\u001b[1;32m    399\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    400\u001b[0m         \u001b[0mn_col\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtrainers_compare\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 401\u001b[0;31m         fig, axes = plt.subplots(\n\u001b[0m\u001b[1;32m    402\u001b[0m             \u001b[0mn_trainers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    403\u001b[0m             \u001b[0mn_col\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/matplotlib/cbook/deprecation.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    449\u001b[0m                 \u001b[0;34m\"parameter will become keyword-only %(removal)s.\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    450\u001b[0m                 name=name, obj_type=f\"parameter of {func.__name__}()\")\n\u001b[0;32m--> 451\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    452\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    453\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/matplotlib/pyplot.py\u001b[0m in \u001b[0;36msubplots\u001b[0;34m(nrows, ncols, sharex, sharey, squeeze, subplot_kw, gridspec_kw, **fig_kw)\u001b[0m\n\u001b[1;32m   1270\u001b[0m     \"\"\"\n\u001b[1;32m   1271\u001b[0m     \u001b[0mfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mfig_kw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1272\u001b[0;31m     axs = fig.subplots(nrows=nrows, ncols=ncols, sharex=sharex, sharey=sharey,\n\u001b[0m\u001b[1;32m   1273\u001b[0m                        \u001b[0msqueeze\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubplot_kw\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msubplot_kw\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1274\u001b[0m                        gridspec_kw=gridspec_kw)\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/matplotlib/cbook/deprecation.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    449\u001b[0m                 \u001b[0;34m\"parameter will become keyword-only %(removal)s.\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    450\u001b[0m                 name=name, obj_type=f\"parameter of {func.__name__}()\")\n\u001b[0;32m--> 451\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    452\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    453\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/matplotlib/figure.py\u001b[0m in \u001b[0;36msubplots\u001b[0;34m(self, nrows, ncols, sharex, sharey, squeeze, subplot_kw, gridspec_kw)\u001b[0m\n\u001b[1;32m   1520\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mgridspec_kw\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1521\u001b[0m             \u001b[0mgridspec_kw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1522\u001b[0;31m         return (self.add_gridspec(nrows, ncols, figure=self, **gridspec_kw)\n\u001b[0m\u001b[1;32m   1523\u001b[0m                 .subplots(sharex=sharex, sharey=sharey, squeeze=squeeze,\n\u001b[1;32m   1524\u001b[0m                           subplot_kw=subplot_kw))\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/matplotlib/figure.py\u001b[0m in \u001b[0;36madd_gridspec\u001b[0;34m(self, nrows, ncols, **kwargs)\u001b[0m\n\u001b[1;32m   2802\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2803\u001b[0m         \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'figure'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pop in case user has added this...\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2804\u001b[0;31m         \u001b[0mgs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGridSpec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mncols\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mncols\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfigure\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2805\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gridspecs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2806\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mgs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/matplotlib/gridspec.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, nrows, ncols, figure, left, bottom, right, top, wspace, hspace, width_ratios, height_ratios)\u001b[0m\n\u001b[1;32m    418\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfigure\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    419\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 420\u001b[0;31m         GridSpecBase.__init__(self, nrows, ncols,\n\u001b[0m\u001b[1;32m    421\u001b[0m                               \u001b[0mwidth_ratios\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwidth_ratios\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    422\u001b[0m                               height_ratios=height_ratios)\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/matplotlib/gridspec.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, nrows, ncols, height_ratios, width_ratios)\u001b[0m\n\u001b[1;32m     46\u001b[0m         \"\"\"\n\u001b[1;32m     47\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mIntegral\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mnrows\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m             raise ValueError(\n\u001b[0m\u001b[1;32m     49\u001b[0m                 f\"Number of rows must be a positive integer, not {nrows}\")\n\u001b[1;32m     50\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mncols\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mIntegral\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mncols\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Number of rows must be a positive integer, not 0"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 864x0 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def filter_npf(d, lnpf, is_elbo, is_lat_LB, is_sigma_LB):\n",
    "    \"\"\"Select only data form single GP.\"\"\"\n",
    "    return {k: v for k, v in d.items() if \"/\"+get_name(lnpf, is_elbo, is_lat_LB, is_sigma_LB) in k}\n",
    "\n",
    "for lnpf in [\"LNP\", \"AttnLNP\", \"ConvLNP\"]:\n",
    "    for is_sigma_LB in [True, False]:\n",
    "        for is_lat_LB in [True, False]:\n",
    "            multi_posterior_gp_gif(\n",
    "                f\"singlegp_{lnpf}_LatLB{str(is_lat_LB)}_SigLB{str(is_sigma_LB)}\",\n",
    "                trainers=filter_npf(trainers_1d, lnpf, is_elbo=False, is_lat_LB=is_lat_LB, is_sigma_LB=is_sigma_LB),\n",
    "                trainers_compare=filter_npf(trainers_1d, lnpf, is_elbo=True, is_lat_LB=is_lat_LB, is_sigma_LB=is_sigma_LB),\n",
    "                datasets=gp_test_datasets,\n",
    "                n_samples=20,  # 20 samples from the latent\n",
    "                title=\"{model_name} | {data_name} | C={n_cntxt}\",\n",
    "                imgsize=(6, 3),\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now visualize all of these plots."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LNP\n",
    "\n",
    "#### No Lower bounds\n",
    "\n",
    "```{figure} ../gifs/singlegp_LNP_LatLBFalse_SigLBFalse.gif\n",
    "---\n",
    "width: 60em\n",
    "name: singlegp_LNP_LatLBFalse_SigLBFalse\n",
    "---\n",
    "```\n",
    "\n",
    "#### Lower bounded std of latent\n",
    "\n",
    "```{figure} ../gifs/singlegp_LNP_LatLBTrue_SigLBFalse.gif\n",
    "---\n",
    "width: 60em\n",
    "name: singlegp_LNP_LatLBTrue_SigLBFalse\n",
    "---\n",
    "```\n",
    "\n",
    "#### Lower bounded std of predictive\n",
    "\n",
    "```{figure} ../gifs/singlegp_LNP_LatLBFalse_SigLBTrue.gif\n",
    "---\n",
    "width: 60em\n",
    "name: singlegp_LNP_LatLBFalse_SigLBTrue\n",
    "---\n",
    "```\n",
    "\n",
    "#### Both Lower Bounds\n",
    "\n",
    "\n",
    "```{figure} ../gifs/singlegp_LNP_LatLBTrue_SigLBTrue.gif\n",
    "---\n",
    "width: 60em\n",
    "name: singlegp_LNP_LatLBTrue_SigLBTrue\n",
    "---\n",
    "```\n",
    "\n",
    "### AttnLNP\n",
    "\n",
    "#### No Lower bounds\n",
    "\n",
    "```{figure} ../gifs/singlegp_AttnLNP_LatLBFalse_SigLBFalse.gif\n",
    "---\n",
    "width: 60em\n",
    "name: singlegp_AttnLNP_LatLBFalse_SigLBFalse\n",
    "---\n",
    "```\n",
    "\n",
    "#### Lower bounded std of latent\n",
    "\n",
    "```{figure} ../gifs/singlegp_AttnLNP_LatLBTrue_SigLBFalse.gif\n",
    "---\n",
    "width: 60em\n",
    "name: singlegp_AttnLNP_LatLBTrue_SigLBFalse\n",
    "---\n",
    "```\n",
    "\n",
    "#### Lower bounded std of predictive\n",
    "\n",
    "```{figure} ../gifs/singlegp_AttnLNP_LatLBFalse_SigLBTrue.gif\n",
    "---\n",
    "width: 60em\n",
    "name: singlegp_AttnLNP_LatLBFalse_SigLBTrue\n",
    "---\n",
    "```\n",
    "\n",
    "#### Both Lower Bounds\n",
    "\n",
    "\n",
    "```{figure} ../gifs/singlegp_AttnLNP_LatLBTrue_SigLBTrue.gif\n",
    "---\n",
    "width: 60em\n",
    "name: singlegp_AttnLNP_LatLBTrue_SigLBTrue\n",
    "---\n",
    "```\n",
    "\n",
    "### ConvLNP\n",
    "\n",
    "\n",
    "```{warning} \n",
    "\n",
    "For NPVI to train with ConvLNP we had to remove the global representation and decrease the number of channels to `z_dim=16`.\n",
    "The models for NPVI and NPML are thus slighlty different.\n",
    "```\n",
    "\n",
    "\n",
    "#### No Lower bounds\n",
    "\n",
    "```{figure} ../gifs/singlegp_ConvLNP_LatLBFalse_SigLBFalse.gif\n",
    "---\n",
    "width: 60em\n",
    "name: singlegp_ConvLNP_LatLBFalse_SigLBFalse\n",
    "---\n",
    "```\n",
    "\n",
    "#### Lower bounded std of latent\n",
    "\n",
    "```{figure} ../gifs/singlegp_ConvLNP_LatLBTrue_SigLBFalse.gif\n",
    "---\n",
    "width: 60em\n",
    "name: singlegp_ConvLNP_LatLBTrue_SigLBFalse\n",
    "---\n",
    "```\n",
    "\n",
    "#### Lower bounded std of predictive\n",
    "\n",
    "```{figure} ../gifs/singlegp_ConvLNP_LatLBFalse_SigLBTrue.gif\n",
    "---\n",
    "width: 60em\n",
    "name: singlegp_ConvLNP_LatLBFalse_SigLBTrue\n",
    "---\n",
    "```\n",
    "\n",
    "#### Both Lower Bounds\n",
    "\n",
    "\n",
    "```{figure} ../gifs/singlegp_ConvLNP_LatLBTrue_SigLBTrue.gif\n",
    "---\n",
    "width: 60em\n",
    "name: singlegp_ConvLNP_LatLBTrue_SigLBTrue\n",
    "---\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "###### ADDITIONAL 1D PLOTS ######\n",
    "\n",
    "#TO Chose"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
