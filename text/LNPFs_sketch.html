

<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Latent NPFs &#8212; Neural Process Family</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.11.2/css/all.min.css" integrity="sha384-KA6wR/X5RY4zFAHpv/CnoG2UW1uogYfdnP67Uv7eULvTveboZJg0qUpmJZb5VqzN" crossorigin="anonymous">
    <link href="../_static/css/index.css" rel="stylesheet">
    <link rel="stylesheet" href="../_static/sphinx-book-theme.css" type="text/css" />
    <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-dropdown.css" />
    <script id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
    <script src="../_static/sphinx-book-theme.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/language_data.js"></script>
    <script src="../_static/togglebutton.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script src="../_static/sphinx-book-theme.js"></script>
    <script >var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <script type="text/x-mathjax-config">MathJax.Hub.Config({"tex2jax": {"inlineMath": [["\\(", "\\)"]], "displayMath": [["\\[", "\\]"]], "processRefs": false, "processEnvironments": false}})</script>
    <script async="async" src="https://unpkg.com/thebelab@latest/lib/index.js"></script>
    <script >
        const thebe_selector = ".thebe"
        const thebe_selector_input = "pre"
        const thebe_selector_output = ".output"
    </script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <link rel="shortcut icon" href="../_static/favicon.ico"/>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />

    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="docsearch:language" content="en">



  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
<a class="navbar-brand text-wrap" href="../index.html">
  
  <img src="../_static/logo.gif" class="logo" alt="logo">
  
  
  <h1 class="site-logo" id="site-title">Neural Process Family</h1>
  
</a>
</div>

<form class="bd-search d-flex align-items-center" action="../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form>

<nav class="bd-links" id="bd-docs-nav" aria-label="Main navigation">
  <ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="Intro.html">
   The Neural Process Family
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Sub-Families
 </span>
</p>
<ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="CNPF.html">
   Conditional NPF
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="LNPF.html">
   Latent NPF
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Reproducibility
 </span>
</p>
<ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="../reproducibility/Datasets.html">
   Datasets
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../reproducibility/CNP.html">
   CNP
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../reproducibility/AttnCNP.html">
   AttnCNP
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../reproducibility/ConvCNP.html">
   ConvCNP
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../reproducibility/LNP.html">
   LNP
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../reproducibility/AttnLNP.html">
   AttnLNP
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../reproducibility/ConvLNP.html">
   ConvLNP
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../reproducibility/Losses.html">
   LNPF Losses
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Reference
 </span>
</p>
<ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="../zbibliography.html">
   Bibliography
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference external" href="https://github.com/YannDubs/Neural-Process-Family">
   GitHub Repo
   <i class="fas fa-external-link-alt">
   </i>
  </a>
 </li>
</ul>

</nav>

 <!-- To handle the deprecated key -->

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="row topbar fixed-top container-xl">
    <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show">
    </div>
    <div class="col pl-2 topbar-main">
        
        <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
            data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
            aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
            title="Toggle navigation" data-toggle="tooltip" data-placement="left">
            <i class="fas fa-bars"></i>
            <i class="fas fa-arrow-left"></i>
            <i class="fas fa-arrow-up"></i>
        </button>
        
        <div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    
    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="../_sources/text/LNPFs_sketch.md"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.md</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
            onClick="window.print()" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
    
</div>
        <!-- Source interaction buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Connect with source repository"><i class="fab fa-github"></i></button>
    <div class="dropdown-buttons sourcebuttons">
        <a class="repository-button"
            href="https://github.com/YannDubs/Neural-Process-Family"><button type="button" class="btn btn-secondary topbarbtn"
                data-toggle="tooltip" data-placement="left" title="Source repository"><i
                    class="fab fa-github"></i>repository</button></a>
        
        
    </div>
</div>


        <!-- Full screen (wrap in <a> to have style consistency -->
        <a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
                data-placement="bottom" onclick="toggleFullScreen()" title="Fullscreen mode"><i
                    class="fas fa-expand"></i></button></a>

        <!-- Launch buttons -->

    </div>

    <!-- Table of contents -->
    <div class="d-none d-md-block col-md-2 bd-toc show">
        <div class="tocsection onthispage pt-5 pb-3">
            <i class="fas fa-list"></i> Contents
        </div>
        <nav id="bd-toc-nav">
            <ul class="nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#overview">
   Overview
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#training-lnpf-members">
   Training LNPF members
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#neural-process-maximum-likelihood-npml">
     Neural Process Maximum Likelihood (NPML)
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#summary">
   Summary
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#neural-process-variational-inference-npvi">
     Neural Process Variational Inference (NPVI)
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#latent-neural-process-lnp">
   Latent Neural Process (LNP)
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#attentive-latent-neural-process-attnlnp">
   Attentive Latent Neural Process (AttnLNP)
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#convolutional-latent-neural-process-convlnp">
   Convolutional Latent Neural Process (ConvLNP)
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#issues-and-discussion">
   Issues and Discussion
  </a>
 </li>
</ul>

        </nav>
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
        
              <div>
                
  <div class="section" id="latent-npfs">
<h1>Latent NPFs<a class="headerlink" href="#latent-npfs" title="Permalink to this headline">¶</a></h1>
<div class="section" id="overview">
<h2>Overview<a class="headerlink" href="#overview" title="Permalink to this headline">¶</a></h2>
<p>We concluded the previous section by noting two important drawbacks of the CNPF:</p>
<ul class="simple">
<li><p>The predictive distribution is factorised across target points, and thus can neither account for correlations in the predictive nor (as a result) produce “coherent” samples from the predictive distribution.</p></li>
<li><p>The predictive distribution requires specification of a particular parametric form (e.g. Gaussian).</p></li>
</ul>
<p>In this section we discuss an alternative parametrisation of <span class="math notranslate nohighlight">\(p_\theta( \mathbf{y}_\mathcal{T} | \mathbf{x}_\mathcal{T}; \mathcal{C})\)</span> that still satisfies our desiredata for NPs, and addresses both of these issues.
The main idea is to introduce a latent variable <span class="math notranslate nohighlight">\(\mathbf{z}\)</span> into the definition of the predictive distribution.
This leads us to the second major branch of the NPF, which we refer to as the Latent Neural Process Sub-family, or LNPF for short.
A graphical representation of the LNPF  is given in <code class="xref std std-numref docutils literal notranslate"><span class="pre">graph_model_LNPs_text</span></code>.</p>
<div class="figure align-default" id="graph-model-lnps-text">
<a class="reference internal image-reference" href="../_images/graph_model_LNPF.svg"><img alt="graphical model LNP" src="../_images/graph_model_LNPF.svg" width="300em" /></a>
<p class="caption"><span class="caption-text">Probabilistic graphical model for LNPs.</span><a class="headerlink" href="#graph-model-lnps-text" title="Permalink to this image">¶</a></p>
</div>
<p>To specify this family of models, we must define a few components:</p>
<ul class="simple">
<li><p>An encoder: <span class="math notranslate nohighlight">\(p_{\theta} \left( \mathbf{z} | \mathcal{C} \right)\)</span>, which provides a <em>distribution</em> over the latent variable <span class="math notranslate nohighlight">\(\mathbf{z}\)</span> having observed the context set <span class="math notranslate nohighlight">\(\mathcal{C}\)</span>. As with other NPF, the encoder needs to be permutation invariant to correctly treat <span class="math notranslate nohighlight">\(\mathcal{C}\)</span> as a set. A typical example is to first have a deterministic representation <span class="math notranslate nohighlight">\(R\)</span> and then use it to output the mean and (log) standard deviations of a Gaussian distribution over <span class="math notranslate nohighlight">\(\mathbf{z}\)</span>.</p></li>
<li><p>A decoder: <span class="math notranslate nohighlight">\(p_{\theta} \left( \mathbf{y}_{\mathcal{T}} | \mathbf{x}_{\mathcal{T}}, \mathbf{z} \right)\)</span>, which provides predictive distributions conditioned on <span class="math notranslate nohighlight">\(\mathbf{z}\)</span> and a target location <span class="math notranslate nohighlight">\(\mathbf{x}_{\mathcal{T}}\)</span>.
The decoder will usually be the same as the CNPF, but using a sample of the latent representation <span class="math notranslate nohighlight">\(\mathbf{z}\)</span> and marginalizing them, rather than a deterministic representation.</p></li>
</ul>
<p>Putting altogether:</p>
<div class="math notranslate nohighlight" id="equation-latent-likelihood">
<span class="eqno">()<a class="headerlink" href="#equation-latent-likelihood" title="Permalink to this equation">¶</a></span>\[\begin{split}\begin{align}
p_{\theta}(\mathbf{y}_\mathcal{T} | \mathbf{x}_\mathcal{T}; \mathcal{C})
&amp;= \int p_{\theta} \left( \mathbf{z} | \mathcal{C} \right) p_{\theta} \left( \mathbf{y}_{\mathcal{T}} | \mathbf{x}_{\mathcal{T}}, \mathbf{z} \right) \mathrm{d}\mathbf{z} &amp; \text{Marginalisation}  \\
&amp;= \int p_{\theta} \left( \mathbf{z} | \mathcal{C} \right) \prod_{t=1}^{T} p_{\theta}(y^{(t)} |  x^{(t)}, \mathbf{z}) \, \mathrm{d}\mathbf{z}  &amp; \text{Factorisation}\\
&amp;= \int p_{\theta} \left( \mathbf{z} | \mathcal{C} \right)  \prod_{t=1}^{T} \mathcal{N} \left( y^{(t)};  \mu^{(t)}, \sigma^{2(t)} \right) \mathrm{d}\mathbf{z} &amp; \text{Gaussianity}
\end{align}\end{split}\]</div>
<div class="hint dropdown admonition">
<p class="admonition-title">Advanced<span class="math notranslate nohighlight">\(\qquad\)</span>Latent variable <span class="math notranslate nohighlight">\(\implies\)</span> consistency</p>
<p>We show that like the CNPF, members of the LNPF also specify consistent stochastic processes conditioned on a fixed context set <span class="math notranslate nohighlight">\(\mathcal{C}\)</span>.</p>
<p>(Consistency under permutation) Let <span class="math notranslate nohighlight">\(\mathbf{x}_{\mathcal{T}} = \{ x^{(t)} \}_{t=1}^T\)</span> be the target inputs and <span class="math notranslate nohighlight">\(\pi\)</span> be any permutation of <span class="math notranslate nohighlight">\(\{1, ..., T\}\)</span>. Then the predictive density is:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align}
    p_\theta(y^{(1)}, ..., y^{(T)} | x^{(1)}, ..., x^{(T)}; \mathcal{C}) &amp;= \int p_{\theta} ( \mathbf{z}) \prod_{t=1}^{T} p_{\theta}(y^{(t)} |  x^{(t)}, \mathbf{z}) \, \mathrm{d}\mathbf{z} \\
    &amp;= \int p_{\theta} ( \mathbf{z}) \prod_{t=1}^{T} p_{\theta}(y^{(\pi(t))} |  y^{(\pi(t))}, \mathbf{z}) \, \mathrm{d}\mathbf{z} \\
    &amp;= p_\theta(y^{(\pi(1))}, ..., y^{(\pi(T))} | x^{(\pi(1))}, ..., x^{(\pi(T))}; \mathcal{C})
\end{align}
\end{split}\]</div>
<p>since multiplication is commutative.</p>
<p>(Consistency under marginalisation)  Consider two target inputs, <span class="math notranslate nohighlight">\(x^{(1)}, x^{(2)}\)</span>. Then by marginalising out the second target output, we get:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align}
    \int p_\theta(y^{(1)}, y^{(2)}| x^{(1)}, x^{(2)}; \mathcal{C}) \, \mathrm{d}y^{(2)} &amp;= \int \int p_\theta(y^{(1)}| x^{(1)}, \mathbf{z})p_\theta(y^{(2)}| x^{(2)}, \mathbf{z}) p_\theta(\mathbf{z} | \mathcal{C}) \, \mathrm{d}\mathbf{z} \mathrm{d}y^{(2)} \\
    &amp;= \int  p_\theta(y^{(1)}| x^{(1)}, \mathbf{z}) p_\theta(\mathbf{z}| \mathcal{C}) \int p_\theta(y^{(2)}| x^{(2)}, \mathbf{z})  \, \mathrm{d}y^{(2)} \mathrm{d}\mathbf{z}  \\
    &amp;= \int  p_\theta(y^{(1)}| x^{(1)}, \mathbf{z}) p_\theta(\mathbf{z}| \mathcal{C}) \mathrm{d}\mathbf{z}  \\
    &amp;= p_\theta(y^{(1)}| x^{(1)}; \mathcal{C})
\end{align}
\end{split}\]</div>
<p>which shows that the predictive distribution obtained by querying an LNPF member at <span class="math notranslate nohighlight">\(x^{(1)}\)</span> is the same as that obtained by querying it at <span class="math notranslate nohighlight">\(x^{(1)}, x^{(2)}\)</span> and then marginalising out the second target point. Of course, the same idea works with collections of any size, and marginalising any subset of the variables.</p>
</div>
<p>Now, you might be worried that we have still made both the factorisation and Gaussian assumptions!
However, while the decoder likelihood <span class="math notranslate nohighlight">\(p_{\theta} \left( \mathbf{y}_{\mathcal{T}} | \mathbf{x}_{\mathcal{T}}, \mathbf{z} \right)\)</span> is still factorised, the predictive distribution we are actually interested in, <span class="math notranslate nohighlight">\(p_\theta( \mathbf{y}_\mathcal{T} | \mathbf{x}_\mathcal{T}; \mathcal{C})\)</span>, is no longer due to the marginalisation of <span class="math notranslate nohighlight">\(\mathbf{z}\)</span>, thus addressing the first problem we associated with the CNPF.
Moreover, the predictive distribution is no longer Gaussian either.
In fact, since the predictive now has the form of an <em>infinite mixture of Gaussians</em>, potentially <em>any</em> predictive density can be represented (i.e. learned) by this form.
This is great news, as it (conceptually) relieves us of the burden of choosing / designing a bespoke likelihood function when deploying the NPF for a new application!</p>
<p>However, there is an important drawback. The key difficulty with the LNPF is that the likelihood we defined in Eq.<a class="reference internal" href="#equation-latent-likelihood">()</a> is no longer <em>analytically tractable</em>.
We now discuss how to train members of the LNPF in general.
After discussing several training procedures, we’ll introduce extensions of each of the CNPF members discussed in the previous chapter to their corresponding member of the LNPF.</p>
</div>
<div class="section" id="training-lnpf-members">
<h2>Training LNPF members<a class="headerlink" href="#training-lnpf-members" title="Permalink to this headline">¶</a></h2>
<p>Ideally, we would like to directly maximize the likelihood defined in Eq.<a class="reference internal" href="#equation-latent-likelihood">()</a> to optimise the parameters of the model.
However, the integral over <span class="math notranslate nohighlight">\(\mathbf{z}\)</span> renders this quantity intractable, so we must consider alternatives.
In fact, this story is not new, and the same issues arise when considering other latent variable models, such as variational auto-encoders (VAEs).</p>
<p>The question of how best to train LNPF members is still open, and there is ongoing research in this area.
In this section, we will cover two methods for training LNPF models, but each have their flaws, and deciding which is the preferred training method must often be answered empirically.</p>
<div class="caution dropdown admonition">
<p class="admonition-title">Disclaimer<span class="math notranslate nohighlight">\(\qquad\)</span> Chronology</p>
<p>In this tutorial, the order in which we introduce the objective functions does not follow the chronology in which they were originally introduced in the literature.
We begin by describing an approximate maximum-likelihood procedure, which was recently introduced by <a class="bibtex reference internal" href="../zbibliography.html#foong2020convnp" id="id1">[FBG+20]</a> due to its simplicity, and its relation to the CNPF training procedure.
Following this, we introduce a variational inference-inspired approach, which was proposed earlier by <a class="bibtex reference internal" href="../zbibliography.html#garnelo2018neural" id="id2">[GSR+18]</a> to train members of the LNPF.</p>
</div>
<div class="section" id="neural-process-maximum-likelihood-npml">
<h3>Neural Process Maximum Likelihood (NPML)<a class="headerlink" href="#neural-process-maximum-likelihood-npml" title="Permalink to this headline">¶</a></h3>
</div>
</div>
<div class="section" id="summary">
<h2>Summary<a class="headerlink" href="#summary" title="Permalink to this headline">¶</a></h2>
<p>Monte Carlo estimate of maximum likelihood</p>
<ul class="simple">
<li><p>Simple</p></li>
<li><p>Empirically works well</p></li>
</ul>
<ul class="simple">
<li><p>Biasied</p></li>
<li><p>Sample inefficient</p></li>
</ul>
<p>First, let’s consider a direct approach to optimising the log-marginal predictive likelihood of LNPF members.
While this quantity is no longer tractable (as it was with members of the CNPF), we can derive an estimator using Monte-Carlo sampling:</p>
<div class="math notranslate nohighlight" id="equation-npml">
<span class="eqno">()<a class="headerlink" href="#equation-npml" title="Permalink to this equation">¶</a></span>\[\begin{split}\begin{align}
\log p_{\theta}(\mathbf{y}_\mathcal{T} | \mathbf{x}_\mathcal{T}; \mathcal{C})
&amp;= \log \int p_{\theta} \left( \mathbf{z} | \mathcal{C} \right) \prod_{t=1}^{T} p_{\theta} \left( y^{(t)} | x^{(t)}, \mathbf{z} \right) \mathrm{d}\mathbf{z} &amp; \text{Marginalisation} \\
&amp; \approx \log \left( \frac{1}{L} \sum_{l=1}^{L} \prod_{t=1}^{T} p_{\theta} \left( y^{(t)} | x^{(t)}, \mathbf{z}_l \right) \right) &amp; \text{Monte-Carlo approximation} 
\end{align}\end{split}\]</div>
<p>where each <span class="math notranslate nohighlight">\(\mathbf{z}_l \sim p_{\theta} \left( \mathbf{z} | \mathcal{C} \right)\)</span>.</p>
<div class="dropdown admonition">
<p class="admonition-title">Implementation<span class="math notranslate nohighlight">\(\qquad\)</span>LogSumExp</p>
<p>In practice, manipulating directly probabilities is prone to numerical instabilities, e.g., multiplying probabilities as in Eq.<a class="reference internal" href="#equation-npml">()</a> will often underflow.
As a result one should manipulate probabilities in the log domain:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{align}
\log p_{\theta}(\mathbf{y}_\mathcal{T} | \mathbf{x}_\mathcal{T}; \mathcal{C})
&amp;\approx \log \left( \frac{1}{L} \sum_{l=1}^{L} \prod_{t=1}^{T} p_{\theta} \left( y^{(t)} | x^{(t)}, \mathbf{z}_l \right) \right) &amp; \text{Eq.{eq}`npml`} \\
&amp; = \log \left( \sum_{l=1}^{L} \exp \left(  \sum_{t=1}^{T} \log p_{\theta} \left( y^{(t)} | x^{(t)}, \mathbf{z}_l \right) \right) \right) - \log L &amp; \text{LogSumExp trick}\\
&amp; = \text{LogSumExp}_{l=1}^{L} \left( \sum_{t=1}^{T} \log p_{\theta} \left( y^{(t)} | x^{(t)}, \mathbf{z}_l \right) \right) - \log L,
\end{align}\end{split}\]</div>
<p>where numerical stable implementations of <a class="reference external" href="https://en.wikipedia.org/wiki/LogSumExp#log-sum-exp_trick_for_log-domain_calculations">LogSumExp</a> can be found in most frameworks.</p>
</div>
<p>Eq.<a class="reference internal" href="#equation-npml">()</a> provides a simple-to-compute objective function for training LNPF-members, which we can then use with standard optimisers to learn the model parameters <span class="math notranslate nohighlight">\(\theta\)</span>.
The final pseudo-code for NPML is given in <code class="xref std std-numref docutils literal notranslate"><span class="pre">npml_pseudocode</span></code>:</p>
<div class="figure align-default" id="npml-pseudocode">
<a class="reference internal image-reference" href="../_images/alg_npml.png"><img alt="Pseudo-code NPML." src="../_images/alg_npml.png" style="width: 35em;" /></a>
<p class="caption"><span class="caption-text">Pseudo-code for a single training step of a LNPF member with NPML.</span><a class="headerlink" href="#npml-pseudocode" title="Permalink to this image">¶</a></p>
</div>
<p>NPML is conceptually very simple as it directly approximates the training procedure of the CNPF, in the sense that it targets the same predictive likelihood during training.
Moreover, it tends to work well in practice, typically leading to models achieving good performance.
However, it suffers from two important drawbacks:</p>
<ol class="simple">
<li><p><em>Bias</em>: When applying the Monte-Carlo approximation, we have employed an unbiased estimator to the predictive likelihood. However, in practice we are interested in the <em>log</em> likelihood. Unfortunately, the log of an unbiased estimator is not itself unbiased. As a result, NPML is a <em>biased</em> (conservative) estimator of the true log-likelihood.</p></li>
<li><p><em>Sample-complexity</em>: In practice it turns out that NPML is quite sensitive to the number of samples <span class="math notranslate nohighlight">\(L\)</span> used to approximate it. In both our GP and image experiments, we find that on the order of 20 samples are required to achieve “good” performance. Of course, the computational and memory costs of training scale linearly with <span class="math notranslate nohighlight">\(L\)</span>, often limiting the number of samples that can be used in practice.</p></li>
</ol>
<p>Unfortunately, addressing this turns out to be quite difficult, and is an open question in training latent variable models in general.
However, we next describe an alternative training procedure that typically works well with fewer samples.</p>
<div class="section" id="neural-process-variational-inference-npvi">
<h3>Neural Process Variational Inference (NPVI)<a class="headerlink" href="#neural-process-variational-inference-npvi" title="Permalink to this headline">¶</a></h3>
<p>NPVI is a training procedure proposed by <a class="bibtex reference internal" href="../zbibliography.html#garnelo2018neural" id="id3">[GSR+18]</a>, which takes inspiration from the literature on variational inference (VI).
The central idea behind this objective function is to use <em>posterior sampling</em> to reduce the variance of NPML.
Namely to use both the context <em>and the target</em> set, <span class="math notranslate nohighlight">\(\mathcal{C} \cup \mathcal{T}\)</span>, to produce the distribution over <span class="math notranslate nohighlight">\(\mathbf{z}\)</span> that is more informative, thus producing more meaningful samples and reducing the number of samples we need to properly estimate this objective.</p>
<p>In our case, the posterior distribution from which we would like to sample is <span class="math notranslate nohighlight">\(p(\mathbf{z} | \mathcal{C}, \mathcal{T})\)</span>, i.e., the distribution of the latent variable having observed <em>both</em> the context and target sets.
Unfortunately, this posterior is intractable.
To address this, <a class="bibtex reference internal" href="../zbibliography.html#garnelo2018neural" id="id4">[GSR+18]</a> propose to replace the true posterior by</p>
<div class="math notranslate nohighlight" id="equation-approximate-posterior">
<span class="eqno">()<a class="headerlink" href="#equation-approximate-posterior" title="Permalink to this equation">¶</a></span>\[\begin{align}
p \left( \mathbf{z} | \mathcal{C}, \mathcal{T} \right) \approx p_{\theta} \left( \mathbf{z} | \mathcal{C} \cup \mathcal{T} \right).
\end{align}\]</div>
<div class="note dropdown admonition">
<p class="admonition-title">Note<span class="math notranslate nohighlight">\(\qquad p \left( \mathbf{z} | \mathcal{C}, \mathcal{T} \right) \neq p_{\theta} \left( \mathbf{z} | \mathcal{C} \cup \mathcal{T} \right)\)</span></p>
<p>Note that these two distributions are different.
We can compute <span class="math notranslate nohighlight">\(p_{\theta} \left( \mathbf{z} | \mathcal{C} \cup \mathcal{T} \right)\)</span> by simply passing both <span class="math notranslate nohighlight">\(\mathcal{C}\)</span> and <span class="math notranslate nohighlight">\(\mathcal{T}\)</span> through the model encoder.
One the other hand, the posterior <span class="math notranslate nohighlight">\(p \left( \mathbf{z} | \mathcal{C}, \mathcal{T} \right)\)</span> is computed using Bayes’ rule and our model definition:</p>
<div class="math notranslate nohighlight">
\[
\begin{align}
p \left( \mathbf{z} | \mathcal{C}, \mathcal{T} \right) = \frac{p_{\theta} \left( \mathbf{z} | \mathcal{C} \right) \prod_{t=1}^{T} p_{\theta} \left( y^{(t)} | x^{(t)}, \mathbf{z} \right)}{ \int p_{\theta} \left( \mathbf{z} | \mathcal{C} \right) \prod_{t=1}^{T} p_{\theta} \left( y^{(t)} | x^{(t)}, \mathbf{z} \right) \mathrm{d} \mathbf{z}}.
\end{align}
\]</div>
<p>Recalling that our decoder is defined by a complicated, non-linear neural network, we can see that this posterior is intractable, as it involves an integration against complicated likelihoods.</p>
</div>
<p>We can now derive the final objective function, which is a <em>lower bound</em> to the log marginal likelihood, by (i) introducing the approximate posterior as a sampling distribution, and (ii) employing a straightforward application of <a class="reference external" href="https://en.wikipedia.org/wiki/Jensen%27s_inequality">Jensen’s inequality</a>.
Denoting <span class="math notranslate nohighlight">\(\mathcal{D} = \mathcal{C} \cup \mathcal{T}\)</span>, we have that</p>
<div class="math notranslate nohighlight" id="equation-npvi">
<span class="eqno">()<a class="headerlink" href="#equation-npvi" title="Permalink to this equation">¶</a></span>\[\begin{split}\begin{align}
\log p_{\theta}(\mathbf{y}_\mathcal{T} | \mathbf{x}_\mathcal{T}, \mathcal{C})
&amp;= \log \int p_{\theta} \left( \mathbf{z} | \mathcal{C} \right)  p_{\theta} \left( \mathbf{y}_{\mathcal{T}} | \mathbf{x}_{\mathcal{T}}, \mathbf{z} \right) \mathrm{d}\mathbf{z} &amp; \text{Marginalisation} \\
&amp; = \log \int p_{\theta} \left( \mathbf{z} | \mathcal{D} \right) \frac{p_{\theta} \left( \mathbf{z} | \mathcal{C} \right)}{p_{\theta} \left( \mathbf{z} | \mathcal{D} \right)} p_{\theta} \left( \mathbf{y}_{\mathcal{T}} | \mathbf{x}_{\mathcal{T}}, \mathbf{z} \right) \mathrm{d}\mathbf{z} &amp; \text{Importance Weight} \\
&amp; \geq \int p_{\theta} \left( \mathbf{z} | \mathcal{D} \right) \left( \log p_{\theta} \left( \mathbf{y}_{\mathcal{T}} | \mathbf{x}_{\mathcal{T}}, \mathbf{z} \right) + \log \frac{p_{\theta} \left( \mathbf{z} | \mathcal{C} \right)}{p_{\theta} \left( \mathbf{z} | \mathcal{D} \right)} \right) &amp; \text{Jensen's inequality} \\
&amp; = \mathbb{E}_{\mathbf{z} \sim p_{\theta} \left( \mathbf{z} | \mathcal{D} \right)} \left[ \log p_{\theta} \left( \mathbf{y}_{\mathcal{T}} | \mathbf{x}_{\mathcal{T}}, \mathbf{z} \right) \right] - \mathrm{KL} \left( p_{\theta} \left( \mathbf{z} | \mathcal{D} \right) \| p_{\boldsymbol \theta} \left( \mathbf{z} | \mathcal{C} \right) \right),
\end{align}\end{split}\]</div>
<p>where <span class="math notranslate nohighlight">\(\mathrm{KL}(p \| q)\)</span> is the Kullback-Liebler (KL) divergence between two distributions <span class="math notranslate nohighlight">\(p\)</span> and <span class="math notranslate nohighlight">\(q\)</span>, and we have used the shorthand <span class="math notranslate nohighlight">\(p_{\theta} \left( \mathbf{y}_{\mathcal{T}} | \mathbf{x}_{\mathcal{T}}, \mathbf{z} \right) = \prod_{t=1}^{T} p_{\theta} \left( y^{(t)} | x^{(t)}, \mathbf{z} \right)\)</span> to ease notation.
Let’s consider what we have achieved in Eq. <a class="reference internal" href="#equation-npvi">()</a>.
We have taken our original objective, and re-expressed it as an expectation with respect to <span class="math notranslate nohighlight">\(p_{\theta} \left( \mathbf{z} | \mathcal{D} \right)\)</span>, which now has access to all of <span class="math notranslate nohighlight">\(\mathcal{D}\)</span>, as opposed to the NPML objective, where the expectation was only with respect to <span class="math notranslate nohighlight">\(p_{\theta} \left( \mathbf{z} | \mathcal{C} \right)\)</span>.</p>
<div class="attention admonition">
<p class="admonition-title">Important</p>
<p>Of course, we can only sample from this approximate posterior during <em>training</em>, when we have access to both the context <em>and</em> target sets.
At test time, we will only have access to the context set, and so the forward pass through the model will be equivalent to that of the model when trained with NPML, i.e., we will only pass the context set through the encoder.
This is an important detail of NPVI: forward passes at meta-train time look different than they do at meta-test time!</p>
</div>
<div class="dropdown hint admonition">
<p class="admonition-title">Advanced<span class="math notranslate nohighlight">\(\qquad\)</span>LNPF as Amortised VI</p>
<p>The above procedure can be seen as a form of <em>amortised</em> VI.
Amortised VI is a method for performing approximate inference and learning in probabilistic latent variable models, where an <em>inference</em> network is trained to approximate the true posterior distributions.</p>
<p>There are many resources available on (amortised) VI (e.g., <a class="reference external" href="https://jaan.io/what-is-variational-autoencoder-vae-tutorial/">Jaan Altosaar’s VAE tutorial</a>, <a class="reference external" href="http://blog.shakirm.com/2015/01/variational-inference-tricks-of-the-trade/">the Spectator’s</a> take, or this <a class="reference external" href="https://arxiv.org/pdf/1711.05597.pdf">review paper on modern VI</a>), and we encourage readers unfamiliar with the concept to take the time to go through some of these.</p>
<p>For our purposes, the following intuitions should suffice:</p>
<p>Assume that we have a latent variable model with a prior <span class="math notranslate nohighlight">\(p(\mathbf{z})\)</span>, and a conditional likelihood for observations <span class="math notranslate nohighlight">\(y\)</span>, written <span class="math notranslate nohighlight">\(p_{\theta} \left(y | \mathbf{z} \right)\)</span>.
The central idea in amortised VI is to introduce an <em>inference network</em>, denoted <span class="math notranslate nohighlight">\(q_{\boldsymbol\phi}(\mathbf{z} | y)\)</span>, which maps observations to distributions over <span class="math notranslate nohighlight">\(\mathbf{z}\)</span>.
We can then use <span class="math notranslate nohighlight">\(q_{\boldsymbol\phi}\)</span> to derive a lower bound to the log-marginal likelihood, just as we did above:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align}
\log p_{\theta}(y)
&amp;= \log \int p_{\theta} \left( \mathbf{z} , y \right)\mathrm{d}\mathbf{z} \\
&amp; = \log \int q_{\boldsymbol\phi} \left( \mathbf{z} | y \right) \frac{p_{\theta} \left( \mathbf{z} , y \right)}{q_{\boldsymbol\phi} \left( \mathbf{z} | y \right)} \mathrm{d}\mathbf{z} \\
&amp; \geq \mathbb{E}_{\mathbf{z} \sim q_{\boldsymbol\phi} \left( \mathbf{z} | y \right)} \left[ \log p_{\theta} \left( y | \mathbf{z} \right) \right] - \mathrm{KL} \left( q_{\boldsymbol\phi} \left( \mathbf{z} | y \right) \| p_{\boldsymbol \theta} \left( \mathbf{z} \right) \right).
\end{align}
\end{split}\]</div>
<p>In the VI terminology, this lower bound is commonly known as the <em>evidence lower bound</em> (ELBO).
So maximising the ELBO with respect to <span class="math notranslate nohighlight">\(\theta\)</span> trains the model to optimise a lower bound on the log-marginal likelihood, which is a sensible thing to do.
Moreover, it turns out that maximising the ELBO with respect to <span class="math notranslate nohighlight">\(\boldsymbol\phi\)</span> minimises the KL divergence between <span class="math notranslate nohighlight">\(q_{\boldsymbol\phi} \left( \mathbf{z} | y \right)\)</span> and the true posterior <span class="math notranslate nohighlight">\(p_\theta(\mathbf{z} | y) = p_\theta(y | \mathbf{z}) p_\theta(\mathbf{z}) / p_\theta(y)\)</span>, so we can think of <span class="math notranslate nohighlight">\(q_{\boldsymbol\phi}\)</span> as approximating the true posterior in a meaningful way.</p>
<p>In the NPF, to approximate the desired posterior, we can introduce a network that maps datasets to distributions over the latent variable.
We already know how to define such networks in the NPF – that’s exactly what the encoder <span class="math notranslate nohighlight">\(p_{\theta}(\mathbf{z} | \mathcal{C})\)</span> does!
In fact, NPVI proposes to use the encoder as the inference network when training LNPF members.</p>
<p>So we can view Eq.<a class="reference internal" href="#equation-npvi">()</a> as performing amortised VI for any member of the LNPF.
The twist on standard amortised VI is that here, we are sharing <span class="math notranslate nohighlight">\(q_{\boldsymbol\phi}\)</span> with a part of the model itself, since <span class="math notranslate nohighlight">\(p_{\theta}(\mathbf{z} | \mathcal{D})\)</span> plays the dual role of being an approximate posterior <span class="math notranslate nohighlight">\(q_{\boldsymbol\phi}\)</span>, and also defining the <em>conditional prior</em> having observed <span class="math notranslate nohighlight">\(\mathcal{D}\)</span>. This somewhat complicates our understanding of the procedure, and could lead to unintended consequences.</p>
</div>
<p>When both the encoder and inference network parameterise Gaussian distributions over <span class="math notranslate nohighlight">\(\mathbf{z}\)</span> (as is standard), the KL-term can be computed analytically.
Hence we can derive an unbiased estimator to Eq.<code class="xref eq docutils literal notranslate"><span class="pre">np_elbo</span></code> by taking samples from <span class="math notranslate nohighlight">\(p_{\theta} \left( \mathbf{z} | \mathcal{D} \right)\)</span> to estimate the first term on the RHS.
<code class="xref std std-numref docutils literal notranslate"><span class="pre">npvi_pseudocode</span></code> provides the pseudo-code for a single training iteration for a LNPF member, using NPVI as the target objective.</p>
<div class="figure align-default" id="npvi-pseudocode">
<a class="reference internal image-reference" href="../_images/alg_npvi.png"><img alt="Pseudo-code NPVI." src="../_images/alg_npvi.png" style="width: 35em;" /></a>
<p class="caption"><span class="caption-text">Pseudo-code for a single training step of a LNPF member with NPVI.</span><a class="headerlink" href="#npvi-pseudocode" title="Permalink to this image">¶</a></p>
</div>
<p>NPVI has several appealing properties:</p>
<ul class="simple">
<li><p>It utilises <em>posterior sampling</em> to reduce the variance of the Monte-Carlo estimator of the intractable expectation. This means that often we can get away with training models taking just a <em>single</em> sample (in contrast with NPML), resulting in computationally and memory efficient training procedures.</p></li>
<li><p>If our approximate posterior can recover the true posterior, the inequality in the ELBO is tight, and we are exactly optimising the log-marginal likelihood.</p></li>
</ul>
<p>However, it also inherits the main drawbacks of VI.
In particular, it is almost never the case that the true posterior can be recovered by our approximate posterior in any practical application.
In these settings:</p>
<ul class="simple">
<li><p>Meaningful guarantees about the quality of the learned models are hard to come by.</p></li>
<li><p>The inequality holds, meaning that we are only optimising a lower-bound to the quantity we actually care about. Moreover, it is often quite difficult to know how tight this bound may be.</p></li>
</ul>
<p>As we have discussed, the most appealing property of NPVI is that it utilises <em>posterior sampling</em> to reduce the variance of the Monte-Carlo estimator of the intractable expectation.
This means that often we can get away with training models taking just a single sample, resulting in computationally and memory efficient training procedures.
However, it also comes with several drawbacks, which can be roughly summarised as follows:</p>
<ul class="simple">
<li><p>NPVI is focused on approximating the posterior distribution, and diverts encoder capacity and attention of the optimiser to recovering the true posterior. However, in the NPF setting, we are typically only interested in the predictive distribution <span class="math notranslate nohighlight">\(p(\mathbf{y}_T | \mathbf{x}_T, \mathcal{C})\)</span>, and it is unclear whether focusing our efforts on <span class="math notranslate nohighlight">\(\mathbf{z}\)</span> is beneficial to achieving higher quality predictive distributions.</p></li>
<li><p>In NPVI, the encoder plays a dual role: it is both part of the model, and used as an inference network. This fact introduces additional complexities in the training procedure, and it may be that using the encoder as an approximate posterior has a detrimental effect on the resulting predictive distributions.</p></li>
</ul>
<p>As we shall see below, it is often the case that models trained with NPML produce better fits than equivalent models trained with NPVI, at the cost of additional computational and memory costs of training.
In pratice however, NPVI is the most commonly employed procedure for training LNPF members.</p>
<div class="warning dropdown admonition">
<p class="admonition-title">Warning<span class="math notranslate nohighlight">\(\qquad\)</span>Biased estimators</p>
<p>Importantly, it is not the case that NPVI avoids the “biased estimator” issue of NPML.
Rather, NPML defines a biased (conservative) estimator of the desired quantity, and NPVI produces an unbiased estimator of a strict lower-bound of the same quantity.
In both cases, unfortunately, we do not have unbiased estimators of the quantity we really care about — <span class="math notranslate nohighlight">\(\log p_{\theta}(\mathbf{y}_\mathcal{T} | \mathbf{x}_\mathcal{T}, \mathcal{C})\)</span>.</p>
<p>Another consequence of this is that it is challenging to evaluate the models quantitatively, since our performance metrics are only ever lower bounds to the true model performance, and quantifying how tight those bounds are is quite challenging.</p>
</div>
<div class="dropdown admonition">
<p class="admonition-title">Implementation<span class="math notranslate nohighlight">\(\qquad\)</span>Lower Bounds</p>
<p>Lower Bounds</p>
</div>
<div class="hint dropdown admonition">
<p class="admonition-title">Advanced<span class="math notranslate nohighlight">\(\qquad\)</span>Relationship between NPML and NPVI</p>
<p>There is a close relationship between the NPML and NPVI objectives.
To see this, we denote the <em>normalising constant</em> for the distribution <span class="math notranslate nohighlight">\(p_{\theta} \left( \mathbf{y}_{\mathcal{T}}, \mathbf{z} | \mathbf{x}_{\mathcal{T}}, \mathcal{C} \right)\)</span> as</p>
<div class="math notranslate nohighlight">
\[
\begin{align}
  Z = \int p_{\theta} \left( \mathbf{y}_{\mathcal{T}} | \mathbf{x}_{\mathcal{T}}, \mathbf{z} \right) p_{\theta} \left( \mathbf{z} | \mathcal{C} \right) \mathrm{d}\mathbf{z}.
\end{align}
\]</div>
<p>Now, we can rewrite the NPVI objective as</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align}
\mathcal{L}_{VI} &amp; (\theta ; D) = \mathbb{E}_{p_{\theta}(\mathbf{z} | \mathcal{D})} \left[ \log p_{\theta} \left( \mathbf{y}_{\mathcal{T}} | \mathbf{x}_{\mathcal{T}}, \mathbf{z} \right) \right] - \mathrm{KL} \left( p_{\theta} \left( \mathbf{z} | \mathcal{D} \right) \| p_{\boldsymbol \theta} \left( \mathbf{z} | \mathcal{C} \right) \right) \\
&amp; = \mathbb{E}_{p_{\theta}(\mathbf{z} | \mathcal{D})} \left[ \log p_{\theta} \left( \mathbf{y}_{\mathcal{T}} | \mathbf{x}_{\mathcal{T}}, \mathbf{z} \right) + \log p_{\theta} \left( \mathbf{z} | \mathcal{C} \right) - \log p_{\theta} \left( \mathbf{z} | \mathcal{D} \right) \right] \\
&amp; = \mathbb{E}_{p_{\theta}(\mathbf{z} | \mathcal{D})} \left[ \log Z + \log \frac{1}{Z} p_{\theta} \left( \mathbf{y}_{\mathcal{T}}, \mathbf{z} | \mathbf{x}_{\mathcal{T}}, \mathcal{C} \right) - \log p_{\theta} \left( \mathbf{z} | \mathcal{D} \right) \right] \\
&amp; = \mathcal{L}_{ML}(\theta; \mathcal{D}) - \mathrm{KL} \left( p_{\theta} \left( \mathbf{z} | \mathcal{D} \right) \| \frac{1}{Z} p_{\theta} \left( \mathbf{y}_{\mathcal{T}}, \mathbf{z} | \mathbf{x}_{\mathcal{T}}, \mathcal{C} \right) \right).
\end{align}
\end{split}\]</div>
<p>Thus, we can see that <span class="math notranslate nohighlight">\(\mathcal{L}_{VI}\)</span> is equal to <span class="math notranslate nohighlight">\(\mathcal{L}_{ML}\)</span> (with infinitely many samples) up to an additional KL term.
This KL term has a nice interpretation as encouraging consistency among predictions with different context sets, which is the kind of consistency <em>not</em> baked into the NPF.</p>
<p>However, this term can also be a <em>distractor</em>.
When dealing with NPF members, we are typically not interested in the latent variable <span class="math notranslate nohighlight">\(\mathbf{z}\)</span>, and most considered tasks require only a “good” approximation to the predictive distribution over <span class="math notranslate nohighlight">\(\mathbf{y}_{\mathcal{T}\)</span>.
Therefore, given only finite model capacity, and finite data, it may be preferable to focus all the model capacity on achieving the best possible predictive distribution (which is what <span class="math notranslate nohighlight">\(\mathcal{L}_{ML}\)</span> focuses on), rather than focusing on the distribution over <span class="math notranslate nohighlight">\(\mathbf{z}\)</span>, as encouraged by the KL term introduced by <span class="math notranslate nohighlight">\(\mathcal{L}_{VI}\)</span>.</p>
</div>
<p>Armed with procedures for training LNPF-members, we turn our attention to the models themselves.
In particular, we next introduce the latent-variable variant of each of the conditional models introduced in the previous section, and we shall see that having addressed the training procedures, the extension to latent variables is quite straightforward from a practical perspective.</p>
</div>
</div>
<div class="section" id="latent-neural-process-lnp">
<h2>Latent Neural Process (LNP)<a class="headerlink" href="#latent-neural-process-lnp" title="Permalink to this headline">¶</a></h2>
<div class="figure align-default" id="computational-graph-lnps-text">
<a class="reference internal image-reference" href="../_images/computational_graph_LNPs1.svg"><img alt="Computational graph LNP" src="../_images/computational_graph_LNPs1.svg" width="300em" /></a>
<p class="caption"><span class="caption-text">Computational graph for LNPS.</span><a class="headerlink" href="#computational-graph-lnps-text" title="Permalink to this image">¶</a></p>
</div>
<p>The latent neural process <a class="bibtex reference internal" href="../zbibliography.html#garnelo2018neural" id="id5">[GSR+18]</a> is the latent counterpart of the CNP, and the first member of the LNPF proposed in the literature.
Given the vector <span class="math notranslate nohighlight">\(R\)</span>, which is computed as in the CNP, we simply pass it through an additional MLP to predict the mean and variance of the latent representation <span class="math notranslate nohighlight">\(\mathbf{z}\)</span>, from which we can produce samples.
We call the computational graph that produces the distribution over <span class="math notranslate nohighlight">\(\mathbf{z}\)</span> the <em>latent path</em>, to distinguish from a computation graph for a deterministic representation, which we refer to as a <em>deterministic path</em>.
The decoder then has the same architecture as that of the CNP, and we can simply pass samples of <span class="math notranslate nohighlight">\(\mathbf{z}\)</span>, together with desired target locations, to produce our predictive distributions.
<code class="xref std std-numref docutils literal notranslate"><span class="pre">computational_graph_LNPs_text</span></code> illustrates the computational graph of the LNP.
Throughout the section we train LNPs with NPVI as in the original paper.</p>
<div class="note dropdown admonition">
<p class="admonition-title">Implementation<span class="math notranslate nohighlight">\(\qquad\)</span>Parameterising the Observation Noise</p>
<p>An important detail in the parameterisation of LNPF members is how the predictive standard deviation is handled, there are 2 possibilities</p>
<ul class="simple">
<li><p><em>Heteroskedastic noise</em>: As with CNPF members, the decoder parameterises a mean <span class="math notranslate nohighlight">\(\mu^{(t)}\)</span> and a standard deviation <span class="math notranslate nohighlight">\(\sigma^{(t)}\)</span> for each target location <span class="math notranslate nohighlight">\(x^{(t)}\)</span>.</p></li>
<li><p><em>Homoskedastic noise</em>: The decoder only parameterises a mean <span class="math notranslate nohighlight">\(\mu^{(t)}\)</span>, while the standard deviation <span class="math notranslate nohighlight">\(\sigma\)</span> is a global (learned) observation noise parameter, i.e., it is shared for all target locations.
Note that the uncertainty of the posterior predictive at two different target locations can still be very different.
Indeed, it is influenced by the variance in the <em>means</em> <span class="math notranslate nohighlight">\(\mu^{(t)}\)</span>, which arises from the uncertainty in the latent variable.</p></li>
</ul>
<p>In practice, a heteroskedastic noise usually performs better than the homoskedastic variant.</p>
</div>
<p>Below, we show the predictive distribution of an LNP trained on samples from the RBF-kernel GP, as the number of observed context points from the underlying function increases.</p>
<div class="figure align-default" id="lnp-rbf-text">
<a class="reference internal image-reference" href="../_images/LNP_rbf.gif"><img alt="LNP on GP with RBF kernel" src="../_images/LNP_rbf.gif" style="width: 35em;" /></a>
<p class="caption"><span class="caption-text">Samples from posterior predictive of LNPs (Blue) and the oracle GP (Green) with RBF kernel.</span><a class="headerlink" href="#lnp-rbf-text" title="Permalink to this image">¶</a></p>
</div>
<p><code class="xref std std-numref docutils literal notranslate"><span class="pre">graph_model_LNPs_text</span></code> shows that the latent variable indeed enables coherent sampling from the posterior predictive.
In fact, within the range <span class="math notranslate nohighlight">\([-1, 1]\)</span> the model produces very nice samples, that seem to properly mimic those from the underlying process.
Nevertheless, here too we see that the LNP suffers from the same underfitting issue as discussed with CNPs.
We again see the tendency to overestimate the uncertainty, and often not pass through all the context points with the mean functions.
Moreover, we can observe that beyond the <span class="math notranslate nohighlight">\([-1, 1]\)</span>, the model seems to “give up” on the context points and uncertainty, despite having been trained on the range <span class="math notranslate nohighlight">\([-2, 2]\)</span>.</p>
<div class="dropdown note admonition">
<p class="admonition-title">Note<span class="math notranslate nohighlight">\(\qquad\)</span>NPVI vs NPML</p>
<p>In the main text we trained LNP with NPVI as in the original paper, we compare the results with NPML on different kernels below.</p>
<div class="figure align-default" id="lnp-all-both-objectives-text">
<a class="reference internal image-reference" href="../_images/singlegp_LNP_LatLBTrue_SigLBTrue1.gif"><img alt="LNP on all kernels with both objectives" src="../_images/singlegp_LNP_LatLBTrue_SigLBTrue1.gif" style="width: 35em;" /></a>
<p class="caption"><span class="caption-text">Predictive distributions of LNPs (Blue) and the oracle GP (Green) with (top) RBF, (center) periodic, and (bottom) Noisy Matern kernels. Models were trained with (left) NPML and (right) NPVI.</span><a class="headerlink" href="#lnp-all-both-objectives-text" title="Permalink to this image">¶</a></p>
</div>
<p><code class="xref std std-numref docutils literal notranslate"><span class="pre">LNP_all_both_objectives_text</span></code> illustrates several interesting points that we tend to see in our experiments.
First, as with the CNP, the model tends to underfit, and in particular fails catastrophically with the periodic kernel.
In some cases, e.g., when trained on the RBF kernel with NPVI, the model seems to collapse the uncertainty arising from the latent variable in certain regions, and rely entirely on the observation noise.
In our experiments, we observe that this tends to occur when training with NPVI, but not with NPML.
Finally, we can see that NPML tends to produce predictive distributions that fit the data better, and tend to have lower uncertainty near the context set points.</p>
</div>
<div class="note admonition">
<p class="admonition-title">Details</p>
<p>Model details, training and more plots in <a class="reference internal" href="../reproducibility/LNP.html"><span class="doc">LNP Notebook</span></a></p>
</div>
</div>
<div class="section" id="attentive-latent-neural-process-attnlnp">
<h2>Attentive Latent Neural Process (AttnLNP)<a class="headerlink" href="#attentive-latent-neural-process-attnlnp" title="Permalink to this headline">¶</a></h2>
<p>The Attentive LNPs <a class="bibtex reference internal" href="../zbibliography.html#kim2019attentive" id="id6">[KMS+19]</a> is the latent counterpart of AttnCNPs.
Differently from the LNP, the AttnLNP <em>added</em> a “latent path” in addition to (rather than instead of) the deterministic path.
The latent path is implemented with the same method as LNPs, i.e. a mean aggregation followed by a parametrization of a Gaussian.
In other words, even though the deterministic representation <span class="math notranslate nohighlight">\(R^{(t)}\)</span> is target specific, the latent representation <span class="math notranslate nohighlight">\(\mathbf{z}\)</span>  is target independent as seen in the computational graph (<code class="xref std std-numref docutils literal notranslate"><span class="pre">computational_graph_AttnLNPs_text</span></code>).</p>
<div class="figure align-default" id="computational-graph-attnlnps-text">
<a class="reference internal image-reference" href="../_images/computational_graph_AttnLNPs1.svg"><img alt="Computational graph AttnLNP" src="../_images/computational_graph_AttnLNPs1.svg" width="400em" /></a>
<p class="caption"><span class="caption-text">Computational graph for AttnLNPS. [drop?]</span><a class="headerlink" href="#computational-graph-attnlnps-text" title="Permalink to this image">¶</a></p>
</div>
<p>Throughout the section we train AttnLNPs with NPVI as in the original paper.
Below, we show the predictive distribution of an AttnLNP trained on samples from RBF, periodic, and noisy Matern kernel GPs, again viewing the predictive as the number of observed context points is increased.</p>
<div class="figure align-default" id="attnlnp-single-gp-text">
<a class="reference internal image-reference" href="../_images/AttnLNP_single_gp1.gif"><img alt="AttnLNP on single GP" src="../_images/AttnLNP_single_gp1.gif" style="width: 35em;" /></a>
<p class="caption"><span class="caption-text">Samples Posterior predictive of AttnLNPs (Blue) and the oracle GP (Green) with RBF,periodic, and noisy Matern kernel.</span><a class="headerlink" href="#attnlnp-single-gp-text" title="Permalink to this image">¶</a></p>
</div>
<p><code class="xref std std-numref docutils literal notranslate"><span class="pre">AttnLNP_single_gp_text</span></code> paints an interesting picture regarding the AttnLNP.
On the one hand, we see that it is able to do a significantly better job in modelling the marginals than the LNP.
However, on closer inspection, we can see several issues with the resulting distributions:</p>
<ul class="simple">
<li><p><em>Kinks</em>: The samples do not seem to be smooth, and we see “kinks” that are similar (though even more pronounced) than in <a class="reference internal" href="CNPF.html#attncnp-single-gp-text"><span class="std std-numref">Fig. 14</span></a>.</p></li>
<li><p><em>Collapse to AttnCNP</em>: In many places the AttnLNP seems to collpase the distribution around the latent variable, and express all its uncertainty via the observation noise. This tends to occur more often for the AttnLNP when trained with NPVI rather than NPML.</p></li>
</ul>
<div class="dropdown note admonition">
<p class="admonition-title">Note<span class="math notranslate nohighlight">\(\qquad\)</span>AttnLNP and “lower bounding” standard deviations</p>
<p>As with the LNP, we compare the performance of the AttnLNP when trained with NPVI and NPML.</p>
<div class="figure align-default" id="attnlnp-all-both-objectives-text">
<a class="reference internal image-reference" href="../_images/singlegp_AttnLNP_LatLBTrue_SigLBTrue1.gif"><img alt="AttnLNP on all kernels with both objectives" src="../_images/singlegp_AttnLNP_LatLBTrue_SigLBTrue1.gif" style="width: 35em;" /></a>
<p class="caption"><span class="caption-text">Predictive distributions of AttnLNPs (Blue) and the oracle GP (Green) with (top) RBF, (center) periodic, and (bottom) Noisy Matern kernels. Models were trained with (left) NPML and (right) NPVI.</span><a class="headerlink" href="#attnlnp-all-both-objectives-text" title="Permalink to this image">¶</a></p>
</div>
<p>Here we see that the AttnLNP tends to “collapse” for both the RBF and noisy Matern kernels when trained with NPVI.
In contrast, when trained with NPML it tends to avoid this behaviour.
This is consistent with what we observe more generally in our experiments.</p>
<p>This leads to an important point regarding the LNPF, which we have so far not discussed.
In the LNPF literature, it is common to “lower bound” the standard deviations of distributions output by models.
This is often achieved by parameterising the standard deviation as</p>
<div class="math notranslate nohighlight">
\[
\begin{align}
	\sigma = \epsilon + (1 - \epsilon) \ln (1 + \exp (f_\sigma)),
\end{align}
\]</div>
<p>where <span class="math notranslate nohighlight">\(\epsilon\)</span> is a small real number (e.g. 0.001), and <span class="math notranslate nohighlight">\(f_\sigma\)</span> is the log standard deviation output by the model.
This “lower bounding” is often used in practice for the standard deviations of both the latent and predictive distributions.
In fact Le et al.<a class="bibtex reference internal" href="../zbibliography.html#le2018empirical" id="id7">[LKG+18]</a> find that this consistently improves performance of the AttnLNP, and recommend it as best practice.
Following the literature, the results displayed in this tutorial all employ such lower bounds.
However, we note that, in our opinion, there is no conceptual justification for this trick, and it is indicative of flaws in the models and or training procedures.
To highlight this point, we demonstrate that ….</p>
</div>
<p>Let us now consider the image experiments as we did with the AttnCNP.</p>
<div class="figure align-default" id="attnlnp-img-text">
<a class="reference internal image-reference" href="../_images/AttnLNP_img1.gif"><img alt="AttnLNP on CelebA, MNIST, ZSMM" src="../_images/AttnLNP_img1.gif" style="width: 45em;" /></a>
<p class="caption"><span class="caption-text">Samples from posterior predictive of an AttnCNP for CelebA <span class="math notranslate nohighlight">\(32\times32\)</span>, MNIST, ZSMM.</span><a class="headerlink" href="#attnlnp-img-text" title="Permalink to this image">¶</a></p>
</div>
<p>From <code class="xref std std-numref docutils literal notranslate"><span class="pre">AttnLNP_img_text</span></code> we see that AttnLNP generates quite impressive samples, and exhibits descent sampling and good performances when the model does not require generalisation (CelebA <span class="math notranslate nohighlight">\(32\times32\)</span>, MNIST).
However, as expected, the model “breaks” for ZSMM as it still cannot extrapolate.</p>
<div class="note admonition">
<p class="admonition-title">Details</p>
<p>Model details, training and more plots in <a class="reference internal" href="../reproducibility/AttnLNP.html"><span class="doc">AttnLNP Notebook</span></a></p>
</div>
</div>
<div class="section" id="convolutional-latent-neural-process-convlnp">
<h2>Convolutional Latent Neural Process (ConvLNP)<a class="headerlink" href="#convolutional-latent-neural-process-convlnp" title="Permalink to this headline">¶</a></h2>
<p>The Convolutional LNP <a class="bibtex reference internal" href="../zbibliography.html#foong2020convnp" id="id8">[FBG+20]</a> is the latent counterpart of the ConvCNP.
Similar to the LNP (and in contrast with the AttnLNP), the latent path <em>replaces</em> the deterministic one, resulting in a latent functional representation (a latent stochastic process) instead of a latent vector valued variable.
We can represent this model with a graphical representation as illustrated in <code class="xref std std-numref docutils literal notranslate"><span class="pre">graph_model_ConvLNPs_text</span></code>.</p>
<div class="figure align-default" id="graph-model-convlnps-text">
<a class="reference internal image-reference" href="../_images/graph_model_ConvLNPs.svg"><img alt="graphical model ConvLNP" src="../_images/graph_model_ConvLNPs.svg" width="200em" /></a>
<p class="caption"><span class="caption-text">Graphical model for ConvLNPs.</span><a class="headerlink" href="#graph-model-convlnps-text" title="Permalink to this image">¶</a></p>
</div>
<p>Another way of viewing the ConvLNP, which is useful in gaining an intuitive understanding of the computational graph (see <code class="xref std std-numref docutils literal notranslate"><span class="pre">computational_graph_ConvLNPs_text</span></code>) is as consisting of two stacked ConvCNPs: the first (the encoder) takes in context sets and outputs a latent stochastic process.
The second (the decoder) takes as input a sample from the latent process and models the posterior predictive conditioned on that sample.</p>
<div class="figure align-default" id="computational-graph-convlnps-text">
<a class="reference internal image-reference" href="../_images/computational_graph_ConvLNPs1.svg"><img alt="Computational graph ConvLNP" src="../_images/computational_graph_ConvLNPs1.svg" width="400em" /></a>
<p class="caption"><span class="caption-text">Computational graph for ConvLNPS. [simplify ? useful to mention or show induced points ? drop?]</span><a class="headerlink" href="#computational-graph-convlnps-text" title="Permalink to this image">¶</a></p>
</div>
<p>One difficulty arises in training the ConvLNP with the NPVI objective, as it requires evaluating the KL divergence between two stochastic processes, which is a tricky proposition.
<a class="bibtex reference internal" href="../zbibliography.html#foong2020convnp" id="id9">[FBG+20]</a> propose a simple approach, that approximates this quantity by instead summing the KL divergences at each discretisation location.
However, as they note, the ConvLNP tends to perform significantly better in most cases when trained with NPML rather than NPVI.
Below, we show similar plots for the ConvLNP on the GP experiments.
However, here we are illustrating the performance of a ConvLNP trained with NPML, not NPVI. (SAME QUESTION: SHOULD WE PUT THESE SIDE BY SIDE WITH A MODEL TRAINED WITH NPML?.)</p>
<div class="note dropdown admonition">
<p class="admonition-title">Note<span class="math notranslate nohighlight">\(\qquad\)</span>Global Latent Representations</p>
<p>In this tutorial, we consider a simple extension to the ConvLNP proposed by <a class="bibtex reference internal" href="../zbibliography.html#foong2020convnp" id="id10">[FBG+20]</a>, which includes a <em>global latent representation</em> as well.
The global representation is computed by average-pooling half of the channels in the latent function, resulting in a translation <em>invariant</em> latent representation (further details regarding this can be found in the ConvLNP notebook).
The intuition behind such a representation is that it may help to capture aspects of the underlying function that are global, allowing the functional representation to represent more localised information.
This intuition is clearest when considering the mixture of GPs experiments discussed below.</p>
</div>
<div class="figure align-default" id="convlnp-single-gp-extrap-text">
<a class="reference internal image-reference" href="../_images/ConvLNP_single_gp_extrap1.gif"><img alt="ConvLNP on GPs with RBF, periodic, Matern kernel" src="../_images/ConvLNP_single_gp_extrap1.gif" style="width: 35em;" /></a>
<p class="caption"><span class="caption-text">Samples Posterior predictive of ConvLNPs (Blue) and the oracle GP (Green) with RBF,periodic, and noisy Matern kernel.</span><a class="headerlink" href="#convlnp-single-gp-extrap-text" title="Permalink to this image">¶</a></p>
</div>
<p>From <code class="xref std std-numref docutils literal notranslate"><span class="pre">ConvLNP_single_gp_extrap_text</span></code> we see that ConvLNP performs very well and the samples are reminiscent of those from a GP, i.e., with much richer variability compared to <code class="xref std std-numref docutils literal notranslate"><span class="pre">AttnLNP_single_gp_text</span></code>.
Further, as in the case of the ConvCNP, we see that the ConvLNP elegantly generalises beyond the range in <span class="math notranslate nohighlight">\(X\)</span>-space on which it was trained.</p>
<p>Next, we consider the more challenging problem of having the ConvLNP model a stochastic process whose posterior predictive is non Gaussian.
We do so by having the following underlying generative process: first, sample one of the 3 kernels discussed above, and second, sample a function from the sampled kernel.
Importantly, the data generating process is a mixture of GPs, and the true posterior predictive process (achieved by marginalising over the different kernels) is non-Gaussian.</p>
<!-- Theoretically, this could still be modelled by a LNPF as the latent variables could represent the both the current kernel and its functions, but adding the global representation corresponds (intuitively) to allowing the ConvLNP to "switch" between kernels. -->
<div class="figure align-default" id="convlnp-kernel-gp-text">
<a class="reference internal image-reference" href="../_images/ConvLNP_kernel_gp1.gif"><img alt="ConvLNP trained on GPs with RBF,Matern,periodic kernel" src="../_images/ConvLNP_kernel_gp1.gif" style="width: 35em;" /></a>
<p class="caption"><span class="caption-text">Similar to <code class="xref std std-numref docutils literal notranslate"><span class="pre">ConvLNP_single_gp_extrap_text</span></code> but the training was performed on all data simultaneously.</span><a class="headerlink" href="#convlnp-kernel-gp-text" title="Permalink to this image">¶</a></p>
</div>
<p><code class="xref std std-numref docutils literal notranslate"><span class="pre">ConvLNP_kernel_gp_text</span></code> demonstrates that ConvLNP performs quite well in this harder setting.
Indeed, it seems to model the predictive process using the periodic kernel when the number of context points is small but quickly (around 10 context points) recovers the correct underlying kernel.
Note how in the middle plot, the ConvLNP becomes progressively more and more ‘confident’ that the process is periodic as more data is observed.
Note that in <code class="xref std std-numref docutils literal notranslate"><span class="pre">ConvLNP_kernel_gp_text</span></code> we are plotting the posterior predictive for the sampled GP, rather than the actual, non-GP posterior predictive process.</p>
<p>[should we also add the results of <a class="reference internal" href="../reproducibility/ConvLNP.html#convlnp-vary-gp"><span class="std std-numref">Fig. 70</span></a> to show that not great when large/ uncountable number of kernels?]</p>
<p>Again, we consider the performance of the ConvLNP in the image setting.
In <code class="xref std std-numref docutils literal notranslate"><span class="pre">ConvLNP_img_text</span></code> we see that the ConvLNP does a reasonable job producing samples when the context sets are uniformly subsampled from images, but struggles with the “structured” context sets, e.g. when the left or bottom halves of the image are missing.
Moreover, the ConvLNP is able to produce samples in the generalisation setting (ZSMM), but these are not always coherent, and include some strange artifacts that seem more similar to sampling the MNIST “texture” than coherent digits.</p>
<div class="figure align-default" id="convlnp-img-text">
<a class="reference internal image-reference" href="../_images/ConvLNP_img1.gif"><img alt="ConvLNP on CelebA, MNIST, ZSMM" src="../_images/ConvLNP_img1.gif" style="width: 45em;" /></a>
<p class="caption"><span class="caption-text">Samples from posterior predictive of an ConvCNP for CelebA <span class="math notranslate nohighlight">\(32\times32\)</span>, MNIST, ZSMM.</span><a class="headerlink" href="#convlnp-img-text" title="Permalink to this image">¶</a></p>
</div>
<p>As discussed in  <a class="reference internal" href="CNPF.html#issues-cnpfs"><span class="std std-ref">the ‘Issues with the CNPF’ section</span></a>, members of the CNPF could not be used to generate coherent samples, nor model non-Gaussian posterior predictive distributions.
In contrast, <code class="xref std std-numref docutils literal notranslate"><span class="pre">ConvLNP_marginal_text</span></code> (right) demonstrates that, as expected, ConvLNP is able to produce non-Gaussian predictives for pixels, with interesting bi-modal and heavy-tailed behaviours.</p>
<div class="figure align-default" id="convlnp-marginal-text">
<a class="reference internal image-reference" href="../_images/ConvLNP_marginal.png"><img alt="Samples from ConvLNP on MNIST and posterior of different pixels" src="../_images/ConvLNP_marginal.png" style="width: 20em;" /></a>
<p class="caption"><span class="caption-text">Samples form the posterior predictive of ConvCNPs on MNIST (left) and posterior predictive of some pixels (right).</span><a class="headerlink" href="#convlnp-marginal-text" title="Permalink to this image">¶</a></p>
</div>
<p>[REVERT PLOTS OF CONVLNP, I made a small modificiation which looks bad…]</p>
</div>
<div class="section" id="issues-and-discussion">
<h2>Issues and Discussion<a class="headerlink" href="#issues-and-discussion" title="Permalink to this headline">¶</a></h2>
<p>We have seen how members of the LNPF utilise a latent variable to define a predictive distribution, thus achieving structured and expressive predictive distributions over target sets.
Despite these advantages, the LNPF suffers from important drawbacks:</p>
<ul class="simple">
<li><p>The training procedure only optimises a biased objective or a lower bound to the true objective.</p></li>
<li><p>Approximating the objective function requires sampling, which can lead to high variance during training.</p></li>
<li><p>They are more memory and computationally demanding, requiring many samples to estimate the objective for NPML.</p></li>
<li><p>It is difficult to quantitatively evaluate and compare different models, since only lower bounds to the log predictive likelihood can be estimated.</p></li>
</ul>
<p>Despite these challenges, the LNPF defines a useful and powerful class of models.
Whether to deploy a member of the CNPF or LNPF depends on the task at hand.
For example, if samples are not required for a particular application, and we have reason to believe a parametric distribution may be a good description of the likelihood, it may well be that a CNPF would be preferable.
Conversely, it is crucial to use the LNPF if sampling or dependencies in the predictive are required, for example in Thompson sampling.</p>
<!-- Finally, we believe there is an exciting area of research in developing NPF members that enjoy the best of both worlds.
Some examples of potential avenues for investigation are:
1. Improving training procedures for the LNPF using ideas from importance sampling and unbiased estimators of the marginal likelihood.
2. Considering flow-based parameterisations that admit closed form likelihood computations, while potentially maintaining structure in the predictive distribution for the target set. -->
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./text"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
        </div>
    </div>
    
    
    <div class='prev-next-bottom'>
        

    </div>
    <footer class="footer mt-5 mt-md-0">
    <div class="container">
      <p>
        
          By Yann Dubois, Jonathan Gordon, ‪Andrew Foong<br/>
        
            &copy; Copyright 2020.<br/>
      </p>
    </div>
  </footer>
</main>


      </div>
    </div>

    <script src="../_static/js/index.js"></script>
    
  </body>
</html>