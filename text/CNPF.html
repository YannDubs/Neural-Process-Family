
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Conditional NPF &#8212; Neural Process Family</title>
    
  <link rel="stylesheet" href="../_static/css/index.d431a4ee1c1efae0e38bdfebc22debff.css">

    
  <link rel="stylesheet"
    href="../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      
  <link rel="stylesheet"
    href="../_static/vendor/open-sans_all/1.44.1/index.css">
  <link rel="stylesheet"
    href="../_static/vendor/lato_latin-ext/1.44.1/index.css">

    
    <link rel="stylesheet" href="../_static/sphinx-book-theme.bfb7730f9caf2ec0b46a44615585038c.css" type="text/css" />
    <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/custom.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-dropdown.css" />
    
  <link rel="preload" as="script" href="../_static/js/index.30270b6e4c972e43c488.js">

    <script id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/language_data.js"></script>
    <script src="../_static/togglebutton.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script >var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../_static/sphinx-book-theme.be0a4a0c39cd630af62a2fcf693f3f06.js"></script>
    <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <script type="text/x-mathjax-config">MathJax.Hub.Config({"tex2jax": {"inlineMath": [["\\(", "\\)"]], "displayMath": [["\\[", "\\]"]], "processRefs": false, "processEnvironments": false}})</script>
    <script async="async" src="https://unpkg.com/thebelab@latest/lib/index.js"></script>
    <script >
        const thebe_selector = ".thebe"
        const thebe_selector_input = "pre"
        const thebe_selector_output = ".output"
    </script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <link rel="shortcut icon" href="../_static/favicon.ico"/>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Latent NPFs" href="LNPF.html" />
    <link rel="prev" title="The Neural Process Family" href="Intro.html" />

    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="en" />



  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
<a class="navbar-brand text-wrap" href="../index.html">
  
  <img src="../_static/logo.gif" class="logo" alt="logo">
  
  
  <h1 class="site-logo" id="site-title">Neural Process Family</h1>
  
</a>
</div>

<form class="bd-search d-flex align-items-center" action="../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form>

<nav class="bd-links" id="bd-docs-nav" aria-label="Main navigation">
  <ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="Intro.html">
   The Neural Process Family
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Sub-Families
 </span>
</p>
<ul class="current nav sidenav_l1">
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   Conditional NPF
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="LNPF.html">
   Latent NPF
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Reproducibility
 </span>
</p>
<ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="../reproducibility/Datasets.html">
   Datasets
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../reproducibility/CNP.html">
   CNP
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../reproducibility/AttnCNP.html">
   AttnCNP
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../reproducibility/ConvCNP.html">
   ConvCNP
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../reproducibility/LNP.html">
   LNP
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../reproducibility/AttnLNP.html">
   AttnLNP
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../reproducibility/ConvLNP.html">
   ConvLNP
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../reproducibility/Losses.html">
   LNPF Losses
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Reference
 </span>
</p>
<ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="../zbibliography.html">
   Bibliography
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference external" href="https://github.com/YannDubs/Neural-Process-Family">
   GitHub Repo
   <i class="fas fa-external-link-alt">
   </i>
  </a>
 </li>
</ul>

</nav>

 <!-- To handle the deprecated key -->

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="row topbar fixed-top container-xl">
    <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show">
    </div>
    <div class="col pl-2 topbar-main">
        
        <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
            data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
            aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
            title="Toggle navigation" data-toggle="tooltip" data-placement="left">
            <i class="fas fa-bars"></i>
            <i class="fas fa-arrow-left"></i>
            <i class="fas fa-arrow-up"></i>
        </button>
        
        <div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    
    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="../_sources/text/CNPF.md"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.md</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
            onClick="window.print()" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
    
</div>
        <!-- Source interaction buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Connect with source repository"><i class="fab fa-github"></i></button>
    <div class="dropdown-buttons sourcebuttons">
        <a class="repository-button"
            href="https://github.com/YannDubs/Neural-Process-Family"><button type="button" class="btn btn-secondary topbarbtn"
                data-toggle="tooltip" data-placement="left" title="Source repository"><i
                    class="fab fa-github"></i>repository</button></a>
        
        
    </div>
</div>


        <!-- Full screen (wrap in <a> to have style consistency -->
        <a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
                data-placement="bottom" onclick="toggleFullScreen()" title="Fullscreen mode"><i
                    class="fas fa-expand"></i></button></a>

        <!-- Launch buttons -->

    </div>

    <!-- Table of contents -->
    <div class="d-none d-md-block col-md-2 bd-toc show">
        
        <div class="tocsection onthispage pt-5 pb-3">
            <i class="fas fa-list"></i> Contents
        </div>
        <nav id="bd-toc-nav">
            <ul class="nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#overview">
   Overview
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#conditional-neural-process-cnp">
   Conditional Neural Process (CNP)
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#attentive-conditional-neural-process-attncnp">
   Attentive Conditional Neural Process (AttnCNP)
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#generalisation-and-extrapolation">
   Generalisation and Extrapolation
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#convolutional-conditional-neural-process-convcnp">
   Convolutional Conditional Neural Process (ConvCNP)
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#translation-equivariance-te">
     Translation Equivariance (TE)
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id10">
     ConvCNP
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#issues-with-the-cnpf">
   Issues With the CNPF
  </a>
 </li>
</ul>

        </nav>
        
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
        
              <div>
                
  <div class="section" id="conditional-npf">
<h1>Conditional NPF<a class="headerlink" href="#conditional-npf" title="Permalink to this headline">¶</a></h1>
<div class="section" id="overview">
<h2>Overview<a class="headerlink" href="#overview" title="Permalink to this headline">¶</a></h2>
<div class="figure align-default" id="graph-model-cnpf">
<a class="reference internal image-reference" href="../_images/graphical_model_CNPF.png"><img alt="../_images/graphical_model_CNPF.png" src="../_images/graphical_model_CNPF.png" style="width: 20em;" /></a>
<p class="caption"><span class="caption-number">Fig. 6 </span><span class="caption-text">Probabilistic graphical model for the Conditional NPF.</span><a class="headerlink" href="#graph-model-cnpf" title="Permalink to this image">¶</a></p>
</div>
<p>The key design choice for NPs is how to model the predictive distribution <span class="math notranslate nohighlight">\(p_{\theta}( \mathbf{y}_\mathcal{T} | \mathbf{x}_\mathcal{T}; \mathcal{C})\)</span>.
In particular, we require the predictive distributions to be consistent with each other for different <span class="math notranslate nohighlight">\(\mathbf{x}_\mathcal{T}\)</span>, as discussed on the <a class="reference internal" href="Intro.html#stochastic-processes"><span class="std std-ref">previous page</span></a>.
A simple way to ensure this is to use a predictive distribution that is <em>factorised</em> conditioned on the context set (as illustrated in <a class="reference internal" href="#graph-model-cnpf"><span class="std std-numref">Fig. 6</span></a>).
In other words, conditioned on the context set <span class="math notranslate nohighlight">\(\mathcal{C}\)</span>, the prediction at each target location is <em>independent</em> of predictions at other target locations.
We can concisely express this assumption as:</p>
<div class="math notranslate nohighlight" id="equation-conditional-predictives">
<span class="eqno">(1)<a class="headerlink" href="#equation-conditional-predictives" title="Permalink to this equation">¶</a></span>\[\begin{align}
p_{\theta}( \mathbf{y}_\mathcal{T} | \mathbf{x}_\mathcal{T}; \mathcal{C}) = \prod_{t=1}^{T} p_{\theta} \left( y^{(t)} | x^{(t)}; \mathcal{C} \right).
\end{align}\]</div>
<div class="hint dropdown admonition">
<p class="admonition-title">Advanced<span class="math notranslate nohighlight">\(\qquad\)</span>Factorisation <span class="math notranslate nohighlight">\(\implies\)</span> Consistency</p>
<p>We show that the CNPF predictive distribution specifies a consistent stochastic processes, given a fixed context set <span class="math notranslate nohighlight">\(\mathcal{C}\)</span>. Recall that we require consistency under <em>marginalisation</em> and <em>permutation</em>.</p>
<p>(Consistency under permutation) Let <span class="math notranslate nohighlight">\(\mathbf{x}_{\mathcal{T}} = \{ x^{(t)} \}_{t=1}^T\)</span> be the target inputs and <span class="math notranslate nohighlight">\(\pi\)</span> be any permutation of <span class="math notranslate nohighlight">\(\{1, ..., T\}\)</span>. Then the predictive density is:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align}
    p_\theta(y^{(1)}, ..., y^{(T)} | x^{(1)}, ..., x^{(T)}; \mathcal{C}) &amp;= \prod_{t=1}^{T} p_\theta( y^{(t)} | x^{(t)}; R) \\
    &amp;= \prod_{t=1}^{T} p_\theta( y^{(\pi(t))} | x^{(\pi(t))}; R) \\
    &amp;= p_\theta(y^{(\pi(1))}, ..., y^{(\pi(T))} | x^{(\pi(1))}, ..., x^{(\pi(T))}; \mathcal{C})
\end{align}
\end{split}\]</div>
<p>since multiplication is commutative.</p>
<p>(Consistency under marginalisation) Consider two target inputs, <span class="math notranslate nohighlight">\(x^{(1)}, x^{(2)}\)</span>. Then by marginalising out the second target output, we get:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align}
    \int p_\theta(y^{(1)}, y^{(2)}| x^{(1)}, x^{(2)}; \mathcal{C}) \, \mathrm{d}y^{(2)} &amp;= \int p_\theta(y^{(1)}| x^{(1)}; R)p_\theta(y^{(2)}| x^{(2)}; R) \, \mathrm{d}y^{(2)} \\
    &amp;= p_\theta(y^{(1)}| x^{(1)}; R) \int p_\theta(y^{(2)}| x^{(2)}; R) \, \mathrm{d}y^{(2)}\\
    &amp;= p_\theta(y^{(1)}| x^{(1)}; \mathcal{C})
\end{align}
\end{split}\]</div>
<p>which shows that the predictive distribution obtained by querying the CNPF member at <span class="math notranslate nohighlight">\(x^{(1)}\)</span> is the same as that obtained by querying it at <span class="math notranslate nohighlight">\(x^{(1)}, x^{(2)}\)</span> and then marginalising out the second target point. Of course, the same idea works with collections of any size, and marginalising any subset of the variables.</p>
</div>
<p>We refer to the sub-family of the NPF containing members that employ this factorisation assumption as the <em>conditional</em> Neural Process (sub-)Family (CNPF).
A typical (though not necessary) choice is to set each <span class="math notranslate nohighlight">\(p_{\theta} \left( y^{(t)} | x^{(t)}; \mathcal{C} \right)\)</span> to be a Gaussian density.
Now, recall that one guiding principle of the NPF is to encode the context set <span class="math notranslate nohighlight">\(\mathcal{C}\)</span> into a global representation <span class="math notranslate nohighlight">\(R\)</span>, and then use a decoder to parametrise each <span class="math notranslate nohighlight">\(p_{\theta} \left( y^{(t)} | x^{(t)}; \mathcal{C} \right)\)</span>.
Putting these together, we can express the predictive distribution of CNPF members as</p>
<div class="math notranslate nohighlight" id="equation-formal">
<span class="eqno">(2)<a class="headerlink" href="#equation-formal" title="Permalink to this equation">¶</a></span>\[\begin{split}\begin{align}
p_{\theta}(\mathbf{y}_\mathcal{T} | \mathbf{x}_\mathcal{T}; \mathcal{C})
&amp;= p_{\theta}(\mathbf{y}_\mathcal{T} | \mathbf{x}_\mathcal{T}; R) &amp; \text{Encoding}  \\
&amp;= \prod_{t=1}^{T} p_{\theta}(y^{(t)} |  x^{(t)}; R)  &amp; \text{Factorisation}\\
&amp;= \prod_{t=1}^{T} \mathcal{N} \left( y^{(t)};  \mu^{(t)}, \sigma^{2(t)} \right) &amp; \text{Gaussianity}
\end{align}\end{split}\]</div>
<p>Where:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align}
R
&amp;:= \mathrm{Enc}_{\theta}\left(\mathcal{C} \right) &amp; \text{Encoding} \\
(\mu^{(t)},\sigma^{2(t)})
&amp;:= \mathrm{Dec}_{\theta}(R,x^{(t)}) &amp; \text{Decoding}
\end{align}
\end{split}\]</div>
<p>CNPF members make an important tradeoff.
On one hand, the factorisation assumption places a severe restriction on the class of predictive stochastic processes we can model.
As discussed at the end of this chapter, this has important consequences, such as the inability of the CNPF to produce coherent samples.
On the other hand, the factorisation assumption makes evaluation of the predictive likelihoods analytically tractable.
This means we can employ a simple maximum-likelihood procedure to train the model parameters, i.e., training amounts to directly maximising the log-likelihood <span class="math notranslate nohighlight">\(\log p_{\theta}(\mathbf{y}_\mathcal{T} | \mathbf{x}_\mathcal{T}; \mathcal{C})\)</span>, as discussed on the <a class="reference internal" href="Intro.html#meta-training"><span class="std std-ref">previous page</span></a>.</p>
<p>Now that we’ve given an overview of the entire CNPF, we’ll discuss three particular members: the Conditional Neural Process (CNP), Attentive Conditional Neural Process (AttnCNP), and the Convolutional Conditional Neural Process (ConvCNP). Each member of the CNPF can be broadly distinguished by:</p>
<ul class="simple">
<li><p>The encoder <span class="math notranslate nohighlight">\(\mathrm{Enc}_{\theta}: \mathcal{C} \mapsto R\)</span>, which has to be permutation invariant to treat <span class="math notranslate nohighlight">\(\mathcal{C}\)</span> as a set.</p></li>
<li><p>The decoder <span class="math notranslate nohighlight">\(\mathrm{Dec}_{\theta}: R,x^{(t)} \mapsto \mu^{(t)},\sigma^{2(t)}\)</span>, which parametrizes the predictive distribution at location <span class="math notranslate nohighlight">\(x^{(t)}\)</span> using the global representation <span class="math notranslate nohighlight">\(R\)</span>.</p></li>
</ul>
<p>We begin by describing the Conditional Neural Process, arguably the simplest member of the CNPF, and the first considered in the literature.</p>
</div>
<div class="section" id="conditional-neural-process-cnp">
<span id="cnp"></span><h2>Conditional Neural Process (CNP)<a class="headerlink" href="#conditional-neural-process-cnp" title="Permalink to this headline">¶</a></h2>
<p>The main idea of the Conditional Neural Process (CNP) <a class="bibtex reference internal" href="../zbibliography.html#garnelo2018conditional" id="id1">[GRM+18]</a> is to enforce permutation invariance of the encoder by first <em>locally</em> encoding each context input-output pair into <span class="math notranslate nohighlight">\(R^{(c)}\)</span>, and then <em>aggregating</em> the local encodings into a global representation, <span class="math notranslate nohighlight">\(R\)</span>,  of the context set using a commutative operation.
Specifically, the local encoder is a feedforward multi-layer perceptron (MLP), while the aggregator is a mean pooling.
The decoder is simply an MLP that takes as input the concatenation of the representation and the target input, <span class="math notranslate nohighlight">\([R; x^{(t)}]\)</span>, and outputs a predictive mean <span class="math notranslate nohighlight">\(\mu^{(t)}\)</span> and variance <span class="math notranslate nohighlight">\(\sigma^{2(t)}\)</span>.</p>
<div class="dropdown admonition">
<p class="admonition-title">Implementation<span class="math notranslate nohighlight">\(\qquad\)</span>Log Std</p>
<p>Typically, we parameterise <span class="math notranslate nohighlight">\(\mathrm{Dec}_{\theta}\)</span> as outputting <span class="math notranslate nohighlight">\((\mu^{(t)}, \log \sigma^{(t)})\)</span>, i.e., the <em>log</em> standard deviation, so as to ensure that no negative variances occur.</p>
</div>
<div class="figure align-default" id="computational-graph-cnps-text">
<a class="reference internal image-reference" href="../_images/computational_graph_CNPs.png"><img alt="Computational graph CNP" src="../_images/computational_graph_CNPs.png" style="width: 25em;" /></a>
<p class="caption"><span class="caption-number">Fig. 7 </span><span class="caption-text">Computational graph for CNPs.</span><a class="headerlink" href="#computational-graph-cnps-text" title="Permalink to this image">¶</a></p>
</div>
<p>To summarise, the CNP is defined by the following design choices (see <a class="reference internal" href="#computational-graph-cnps-text"><span class="std std-numref">Fig. 7</span></a>):</p>
<ul class="simple">
<li><p>Encoding: <span class="math notranslate nohighlight">\(R = \mathrm{Enc}_{\theta}(\mathcal{C}) = \frac{1}{C} \sum_{c=1}^{C} \mathrm{MLP} \left( [x^{(c)}; y^{(c)}] \right)\)</span> .</p></li>
<li><p>Decoding: <span class="math notranslate nohighlight">\((\mu^{(t)}, \sigma^{2(t)}) = \mathrm{Dec}_{\theta}(R,x^{(t)}) =  \mathrm{MLP} \left( [R,x^{(t)}] \right)\)</span>.</p></li>
</ul>
<p>Note that the encoder is permutation invariant due to the commutativity of the sum operation, i.e., the order does not matter.
Importantly, if the local encoder and the decoder were universal function approximators (think of “infinitely wide” MLP and unconstrained dimension of <span class="math notranslate nohighlight">\(R\)</span>) the CNP would essentially be able to predict any mean <span class="math notranslate nohighlight">\(\mu^{(t)}\)</span> and variance <span class="math notranslate nohighlight">\(\sigma^{2(t)}\)</span> thanks to the local encoder+aggregator (DeepSets; <a class="bibtex reference internal" href="../zbibliography.html#zaheer2017deep" id="id2">[ZKR+17]</a>).</p>
<div class="dropdown tip admonition">
<p class="admonition-title">Advanced<span class="math notranslate nohighlight">\(\qquad\)</span>Universality and DeepSets</p>
<p>There is a strong relationship between this local encoder+aggregator and a class of models known as DeepSets networks introduced by Zaheer et al.<a class="bibtex reference internal" href="../zbibliography.html#zaheer2017deep" id="id3">[ZKR+17]</a>.
In particular, Zaheer et al. show that (subject to certain conditions) <em>any</em> continuous function operating on sets (<span class="math notranslate nohighlight">\(S\)</span>) can be expressed as:</p>
<div class="math notranslate nohighlight">
\[
\begin{align}
  f(S) = \rho \left( \sum_{s \in S} \phi(s) \right),
\end{align}
\]</div>
<p>for appropriate functions <span class="math notranslate nohighlight">\(\rho\)</span> and <span class="math notranslate nohighlight">\(\phi\)</span>.
This is known as a “sum decomposition” or “Deep Sets encoding”.
This result tells us that, as long as <span class="math notranslate nohighlight">\(\rho\)</span> and <span class="math notranslate nohighlight">\(\phi\)</span> are universal function approximators, this sum-decomposition can be done without loss of generality in terms of the class of permutation-invariant maps that can be expressed.</p>
<p>CNPs make heavy use this type of architecture.
To highlight the similarities, we can express each of the mean and standard deviation functions of the CNP as</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align}
  \mu^{(t)} = f_\mu \left( x^{(t)}; \mathcal{C} \right) &amp;= \rho_\mu \left( \left[x^{(t)}; \frac{1}{|\mathcal{C}|} \sum_{(x, y) \in \mathcal{C}} \phi([x;y])\right] \right), \\
  \sigma^{2(t)} = f_{\sigma^2} \left( x^{(t)}; \mathcal{C} \right) &amp;= \rho_\sigma \left( \left[x^{(t)}; \frac{1}{|\mathcal{C}|} \sum_{(x, y) \in \mathcal{C}} \phi([x;y])\right] \right)
\end{align}
\end{split}\]</div>
<p>where <span class="math notranslate nohighlight">\([\cdot ; \cdot ]\)</span> is the concatenation operation.
In fact, the only difference between the CNP form and the DeepSets of Zaheer et al. is the concatenation operation in <span class="math notranslate nohighlight">\(\rho\)</span>, and the use of the mean for aggregation, rather than summation.
It is possible to leverage this relationship to show that CNPs can recover any continuous mean and variance functions, which provides justification for the proposed architecture.</p>
</div>
<p><a class="reference internal" href="#cnp-cnpf"><span class="std std-numref">Fig. 8</span></a> shows a schematic animation of the forward pass of a CNP.
We see that every <span class="math notranslate nohighlight">\((x, y)\)</span> pair in the context set (here with three datapoints) is locally encoded by an MLP <span class="math notranslate nohighlight">\(e\)</span>.
The local encodings <span class="math notranslate nohighlight">\(\{r_1, r_2, r_3\}\)</span> are then aggregated by a mean pooling <span class="math notranslate nohighlight">\(a\)</span> to a global representation <span class="math notranslate nohighlight">\(r\)</span>.
Finally, the global representation <span class="math notranslate nohighlight">\(r\)</span> is fed along with the target input <span class="math notranslate nohighlight">\(x_{T}\)</span> into a decoder MLP <span class="math notranslate nohighlight">\(d\)</span> to yield the mean and variance of the predictive distribution of the target output <span class="math notranslate nohighlight">\(y\)</span>.</p>
<div class="figure align-default" id="cnp-cnpf">
<a class="reference internal image-reference" href="../_images/NPFs.gif"><img alt="Schematic representation of CNP forward pass." src="../_images/NPFs.gif" style="width: 30em;" /></a>
<p class="caption"><span class="caption-number">Fig. 8 </span><span class="caption-text">Schematic representation of the forward pass of members of the CNP taken from <a class="reference external" href="https://www.martagarnelo.com/conditional-neural-processes">Marta Garnelo</a>. <span class="math notranslate nohighlight">\(e\)</span> is the local encoder MLP, <span class="math notranslate nohighlight">\(a\)</span> is a mean-pooling aggregation, <span class="math notranslate nohighlight">\(d\)</span> is the decoder MLP.</span><a class="headerlink" href="#cnp-cnpf" title="Permalink to this image">¶</a></p>
</div>
<p>Notice that the computational cost of making predictions for <span class="math notranslate nohighlight">\(T\)</span> target points conditioned on <span class="math notranslate nohighlight">\(C\)</span> context points with this design is <span class="math notranslate nohighlight">\(\mathcal{O}(T+C)\)</span>.
Indeed, each <span class="math notranslate nohighlight">\((x,y)\)</span> pair of the context set is encoded independently (<span class="math notranslate nohighlight">\(\mathcal{O}(C)\)</span>) and the representation <span class="math notranslate nohighlight">\(R\)</span> can then be re-used for predicting at each target location (<span class="math notranslate nohighlight">\(\mathcal{O}(T)\)</span>).
This means that once trained, CNPs are much more efficient than GPs (which scale as <span class="math notranslate nohighlight">\(\mathcal{O}((C+T)^3)\)</span>).</p>
<p>Let’s see what prediction using a CNP looks like in practice.
We first consider a simple 1D regression task trained on samples from a GP with a radial basis function (RBF) kernel (data details in <a class="reference internal" href="../reproducibility/Datasets.html"><span class="doc">Datasets Notebook</span></a>).
Besides providing useful (and aesthetically pleasing) visualisations, the GPs admit closed form posterior predictive distributions, which allow us to compare to the “best possible” distributions for a given context set.
In particular, if the CNP was “perfect”, it would exactly match the predictions of the oracle GP.</p>
<div class="figure align-default" id="cnp-rbf-text">
<a class="reference internal image-reference" href="../_images/CNP_rbf.gif"><img alt="CNP on GP with RBF kernel" src="../_images/CNP_rbf.gif" style="width: 35em;" /></a>
<p class="caption"><span class="caption-number">Fig. 9 </span><span class="caption-text">Predictive distribution of a CNP (the blue line represents the predicted mean function and the shaded region shows a standard deviation on each side <span class="math notranslate nohighlight">\([\mu-\sigma,\mu+\sigma]\)</span>) and the oracle GP (green line represents the ground truth mean and the dashed line show a standard deviations on each side) with RBF kernel.</span><a class="headerlink" href="#cnp-rbf-text" title="Permalink to this image">¶</a></p>
</div>
<p><a class="reference internal" href="#cnp-rbf-text"><span class="std std-numref">Fig. 9</span></a> provides the predictive distribution for a CNP trained on many samples from such a GP.
The figure demonstrates that the CNP performs quite well in this setting.
As more data is observed, the predictions become tighter, as we would hope.
Moreover, we can see that the CNP predictions quite accurately track the ground truth predictive distribution.</p>
<p>That being said, we can see some signs of underfitting: for example, the predictive mean does not pass through all the context points, despite there being no noise in the data-generating distribution.
The underfitting becomes clear when considering more complicated kernels, such as a periodic kernel (i.e. GPs generating random periodic functions as seen in <a class="reference internal" href="../reproducibility/Datasets.html#samples-from-gp"><span class="std std-ref">Datasets Notebook</span></a>).
One thing we can notice about the ground truth GP predictions is that it leverages the periodic structure in its predictions.</p>
<div class="figure align-default" id="cnp-periodic-text">
<a class="reference internal image-reference" href="../_images/CNP_periodic.gif"><img alt="CNP on GP with Periodic kernel" src="../_images/CNP_periodic.gif" style="width: 35em;" /></a>
<p class="caption"><span class="caption-number">Fig. 10 </span><span class="caption-text">Posterior predictive of a CNP (Blue line for the mean with shaded area for <span class="math notranslate nohighlight">\([\mu-\sigma,\mu+\sigma]\)</span>) and the oracle GP (Green line for the mean with dotted lines for +/- standard deviation) with Periodic kernel.</span><a class="headerlink" href="#cnp-periodic-text" title="Permalink to this image">¶</a></p>
</div>
<p>In contrast, we see that the CNP completely fails to model the predictive distribution: the mean function is overly smooth and hardly passes through the context points.
Moreover, it seems that no notion of periodicity has been learned in the predictions.
Finally, the uncertainty seems constant, and is significantly overestimated everywhere.
It seems that the CNP has failed to learn the more complex structure of the optimal predictive distribution for this kernel.</p>
<p>Let’s now test the CNP (same architecture) on a more interesting task, one where we do <em>not</em> have access to the ground truth predictive distribution: image completion.
Note that NPs can be used to model images, as an image can be viewed as a function from pixel locations to pixel intensities or RGB channels— expand the dropdown below if this is not obvious.</p>
<div class="note dropdown admonition">
<p class="admonition-title">Note<span class="math notranslate nohighlight">\(\qquad\)</span>Images as Functions</p>
<p>In the Neural-Process literature, and in this tutorial, we view images as real-valued <em>functions</em> on the two-dimensional plane.
Each input <span class="math notranslate nohighlight">\(x\)</span> is a two-dimensional vector denoting pixel location in the image, and each <span class="math notranslate nohighlight">\(y\)</span> is a real number representing pixel intensity (or a three-dimensional vector for RGB images). <a class="reference internal" href="#images-as-functions-text"><span class="std std-numref">Fig. 11</span></a> illustrates how to interpret an MNIST image as a function:</p>
<div class="figure align-default" id="images-as-functions-text">
<a class="reference internal image-reference" href="../_images/images_as_functions.png"><img alt="Viewing images as functions" src="../_images/images_as_functions.png" style="width: 30em;" /></a>
<p class="caption"><span class="caption-number">Fig. 11 </span><span class="caption-text">Viewing images as functions from <span class="math notranslate nohighlight">\(\mathbb{Z}^2 \to \mathbb{R}\)</span>. Figure from <a class="bibtex reference internal" href="../zbibliography.html#garnelo2018neural" id="id4">[GSR+18]</a>.</span><a class="headerlink" href="#images-as-functions-text" title="Permalink to this image">¶</a></p>
</div>
</div>
<p>During meta-training, we treat each image as a sampled function, and split the image into context and target pixels. At test time, we can feed in a new context set and query the CNP at all the pixel locations, to interpolate the missing values / targets in the image. <a class="reference internal" href="#cnp-img-interp-text"><span class="std std-numref">Fig. 12</span></a> shows the results:</p>
<div class="figure align-default" id="cnp-img-interp-text">
<a class="reference internal image-reference" href="../_images/CNP_img_interp1.gif"><img alt="CNP on CelebA and MNIST" src="../_images/CNP_img_interp1.gif" style="width: 30em;" /></a>
<p class="caption"><span class="caption-number">Fig. 12 </span><span class="caption-text">Posterior predictive of a CNP on CelebA <span class="math notranslate nohighlight">\(32\times32\)</span> and MNIST.</span><a class="headerlink" href="#cnp-img-interp-text" title="Permalink to this image">¶</a></p>
</div>
<p>These results are quite impressive, however there are still some signs of underfitting. In particular, the interpolations are not very sharp, and do not totally resemble the ground truth image even when there are many context points. Nevertheless, this experiment demonstrates the power of neural processes: they can be applied out-of-the-box to learn this complicated structure directly from data, something that would be very difficult with a GP.</p>
<p>One potential solution to the overfitting problem, motivated by the universality of CNPs, is to increase the capacity of the networks <span class="math notranslate nohighlight">\(\mathrm{Enc}_{\theta}\)</span> and <span class="math notranslate nohighlight">\(\mathrm{Dec}_{\theta}\)</span>, as well as increase the dimensionality of <span class="math notranslate nohighlight">\(R\)</span>.
Unfortunately, it turns out that the CNP’s modelling power scales quite poorly with the capacity of its networks.
A more promising avenue, which we explore next, is to consider the <em>inductive biases</em> of its architectures.</p>
<div class="note admonition">
<p class="admonition-title">Note</p>
<p>Model details and more plots, along with code for constructing and training CNPs, can be found in <a class="reference internal" href="../reproducibility/CNP.html"><span class="doc">CNP Notebook</span></a>.
We also provide pretrained models to play around with.</p>
</div>
</div>
<div class="section" id="attentive-conditional-neural-process-attncnp">
<span id="attncnp"></span><h2>Attentive Conditional Neural Process (AttnCNP)<a class="headerlink" href="#attentive-conditional-neural-process-attncnp" title="Permalink to this headline">¶</a></h2>
<p>One possible explanation for CNP’s underfitting is that all points in the target set share a <em>single</em> global representation <span class="math notranslate nohighlight">\(R\)</span> of the context set, i.e., <span class="math notranslate nohighlight">\(R\)</span> is <em>independent</em> of the location of the target input.
This implies that all points in the context set are given the same “importance”, regardless of the location at which a prediction is being made.
For example, CNPs struggle to take advantage of the fact that if a target point is very close to a context point, they will often both have similar values. One possible solution is to use a <em>target-specific</em> representation <span class="math notranslate nohighlight">\(R^{(t)}\)</span>.</p>
<p>To achieve this, Kim et al. <a class="bibtex reference internal" href="../zbibliography.html#kim2019attentive" id="id5">[KMS+19]</a> propose the Attentive CNP (AttnCNP<a class="footnote-reference brackets" href="#id16" id="id6">1</a>), which replace CNPs’ mean aggregation by an <em>attention mechanism</em> <a class="bibtex reference internal" href="../zbibliography.html#bahdanau2014neural" id="id7">[BCB15]</a>.
There are many great resources available about the use of attention mechanisms in machine learning (e.g. <a class="reference external" href="https://distill.pub/2016/augmented-rnns/#attentional-interfaces">Distill’s interactive visualisation</a>, <a class="reference external" href="https://lilianweng.github.io/lil-log/2018/06/24/attention-attention.html">Lil’Log</a>, or the <a class="reference external" href="http://jalammar.github.io/illustrated-transformer/">Illustrated Transformer</a>), and we encourage readers unfamiliar with the concept to look through these.
For our purposes, it suffices to think of attention mechanisms as learning to <em>attend</em> to specific parts of an input that are particularly relevant to the desired output, giving them more <em>weight</em> than others when making a prediction.
Specifically, the attention mechanism is a function <span class="math notranslate nohighlight">\(w_{\theta}(\cdot, \cdot)\)</span> that weights each context point (the keys) <em>for every target location</em> (the querries), <span class="math notranslate nohighlight">\(w_{\theta}(x^{(c)},x^{(t)})\)</span>.
The AttnCNP then replaces CNPs’ simple average by a (more general) weighted average which gives a larger weight to “important” context points.</p>
<p>To illustrate how attention can alleviate underfitting, imagine that our context set contains two observations with inputs <span class="math notranslate nohighlight">\(x^{(1)}, x^{(2)}\)</span> that are “very far” apart in input space. These observations (input-output pairs) are then mapped by the encoder to the local representations <span class="math notranslate nohighlight">\(R^{(1)}, R^{(2)}\)</span> respectively.
Intuitively, when making predictions close to <span class="math notranslate nohighlight">\(x^{(1)}\)</span>, we should focus on <span class="math notranslate nohighlight">\(R^{(1)}\)</span> and ignore <span class="math notranslate nohighlight">\(R^{(2)}\)</span>, since <span class="math notranslate nohighlight">\(R^{(1)}\)</span> contains much more information about this region of input space. An attention mechanism allows us to parameterise and generalise this intuition, and learn it directly from the data!</p>
<p>This gain in expressivity comes at the cost of increased computational complexity, from  <span class="math notranslate nohighlight">\(\mathcal{O}(T+C)\)</span> to <span class="math notranslate nohighlight">\(\mathcal{O}(T*C)\)</span>, as a representation of the context set now needs to be computed for each target point.</p>
<div class="attention admonition">
<p class="admonition-title">Functional Representation</p>
<p>Notice that the encoder <span class="math notranslate nohighlight">\(\mathrm{Enc}_{\theta}\)</span> does not take the target location <span class="math notranslate nohighlight">\(x^{(t)}\)</span> and can thus not directly predict a <em>target-specific</em> representation <span class="math notranslate nohighlight">\(R^{(t)}\)</span>.
To make the AttnCNP fit in an encoder – global representation – decoder framework we have to treat the global representation as a <em>function</em> of the form <span class="math notranslate nohighlight">\(R : \mathcal{X} \to \mathbb{R}^{dimR}\)</span> instead of a vector.
In the decoder, this function will be queried at the target position <span class="math notranslate nohighlight">\(x^{(t)}\)</span> to yield a target specific vector representation <span class="math notranslate nohighlight">\(R^{(t)} = R(x^{(t)})\)</span>.</p>
</div>
<p>To summarise, the AttnCNP is defined by the following design choices (see <a class="reference internal" href="#computational-graph-attncnps-text"><span class="std std-numref">Fig. 13</span></a>):</p>
<ul class="simple">
<li><p>Encoding: <span class="math notranslate nohighlight">\(R(\cdot) = \mathrm{Enc}_{\theta}(\mathcal{C}) = \sum_{c=1}^{C} w_{\theta} \left( x^{(c)}, \cdot \right) \mathrm{MLP} \left( [x^{(c)}; y^{(c)}] \right)\)</span> .</p></li>
<li><p>Decoding: <span class="math notranslate nohighlight">\((\mu^{(t)}, \sigma^{2(t)}) = \mathrm{Dec}_{\theta}(R, x^{(t)}) =  \mathrm{MLP} \left( [R(x^{(t)}),x^{(t)}] \right)\)</span>.</p></li>
</ul>
<p>Note that, as for the CNP, the encoder is permutation invariant due to the commutativity of the sum operation.</p>
<div class="figure align-default" id="computational-graph-attncnps-text">
<a class="reference internal image-reference" href="../_images/computational_graph_AttnCNPs.png"><img alt="Computational graph of AttnCNP" src="../_images/computational_graph_AttnCNPs.png" style="width: 25em;" /></a>
<p class="caption"><span class="caption-number">Fig. 13 </span><span class="caption-text">Computational graph for AttnCNPs.</span><a class="headerlink" href="#computational-graph-attncnps-text" title="Permalink to this image">¶</a></p>
</div>
<div class="dropdown note admonition">
<p class="admonition-title">Note<span class="math notranslate nohighlight">\(\qquad\)</span>Self-Attention</p>
<p>In the above discussion we only consider an attention mechanism between the target and context set (referred to as <em>cross-attention</em>).
In addition, AttnCNP often first uses an attention mechanism between context points: <em>self-attention</em>, i.e., the local encoding <span class="math notranslate nohighlight">\(\mathrm{MLP} \left( [x^{(c)}; y^{(c)}] \right)\)</span> is replaced by <span class="math notranslate nohighlight">\(\sum_{c'=1}^{C} w_{\theta} \left( x^{(c')}, x^{(c)} \right) \mathrm{MLP} \left( [x^{(c')}; y^{(c')}] \right)\)</span>.</p>
<p>Notice that this does not impact the permutation invariance of the predictions — if the context set is permuted, so will the set of local representations. When cross-attention is applied, the ordering of this set is irrelevant, hence the predictions are unaffected.</p>
<p>Using a self-attention layer will also increase the computational complexity to <span class="math notranslate nohighlight">\(\mathcal{O}(C(C+T))\)</span> as each context point will first attend to every other context point in <span class="math notranslate nohighlight">\(\mathcal{O}(C^2)\)</span> before the cross attention layer which is <span class="math notranslate nohighlight">\(\mathcal{O}(C*T)\)</span>.</p>
</div>
<p>Without further ado, let’s see how the AttnCNP performs in practice.
We will first evaluate it on GP regression with different kernels (RBF, Periodic, and Noisy Matern).</p>
<div class="figure align-default" id="attncnp-single-gp-text">
<a class="reference internal image-reference" href="../_images/AttnCNP_single_gp1.gif"><img alt="AttnCNP on GPs with RBF, Periodic, Noisy Matern kernel" src="../_images/AttnCNP_single_gp1.gif" style="width: 35em;" /></a>
<p class="caption"><span class="caption-number">Fig. 14 </span><span class="caption-text">Posterior predictive of AttnCNPs (Blue line for the mean with shaded area for <span class="math notranslate nohighlight">\([\mu-\sigma,\mu+\sigma]\)</span>) and the oracle GP (Green line for the mean with dotted lines for +/- standard deviation) with RBF, Periodic, and Noisy Matern kernel.</span><a class="headerlink" href="#attncnp-single-gp-text" title="Permalink to this image">¶</a></p>
</div>
<p><a class="reference internal" href="#attncnp-single-gp-text"><span class="std std-numref">Fig. 14</span></a> demonstrates that, as desired, AttnCNP alleviates many of the underfitting issues of the CNP, and generally performs much better on the challenging kernels.
However, looking closely at the resulting fits, we can still see some dissatisfying properties:</p>
<ul class="simple">
<li><p>The fit on the Periodic kernel is still not <em>great</em>.
In particular, we see that the mean and variance functions of the AttnCNP often fail to track the oracle GP, as they only partially leverage the periodic structure.</p></li>
<li><p>The posterior predictive of the AttnCNP has “kinks”, i.e., it is not very smooth. Notice that these kinks usually appear between 2 context points. This leads us to believe that they are a consequence of the AttnCNP abruptly changing its attention from one context point to the other.</p></li>
</ul>
<p>Overall, AttnCNP performs quite well in this setting.
Next, we turn our attention (pun intended) to the image setting:</p>
<div class="figure align-default" id="attncnp-img-interp-text">
<a class="reference internal image-reference" href="../_images/AttnCNP_img_interp.gif"><img alt="AttnCNP on CelebA and MNIST" src="../_images/AttnCNP_img_interp.gif" style="width: 30em;" /></a>
<p class="caption"><span class="caption-number">Fig. 15 </span><span class="caption-text">Posterior predictive of an AttnCNP for CelebA <span class="math notranslate nohighlight">\(32\times32\)</span> and MNIST.</span><a class="headerlink" href="#attncnp-img-interp-text" title="Permalink to this image">¶</a></p>
</div>
<p><a class="reference internal" href="#attncnp-img-interp-text"><span class="std std-numref">Fig. 15</span></a> illustrates the performance of the AttnCNP on image reconstruction tasks with CelebA (left) and MNIST (right). Note that the reconstructions are sharper than those for the CNP. Interestingly, when only a vertical or horizontal slice is shown, the ANP seems to “blur” out its reconstruction somewhat.</p>
<!-- The results are quite impressive, as the AttnCNP is able to learn complicated structure in the underlying process, and even produce descent reconstructions of structured obfuscation of the image that is different from the random context pixels seen during training.
The image experiments hammer home an important advantage of the NPF over other approaches to modelling stochastic processes:
the same architecture can scale to complicated underlying processes, learning the important properties from the data (rather than requiring intricate kernel design, as in the case of GPs).
Intuitively, we can think of NPF models as implicitly learning these properties, or kernels, from the data, while allowing us to bake in some inductive biases into the architecture of the model. -->
<div class="note admonition">
<p class="admonition-title">Note</p>
<p>Model details, training and more plots in <a class="reference internal" href="../reproducibility/AttnCNP.html"><span class="doc">AttnCNP Notebook</span></a>.
We also provide pretrained models to play around with.</p>
</div>
</div>
<div class="section" id="generalisation-and-extrapolation">
<span id="extrapolation"></span><h2>Generalisation and Extrapolation<a class="headerlink" href="#generalisation-and-extrapolation" title="Permalink to this headline">¶</a></h2>
<p>So far, we have seen that well designed CNPF members can flexibly model a range of stochastic processes by being trained from functions sampled from the desired stochastic process.
Next, we consider the question of <em>generalisation</em> and <em>extrapolation</em> with CNPF members.</p>
<p>Let’s begin by discussing these properties in GPs. Many GPs used in practice have a property known as <em>stationarity</em>. Roughly speaking, this means that the GP gives the same predictions regardless of the absolute position of the context set in input space — only relative position matters. One reason this is useful is that stationary GPs will make sensible predictions regardless of the range of the inputs you give it. For example, imagine performing time-series prediction. As time goes on, the input range of the data increases. Stationary GPs can handle this without any issues.</p>
<p>In contrast, one downside of the CNP and AttnCNP is that it learns predictions solely through the data that it is presented with during meta-training. If this data has a limited range in input space, then there is no reason to believe that the CNP or AttnCNP will be able to make sensible predictions when queried outside of this range. In fact, we know that neural networks are typically quite bad at generalising outside their training distribution, i.e., in the <em>out-of-distribution</em> (OOD) regime.</p>
<!-- One property of GPs is that they can condition predictions on data observed in any region of $X$-space.
A possible downside from the fact that neural processes learn how to model a stochastic process from a dataset, is that during training we must specify a bounded range of $X$ on which data is observed, since we can never sample data from an infinite range.
We know that neural networks are typically quite bad at generalising outside the training distribution, and so we might suspect that CNPF models will not exhibit this appealing property. -->
<p>Let’s first probe this question on the 1D regression experiments.
To do so, we examine what happens when the context and target set contains points located <em>outside</em> the training range.</p>
<div class="figure align-default" id="cnp-attncnp-rbf-extrap-text">
<a class="reference internal image-reference" href="../_images/CNP_AttnCNP_rbf_extrap.gif"><img alt="extrapolation of CNP on GPs with RBF kernel" src="../_images/CNP_AttnCNP_rbf_extrap.gif" style="width: 35em;" /></a>
<p class="caption"><span class="caption-number">Fig. 16 </span><span class="caption-text">Extrapolation of posterior predictive of CNP (Top) and AttnCNP (Bottom) and the oracle GP (Green) with RBF kernel. Left of the red vertical line is the training range, everything to the right is the “extrapolation range”.</span><a class="headerlink" href="#cnp-attncnp-rbf-extrap-text" title="Permalink to this image">¶</a></p>
</div>
<p><a class="reference internal" href="#cnp-attncnp-rbf-extrap-text"><span class="std std-numref">Fig. 16</span></a> clearly shows that the CNP and the AttnCNP  break as soon as the target and context points are outside the training range.
In other words, they are not able to model the fact that the RBF kernel is stationary, i.e., that the absolute position of target points is not important, but only their relative position to the context points.
Interestingly, they both fail in different ways: the CNP seems to fail for any target location, while the AttnCNP fails only when the target locations are in the extrapolation regime — suggesting that it can deal with context set extrapolation.</p>
<p>We can also observe this phenomenon in the image setting.
For example let us evaluate the CNP and AttnCNP on Zero Shot Multi MNIST (ZSMM) where the training set consists of translated MNIST examples, while the test images are larger canvases with 2 digits. Refer to <a class="reference internal" href="../reproducibility/Datasets.html"><span class="doc">Datasets Notebook</span></a> for training and testing examples.</p>
<div class="sphinx-bs container-fluid docutils">
<div class="row docutils">
<div class="d-flex col-lg-6 col-md-6 col-sm-12 col-xs-12 p-0 m-0 docutils">
<div class="card w-100 shadow-none border-0 docutils">
<div class="card-body docutils">
<div class="figure align-default" id="cnp-img-extrap-text">
<a class="reference internal image-reference" href="../_images/CNP_img_extrap.gif"><img alt="CNP on ZSMM" src="../_images/CNP_img_extrap.gif" style="width: 100%;" /></a>
<p class="caption"><span class="caption-number">Fig. 17 </span><span class="caption-text">Posterior predictive of an CNP for ZSMM.</span><a class="headerlink" href="#cnp-img-extrap-text" title="Permalink to this image">¶</a></p>
</div>
</div>
</div>
</div>
<div class="d-flex col-lg-6 col-md-6 col-sm-12 col-xs-12 p-0 m-0 docutils">
<div class="card w-100 shadow-none border-0 docutils">
<div class="card-body docutils">
<div class="figure align-default" id="attncnp-img-extrap-text">
<a class="reference internal image-reference" href="../_images/AttnCNP_img_extrap.gif"><img alt="AttnCNP on ZSMM" src="../_images/AttnCNP_img_extrap.gif" style="width: 100%;" /></a>
<p class="caption"><span class="caption-number">Fig. 18 </span><span class="caption-text">Posterior predictive of an AttnCNP for ZSMM.</span><a class="headerlink" href="#attncnp-img-extrap-text" title="Permalink to this image">¶</a></p>
</div>
</div>
</div>
</div>
</div>
</div>
<p>Again we see in <a class="reference internal" href="#cnp-img-extrap-text"><span class="std std-numref">Fig. 17</span></a> and <a class="reference internal" href="#attncnp-img-extrap-text"><span class="std std-numref">Fig. 18</span></a> that the models completely break in this generalisation task. They are unable to spatially extrapolate to multiple, uncentered digits.
This is likely not surprising to anyone who has worked with neural nets as the test set here is significantly different of the training set.
Despite the challenging nature of this task, it turns out that we can in fact construct NPs that perform well, by building in the appropriate inductive biases.
This leads us to our next CNPF member — the ConvCNP.</p>
</div>
<div class="section" id="convolutional-conditional-neural-process-convcnp">
<span id="convcnp"></span><h2>Convolutional Conditional Neural Process (ConvCNP)<a class="headerlink" href="#convolutional-conditional-neural-process-convcnp" title="Permalink to this headline">¶</a></h2>
<div class="caution admonition">
<p class="admonition-title">Disclaimer</p>
<p>The authors of this tutorial are co-authors on the ConvCNP paper.</p>
</div>
<div class="section" id="translation-equivariance-te">
<span id="te"></span><h3>Translation Equivariance (TE)<a class="headerlink" href="#translation-equivariance-te" title="Permalink to this headline">¶</a></h3>
<p>It turns out that the type of generalisation we are looking for — that the predictions of NPs depend on the <em>relative</em> position in input space of context and target points rather than the absolute one —
can be mathematically expressed as a property called <em>translation equivariance</em> (TE).
Intuitively, TE states that if our observations are shifted in input space (which may be time, as in audio waveforms, or spatial coordinates, as in image data), then the resulting predictions should be shifted by the same amount.
This simple inductive bias, when appropriate, is extremely effective.
For example, convolutional neural networks (CNNs) were explicitly designed to satisfy this property <a class="bibtex reference internal" href="../zbibliography.html#fukushima1982neocognitron" id="id8">[FM82]</a><a class="bibtex reference internal" href="../zbibliography.html#lecun1989backpropagation" id="id9">[LBD+89]</a>, making them the state-of-the-art architecture for spatially-structured data.</p>
<p>In <a class="reference internal" href="#translation-equivariance-text"><span class="std std-numref">Fig. 19</span></a>, we visualise translation equivariance in the setting of stochastic process prediction.
Here, we show stationary GP regression, which leads to translation equivariant predictions.
We can see that as the context set is <em>translated</em>, i.e., all the data points in <span class="math notranslate nohighlight">\(\mathcal{C}\)</span> are shifted in input space by the same amount, so is the resulting predictive distribution.
To achieve spatial generalisation, we would like this property also to hold for neural processes.</p>
<div class="figure align-default" id="translation-equivariance-text">
<a class="reference internal image-reference" href="../_images/te_vis_complete.png"><img alt="Translation equivariant mapping from dataset to predictive" src="../_images/te_vis_complete.png" style="width: 35em;" /></a>
<p class="caption"><span class="caption-number">Fig. 19 </span><span class="caption-text">Example of a translation equivariant mapping from a dataset to a predictive stochastic process.</span><a class="headerlink" href="#translation-equivariance-text" title="Permalink to this image">¶</a></p>
</div>
<div class="hint dropdown admonition">
<p class="admonition-title">Advanced<span class="math notranslate nohighlight">\(\qquad\)</span>Translation equivariance</p>
<p>More precisely, translation equivariance is a property of a <em>map</em> between two spaces, where each space has a notion of translation defined on it (more precisely, there is a <em>group action</em> of the translation group on each space). In the CNPF case, the input space <span class="math notranslate nohighlight">\(\mathcal{Z}\)</span> is the space of finite datasets, such that each <span class="math notranslate nohighlight">\(Z \in \mathcal{Z}\)</span> can be written as <span class="math notranslate nohighlight">\(Z = \{(x_n, y_n)\}_{n=1}^N\)</span> for some <span class="math notranslate nohighlight">\(N \in \mathbb{N}\)</span>. The output space <span class="math notranslate nohighlight">\(\mathcal{H}\)</span> is a space of continuous functions on <span class="math notranslate nohighlight">\(\mathcal{X}\)</span>, where <span class="math notranslate nohighlight">\(\mathcal{X}\)</span> is the input domain of the regression problem (e.g., time, spatial position). Think of <span class="math notranslate nohighlight">\(\mathcal{H}\)</span> as the set of all possible predictive mean and variance functions. The map acting from <span class="math notranslate nohighlight">\(\mathcal{Z}\)</span> to <span class="math notranslate nohighlight">\(\mathcal{H}\)</span> is then the CNPF member itself.</p>
<p>Let <span class="math notranslate nohighlight">\(\tau \in \mathcal{X}\)</span> be a translation vector. We define <em>translations</em> on each of these spaces as:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align}
T \colon \mathcal{X} \times \mathcal{Z} \to \mathcal{Z}; &amp; \qquad T_\tau Z = \{ (x_n + \tau, y_n) \}_{n=1}^N \\
T' \colon \mathcal{X} \times \mathcal{H} \to \mathcal{H}; &amp; \qquad T'_\tau h(x) = h( x - \tau).
\end{align}
\end{split}\]</div>
<p>Then, a mapping <span class="math notranslate nohighlight">\(f \colon \mathcal{Z} \to \mathcal{H}\)</span> is said to be <em>translation equivariant</em> if <span class="math notranslate nohighlight">\(f(T_\tau Z) = T'_\tau f(Z)\)</span> for all <span class="math notranslate nohighlight">\(\tau \in \mathcal{X}\)</span> and <span class="math notranslate nohighlight">\(Z \in \mathcal{Z}\)</span>. In other words, “shifting then predicting” gives the same result as “predicting then shifting”.</p>
</div>
</div>
<div class="section" id="id10">
<h3>ConvCNP<a class="headerlink" href="#id10" title="Permalink to this headline">¶</a></h3>
<p>This provides the central motivation behind the ConvCNP <a class="bibtex reference internal" href="../zbibliography.html#gordon2019convolutional" id="id11">[GBF+20]</a>: <em>baking TE into the CNPF</em>, whilst preserving its other desirable properties.
Specifically, we would like the encoder to be a TE map between the context set <span class="math notranslate nohighlight">\(\mathcal{C}\)</span> and a functional representation <span class="math notranslate nohighlight">\(R(\cdot)\)</span>, which as for AttnCNP will then be queried at the target location <span class="math notranslate nohighlight">\(R^{(t)}=R(x^{(t)})\)</span>.
In deep learning, the prime candidate for a TE encoder is a CNN.
There is however an issue: the inputs and outputs to a CNN are discrete signals (e.g. images) and thus cannot take as input sets nor can they be queried at continuous (target) location <span class="math notranslate nohighlight">\(x^{(t)}\)</span>. Gordon et al.
<a class="bibtex reference internal" href="../zbibliography.html#gordon2019convolutional" id="id12">[GBF+20]</a> solve this issue by introducing the SetConv layer, an operation which extends standard convolutions to sets and could be very useful beyond the NPF framework.</p>
<div class="attention admonition">
<p class="admonition-title">SetConv</p>
<p>Standard convolutional layers in deep learning take in a discrete signal/function (e.g. a <span class="math notranslate nohighlight">\(128\times128\)</span> monochrome image that can be seen as a function from <span class="math notranslate nohighlight">\(\{0, \dots , 127\}^2 \to [0,1]\)</span>) and outputs a discrete signal/function (e.g. another <span class="math notranslate nohighlight">\(128\times128\)</span> monochrome image).
The SetConv layer extends this operation to sets, i.e., it takes as input a set of continuous input-output pairs <span class="math notranslate nohighlight">\(\{(x^{(c)},y^{(c)})\}_{c=1}^{C}\)</span> (e.g. a time-series sampled at irregular points) and outputs a function that can be queried at continuous locations <span class="math notranslate nohighlight">\(x\)</span>.</p>
<div class="math notranslate nohighlight" id="equation-set-conv">
<span class="eqno">(3)<a class="headerlink" href="#equation-set-conv" title="Permalink to this equation">¶</a></span>\[\begin{split}\begin{align}
\mathrm{SetConv} \left( \{(x^{(c)},y^{(c)})\}_{c=1}^{C} \right)(x) = \sum_{c=1}^{C} \begin{bmatrix} 1 \\ y^{(c)} \end{bmatrix} w_{\theta} \left( x - x^{(c)} \right).
\end{align}\end{split}\]</div>
<p>Here, <span class="math notranslate nohighlight">\(w_{\theta}\)</span> is a function that maps the <em>distance between</em> <span class="math notranslate nohighlight">\(x^{(c)}\)</span> and <span class="math notranslate nohighlight">\(x\)</span> to a real number.
It is most often chosen to be an RBF: <span class="math notranslate nohighlight">\(w_{\theta}(r) = \exp(- \frac{\|r\|^2_2}{\ell^2} )\)</span>, where <span class="math notranslate nohighlight">\(\ell\)</span> is a learnable lengthscale parameter. You can think of this operation as simply placing Gaussian bumps down at every datapoint, similar to <a class="reference external" href="https://en.wikipedia.org/wiki/Kernel_density_estimation">Kernel Density Estimation</a>.</p>
<p>Note that the SetConv operation is permutation invariant due to the sum operation.Furthermore, it is very similar to an attention mechanism, the main difference being that:</p>
<ul class="simple">
<li><p>The weight only depends on the distance <span class="math notranslate nohighlight">\(x^{(c)}-x\)</span> rather than on their absolute values. This is the key for TE, which intuitively requires the mapping to only depend on relative positions rather than absolute ones.</p></li>
<li><p>We append a constant 1 to the value,  <span class="math notranslate nohighlight">\(\begin{bmatrix} 1 \\ y^{(c)} \end{bmatrix}\)</span>, which results in an additional <em>channel</em>. Intuitively, we can think of this additional channel — referred to as the <em>density channel</em> — as keeping track of where data was observed in <span class="math notranslate nohighlight">\(\mathcal{C}\)</span>.</p></li>
</ul>
<div class="note dropdown admonition">
<p class="admonition-title">Note<span class="math notranslate nohighlight">\(\qquad\)</span>Density Channel</p>
<p>To better understand the role of the density channel, consider a context set containing a point <span class="math notranslate nohighlight">\((x^{(c')}, y^{(c')})\)</span>, with <span class="math notranslate nohighlight">\(y^{(c')} = 0\)</span>. Without the density channel, this point would have no impact on the output of the SetConv:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align}
\mathrm{SetConv} \left( \{(x^{(c)},y^{(c)})\}_{c=1}^{C} \right)(x)
&amp;= \sum_{c=1}^{C} y^{(c)} w_{\theta} \left( x - x^{(c)} \right) \\
&amp;= \left( y^{(c')} w_{\theta} \left( x - x^{(c')} \right) \right) + \sum_{c \neq c'}  y^{(c)} w_{\theta} \left( x - x^{(c)} \right) \\
&amp;= 0 + \sum_{c \neq c'}  y^{(c)} w_{\theta} \left( x - x^{(c)} \right)
\end{align}
\end{split}\]</div>
<p>Hence, without a density channel, the encoder would be unable to distinguish between observing a context point with <span class="math notranslate nohighlight">\(y^{(c)} = 0\)</span>, and not observing a context point at all! With the density channel, the contribution of the point <span class="math notranslate nohighlight">\(c'\)</span> becomes non-zero, specifically it contributes <span class="math notranslate nohighlight">\(\begin{bmatrix} w_{\theta} \left( x - x^{(c)} \right) \\ 0 \end{bmatrix}\)</span>  to the predictions. This turns out to be important in practice, as well as in the theoretical proofs regarding the expressivity of the ConvCNP.</p>
</div>
<div class="note dropdown admonition">
<p class="admonition-title">Note<span class="math notranslate nohighlight">\(\qquad\)</span>Normalisation</p>
<p>It is common practice to use the density channel to <em>normalise</em> the output of the non-density channels (also called the <em>signal</em> channels), so that the functional encoding becomes:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align}
\mathrm{density}(x)
&amp;= \sum_{c=1}^{C}  w_{\theta}(x^{(c)} - x)\\
\mathrm{signal}(x)
&amp;= \sum_{c=1}^{C} y^{(c)} w_{\theta}(x^{(c)} - x) \\
\mathrm{SetConv}(\mathcal{C})(x) &amp;=  \begin{bmatrix} \mathrm{density}(x) \\ \mathrm{signal}(x) / \mathrm{density}(x) \end{bmatrix}
\end{align}
\end{split}\]</div>
<p>Intuitively, this ensures that the magnitude of the signal channel doesn’t blow up if there are a large number of context points at the same spot. The density channel of the ConvCNP encoder can be seen as (a scaled version of) a kernel density estimate, and the normalised signal channel can be seen as <a class="reference external" href="https://en.wikipedia.org/wiki/Kernel_regression#Nadaraya%E2%80%93Watson_kernel_regression">Nadaraya-Watson kernel regression</a>.</p>
</div>
<p>Note that if <span class="math notranslate nohighlight">\(x^{(c)},x^{(t)}\)</span> are discrete, the SetConv essentially recovers the standard convolutional layer, denoted Conv. For example, let <span class="math notranslate nohighlight">\(I\)</span> be a <span class="math notranslate nohighlight">\(128\times128\)</span> monochrome image, then</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\mathrm{SetConv}(\{(x^{(c)},I(x^{(c)}))\}_{x^{(c)} \in \{0, \dots , 127\}^2 })(x^{(t)}) = \begin{bmatrix} 1 \\ \mathrm{Conv}(I)[x^{t}] \end{bmatrix}
\end{split}\]</div>
<p>for all pixel locations <span class="math notranslate nohighlight">\(x^{(t)} \in \{0, \dots , 127\}^2 \)</span>, where <span class="math notranslate nohighlight">\(1\)</span> comes from the fact that the density channel is always <span class="math notranslate nohighlight">\(1\)</span> when their are no “missing values”.</p>
</div>
<p>Armed with this convolution mapping a set to continuous a function, we can use a CNN as our encoder by “wrapping it” around two SetConvs.
Specifically, the encoder of the ConvCNP first uses a SetConv to ensure that the encoder can take the context set <span class="math notranslate nohighlight">\(\mathcal{C}\)</span> as input.
The output of the SetConv (a continuous function) is then <em>discretised</em> — by evaluating it at an evenly spaced grid of input locations <span class="math notranslate nohighlight">\(\{ \mathrm{SetConv}\left( \mathcal{C} \right)(x^{(u)}) \}_{u=1}^U\)</span> — so that it can be  given as input to a CNN.
Finally the output of the CNN (a discrete function) is passed through an additional SetConv to obtain a continuous functional representation <span class="math notranslate nohighlight">\(R\)</span>.</p>
<div class="warning admonition">
<p class="admonition-title">Warning</p>
<p>The discretisation means that the resulting ConvCNP can only be approximately TE, where the quality of the approximation is controlled by the number of points <span class="math notranslate nohighlight">\(U\)</span>.
If the spacing between the grid points is <span class="math notranslate nohighlight">\(\Delta\)</span>, the ConvCNP would not be expected to be equivariant to shifts of the input that are smaller than <span class="math notranslate nohighlight">\(\Delta\)</span>.</p>
</div>
<p>Similarly to AttnCNP, the decoder applies the resulting functional representation to the target location to get a target specific representation <span class="math notranslate nohighlight">\(R^{(t)}=R(x^{(t)})\)</span>, which is then used by an MLP to parametrize the final Gaussian distribution.
The only difference with AttnCNP being that the MLP does not directly take <span class="math notranslate nohighlight">\(x^{(t)}\)</span> as input, to ensure that the ConvCNP is TE.</p>
<p>Putting everything together, we can define the ConvCNP using the following design choices (illustrated in <a class="reference internal" href="#computational-graph-convcnps-text"><span class="std std-numref">Fig. 20</span></a>):</p>
<ul class="simple">
<li><p>Encoding: <span class="math notranslate nohighlight">\(R(\cdot) = \mathrm{Enc}_{\theta}(\mathcal{C}) = \mathrm{SetConv} \left( \mathrm{CNN}\left(\{ \mathrm{SetConv}\left( \mathcal{C} \right)(x^{(u)}) \}_{u=1}^U \right) \right)\)</span> .</p></li>
<li><p>Decoding: <span class="math notranslate nohighlight">\((\mu^{(t)}, \sigma^{2(t)}) = \mathrm{Dec}_{\theta}(R, x^{(t)}) =  \mathrm{MLP} \left( R(x^{(t)}) \right)\)</span>.</p></li>
</ul>
<div class="figure align-default" id="computational-graph-convcnps-text">
<a class="reference internal image-reference" href="../_images/computational_graph_ConvCNPs.png"><img alt="Computational graph ConvCNP" src="../_images/computational_graph_ConvCNPs.png" style="width: 25em;" /></a>
<p class="caption"><span class="caption-number">Fig. 20 </span><span class="caption-text">Computational graph of ConvCNP.</span><a class="headerlink" href="#computational-graph-convcnps-text" title="Permalink to this image">¶</a></p>
</div>
<div class="caution dropdown admonition">
<p class="admonition-title">Disclaimer<span class="math notranslate nohighlight">\(\qquad\)</span> Encoder vs Decoder</p>
<p>Note that the separation of the ConvCNP into encoder and decoder is somewhat arbitrary. You could also view the encoder as the first SetConv, and the decoder as the CNN with the second SetConv, which is the view presented in the original ConvCNP paper.</p>
</div>
<p>Importantly, if the CNN was a universal function approximator (think about “infinite channels” in the CNN and <span class="math notranslate nohighlight">\(U \to \infty\)</span>) the ConvCNP would essentially be able to predict any mean <span class="math notranslate nohighlight">\(\mu^{(t)}\)</span> and variance <span class="math notranslate nohighlight">\(\sigma^{2(t)}\)</span> that can be predicted with a TE map (ConvDeepSets; <a class="bibtex reference internal" href="../zbibliography.html#gordon2019convolutional" id="id13">[GBF+20]</a>).</p>
<div class="dropdown hint admonition">
<p class="admonition-title">Advanced<span class="math notranslate nohighlight">\(\qquad\)</span>ConvDeepSets</p>
<p>Similar to how CNPs make heavy use of “DeepSet” networks, ConvCNPs rely on the <em>ConvDeepSet</em> architecture which was also proposed by Gordon et al. <a class="bibtex reference internal" href="../zbibliography.html#gordon2019convolutional" id="id14">[GBF+20]</a>.
ConvDeepSets map sets of the form <span class="math notranslate nohighlight">\(Z = \{(x_n, y_n)\}_{n=1}^N\)</span>, <span class="math notranslate nohighlight">\(N \in \mathbb{N}\)</span> to a space of functions <span class="math notranslate nohighlight">\(\mathcal{H}\)</span>, and have the following form</p>
<div class="math notranslate nohighlight">
\[
\begin{align}
f(Z) = \rho(E (Z)); \qquad E(Z) = \sum_{(x,y) \in Z} \phi(y) w_{\theta}(\cdot - x) \in \mathcal{H},
\end{align}
\]</div>
<p>for appropriate functions <span class="math notranslate nohighlight">\(\phi\)</span>, and <span class="math notranslate nohighlight">\(w_{\theta}\)</span>, and <span class="math notranslate nohighlight">\(\rho\)</span> translation equivariant.
The key result of Gordon et al. is to demonstrate that not only are functions of this form translation equivariant, but under mild conditions, <em>any</em> continuous and translation equivariant function on sets <span class="math notranslate nohighlight">\(Z\)</span> has a representation of this form.
This result is analagous to that of Zaheer et al., but extended to translation equivariant functions on sets.</p>
<!-- Gordon et al. rely on the universal approximators for $phi$, the notion of _interpolating kernels_ for $\psi$, and specify $\rho$ as universal approximators of continuous _and_ translation equivariant functions between function spaces. -->
<p>ConvCNPs then define the parameterisations of the mean and variance functions using ConvDeepSets.
In particular, ConvCNPs specify <span class="math notranslate nohighlight">\(\phi \colon y \mapsto (1, y)^T\)</span>, <span class="math notranslate nohighlight">\(\rho\)</span> as a CNN, and <span class="math notranslate nohighlight">\(w_{\theta}\)</span> as an RBF kernel, and use ConvDeepSets to parameterise the predictive mean and standard deviation functions.
Thus, ConvDeepSets motivate the architectural choices of the ConvCNP analagously to how DeepSets can be used to provide justification for the CNP architecture.</p>
</div>
<p><a class="reference internal" href="#convcnp-text"><span class="std std-numref">Fig. 21</span></a> shows a schematic animation of the forward pass of a ConvCNP.
We see that every <span class="math notranslate nohighlight">\((x, y)\)</span> pair in the context set (here with ten datapoints) goes through a SetConv.
After concatenting the density channel, we discretize both the signal and the density channel so that they can be used as input to a CNN.
The result then goes through a second SetConv to ouput a functional representation <span class="math notranslate nohighlight">\(R(\cdot)\)</span> which an be querried at any target location <span class="math notranslate nohighlight">\(x^{(t)}\)</span>.
Finally, the global representation evaluated at each target <span class="math notranslate nohighlight">\(R(x^{(t)})\)</span> is fed into a decoder MLP to yield the mean and variance of the predictive distribution of the target output <span class="math notranslate nohighlight">\(y\)</span>.</p>
<div class="figure align-default" id="convcnp-text">
<a class="reference internal image-reference" href="../_images/explain_convcnp_RBF_10cntxt_largelength.gif"><img alt="Schematic representation of ConvCNP forward pass." src="../_images/explain_convcnp_RBF_10cntxt_largelength.gif" style="width: 30em;" /></a>
<p class="caption"><span class="caption-number">Fig. 21 </span><span class="caption-text">Schematic representation of the forward pass of members of the ConvCNP.</span><a class="headerlink" href="#convcnp-text" title="Permalink to this image">¶</a></p>
</div>
<div class="note dropdown admonition">
<p class="admonition-title">Implementation<span class="math notranslate nohighlight">\(\qquad\)</span>On-the-grid ConvCNP</p>
<p>The architecture described so far is referred to as the “off-the-grid” ConvCNP in the original paper, since it can handle input observations at any point on the <span class="math notranslate nohighlight">\(x\)</span>-axis. However, we sometimes have situations where inputs end up regularly spaced on the <span class="math notranslate nohighlight">\(x\)</span>-axis. A prime example of this is in image data, where the input image lives on a regularly-spaced 2D grid. For this case, the authors propose the “on-the-grid” ConvCNP, which is a simplified version that is easier to implement.</p>
<p>In the on-the-grid ConvCNP, the input data already lives on a discretised grid. Hence, after appending a density channel, this can immediately be fed into a CNN, without the need for SetConv’s. Suppose we have an image, represented as an <span class="math notranslate nohighlight">\(H \times W\)</span> matrix. Within this image, we have <span class="math notranslate nohighlight">\(C\)</span> observed pixels and <span class="math notranslate nohighlight">\(HW - C\)</span> unobserved pixels. We represent an observed pixel with the vector <span class="math notranslate nohighlight">\([1, y^{(c)}]^T\)</span>, and an unobserved pixel with the vector <span class="math notranslate nohighlight">\([0, 0]^T\)</span>. As before, the first element of this vector is the density channel, and indicates that a datapoint has been observed.</p>
<p>To implement this, let the input image be <span class="math notranslate nohighlight">\(I \in \mathbb{R}^{H \times W}\)</span>. Let <span class="math notranslate nohighlight">\(M \in \mathbb{R}^{H \times W}\)</span> be a mask matrix, with <span class="math notranslate nohighlight">\(M_{i,j} = 1\)</span> if the pixel at location <span class="math notranslate nohighlight">\((i,j)\)</span> is in the context set, and <span class="math notranslate nohighlight">\(0\)</span> otherwise. Then we can compute the density channel as <span class="math notranslate nohighlight">\(\mathrm{density} = M\)</span> and the signal channel as <span class="math notranslate nohighlight">\(\mathrm{signal} = I \odot M\)</span>, where <span class="math notranslate nohighlight">\(\odot\)</span> denotes element-wise multiplication. We then stack these matrices as <span class="math notranslate nohighlight">\([\mathrm{density}, \mathrm{signal}]^T\)</span>. This can then be passed into a CNN, and the CNN can output one channel for the predictive mean, and another for the log predictive variance.</p>
</div>
<div class="dropdown note admonition">
<p class="admonition-title">Note<span class="math notranslate nohighlight">\(\qquad\)</span>Computational Complexity</p>
<p>Computing the discrete functional representation requires considering <span class="math notranslate nohighlight">\(C\)</span> points in the context set for each discretised function location which scales as <span class="math notranslate nohighlight">\(\mathcal{O}(U*C)\)</span>.
Similarly, computing the predictive at the target inputs scales as <span class="math notranslate nohighlight">\(\mathcal{O}(U*T)\)</span>. Finally, if the convolutional kernel has width <span class="math notranslate nohighlight">\(K\)</span>, the complexity of the CNN scales as <span class="math notranslate nohighlight">\(\mathcal{O}(U*K)\)</span> — here we are ignoring the depth and number of channels. Hence the computational complexity of inference in a ConvCNP is <span class="math notranslate nohighlight">\(\mathcal{O}(U(C+K+T))\)</span>. In the on-the-grid ConvCNP, the computational complexity is simply <span class="math notranslate nohighlight">\(\mathcal{O}(U*K)\)</span>, where <span class="math notranslate nohighlight">\(U\)</span> is the number of pixels for image data.</p>
<p>This shows that there is a trade-off: if the number of discretisation points <span class="math notranslate nohighlight">\(U\)</span> is too large then the computational cost will not be manageable, but if it is too small then ConvCNP will be only very “coarsely” TE.</p>
</div>
<p>Now that we have constructed a translation equivariant member of the CNPF, we can test it in the more challenging extrapolation regime.
We begin with the same set of GP experiments, but this time already including data observed from outside the original training range.</p>
<div class="figure align-default" id="convcnp-single-gp-extrap-text">
<a class="reference internal image-reference" href="../_images/ConvCNP_single_gp_extrap1.gif"><img alt="../_images/ConvCNP_single_gp_extrap1.gif" src="../_images/ConvCNP_single_gp_extrap1.gif" style="width: 35em;" /></a>
<p class="caption"><span class="caption-number">Fig. 22 </span><span class="caption-text">Extrapolation (red dashes) of posterior predictive of ConvCNPs (Blue) and the oracle GP (Green) with (top) RBF, (center) periodic, and (bottom) Noisy Matern kernel.</span><a class="headerlink" href="#convcnp-single-gp-extrap-text" title="Permalink to this image">¶</a></p>
</div>
<p><a class="reference internal" href="#convcnp-single-gp-extrap-text"><span class="std std-numref">Fig. 22</span></a> demonstrates that the ConvCNP indeed performs very well!
In particular, we can see that:</p>
<ul class="simple">
<li><p>Like the use of attention, the TE inductive bias also helps the model avoid the tendency to underfit the data.</p></li>
<li><p>Unlike the other members of the CNPF, the ConvCNP is able to extrapolate outside of the training range.
Note that this is a direct consequence of TE.</p></li>
<li><p>Unlike attention, the ConvCNP produces smooth mean and variance functions, avoiding the “kinks” introduced by the AttnCNP.</p></li>
<li><p>The ConvCNP is able to learn about the underlying structure in the periodic kernel.
We can see this by noting that it produces periodic predictions, even “far” away from the observed data.</p></li>
</ul>
<div class="dropdown note admonition">
<p class="admonition-title">Note<span class="math notranslate nohighlight">\(\qquad\)</span>Finite Receptive Field</p>
<p>The periodic kernel example is a little misleading, as the ConvCNP does not recover the underlying GP predictions everywhere.
In fact, we know that it cannot exactly recover the underlying process.
Indeed, it can only model <em>local</em> periodicity because it has a bounded <em>receptive field</em> — the
size of the input region that can affect a particular output. See <a class="reference external" href="https://distill.pub/2019/computing-receptive-fields/">this Distill article</a> for a further explanation of CNN receptive fields.
This is best seen when considering a much larger target interval (<span class="math notranslate nohighlight">\([-2,14]\)</span> instead of <span class="math notranslate nohighlight">\([0,4]\)</span>):</p>
<div class="figure align-default" id="convcnp-periodic-large-extrap-text">
<a class="reference internal image-reference" href="../_images/ConvCNP_periodic_large_extrap1.gif"><img alt="ConvCNP on single images" src="../_images/ConvCNP_periodic_large_extrap1.gif" style="width: 35em;" /></a>
<p class="caption"><span class="caption-number">Fig. 23 </span><span class="caption-text">Large extrapolation (red dashes) of posterior predictive of ConvCNPs (Blue) and the oracle GP (Green) with periodic kernel.</span><a class="headerlink" href="#convcnp-periodic-large-extrap-text" title="Permalink to this image">¶</a></p>
</div>
<p>In fact, this is true for any of the GPs above, all of which have “infinite receptive fields” — in principle, an observation at one point affects the predictions along the entire <span class="math notranslate nohighlight">\(x\)</span>-axis. This means that no model with a bounded receptive field can <em>exactly</em> recover the GP predictive distribution everywhere.
In practice however, most GPs with non-periodic kernels (e.g., with RBF and Matern kernels) have a finite length-scale, and points much further apart than the length-scale are, for all practical purposes, independent.</p>
<p>This discussion alludes to one of the key design choices of the ConvCNP, which is the size of its receptive field.
Note that, unlike standard CNNs, the resulting receptive field does not only depend on the CNN architecture, but also on the granularity of the discretisation employed on the functional representation.</p>
</div>
<p>Let’s now examine the performance of the ConvCNP on more challenging image experiments.
As with the AttnCNP, we consider CelebA and MNIST reconstruction experiments, but also include the Zero-Shot Multi-MNIST (ZSMM) experiments that evaluate the model’s ability to generalise beyond the training data.</p>
<div class="figure align-default" id="convcnp-img-text">
<a class="reference internal image-reference" href="../_images/ConvCNP_img1.gif"><img alt="ConvCNP on CelebA, MNIST, ZSMM" src="../_images/ConvCNP_img1.gif" style="width: 45em;" /></a>
<p class="caption"><span class="caption-number">Fig. 24 </span><span class="caption-text">Posterior predictive of an ConvCNP for CelebA, MNIST, and ZSMM.</span><a class="headerlink" href="#convcnp-img-text" title="Permalink to this image">¶</a></p>
</div>
<p>From <a class="reference internal" href="#convcnp-img-text"><span class="std std-numref">Fig. 24</span></a> we see that the ConvCNP performs quite well on all datasets when the context set is large enough and uniformly sampled, even when extrapolation is needed (ZSMM).
However, performance is less impressive when the context set is very small or when it is structured, e.g., half images.
In our experiments we find that this is more of an issue for the ConvCNP than the AttnCNP (<a class="reference internal" href="../reproducibility/AttnCNP.html#attncnp-img"><span class="std std-numref">Fig. 54</span></a>); we hypothesize that this happens because the effective receptive field of the former is too small.</p>
<div class="dropdown note admonition">
<p class="admonition-title">Note<span class="math notranslate nohighlight">\(\qquad\)</span>Effective Receptive Field</p>
<p>We call the <em>effective</em> receptive field <a class="bibtex reference internal" href="../zbibliography.html#luo2016understanding" id="id15">[LLUZ16]</a>, the empirical receptive field of a <em>trained</em> model rather than the theoretical receptive field for a given architecture.
For example if a ConvCNP always observes many context points during training, then every target point will be close to some to context points and the ConvCNP will thus not need to learn to depend on context points that are far from target points.
The size of the effective receptive field can thus be increased by reducing the size of the context set seen during training, but this solution is somewhat dissatisfying in that it requires tinkering with the training procedure.</p>
<p>In contrast, this is not really an issue with AttnCNP which both always has to attend to all the context points, i.e., it has an “infinite” receptive field.</p>
</div>
<p>Although the previous plots look good, you might wonder how such a model compares to standard interpolation baselines.
To answer this question we will look at larger images to see the more fine grained details.
Specifically, let’s consider a ConvCNP trained on <span class="math notranslate nohighlight">\(128 \times 128\)</span> CelebA:</p>
<div class="figure align-default" id="convcnp-img-baselines">
<a class="reference internal image-reference" href="../_images/ConvCNP_img_baselines1.gif"><img alt="ConvCNP and baselines on CelebA 128" src="../_images/ConvCNP_img_baselines1.gif" style="width: 25em;" /></a>
<p class="caption"><span class="caption-number">Fig. 25 </span><span class="caption-text">ConvCNP and Nearest neighbour, bilinear, bicubic interpolation on CelebA 128.</span><a class="headerlink" href="#convcnp-img-baselines" title="Permalink to this image">¶</a></p>
</div>
<p><a class="reference internal" href="#convcnp-img-baselines"><span class="std std-numref">Fig. 25</span></a> shows that the ConvCNP performs much better than baseline interpolation methods. Having seen such encouraging results, as well as the decent zero shot generalisation capability of the ConvCNP on ZSMM, it is natural to want to evaluate the model on actual images with multiple faces with different scales and orientations:</p>
<div class="figure align-default" id="convcnp-img-zeroshot-text">
<a class="reference internal image-reference" href="../_images/ConvCNP_img_zeroshot.png"><img alt="Zero shot generalization of ConvCNP to a real picture" src="../_images/ConvCNP_img_zeroshot.png" style="width: 20em;" /></a>
<p class="caption"><span class="caption-number">Fig. 26 </span><span class="caption-text">Zero shot generalization of a ConvCNP trained on CelebA and evaluated on Ellen’s selfie. We also show a baseline bilinear interpolator.</span><a class="headerlink" href="#convcnp-img-zeroshot-text" title="Permalink to this image">¶</a></p>
</div>
<p>From <a class="reference internal" href="#convcnp-img-zeroshot-text"><span class="std std-numref">Fig. 26</span></a> we see that the model trained on single faces is able to generalise reasonably well to real world data in a zero shot fashion. One possible application of the ConvCNP is increasing the resolution of an image.
This can be achieved by querying positions “in between” pixels.</p>
<div class="figure align-default" id="convcnp-superes-baseline-text">
<a class="reference internal image-reference" href="../_images/ConvCNP_superes_baseline.png"><img alt="Increasing image resolution with ConvCNP" src="../_images/ConvCNP_superes_baseline.png" style="width: 30em;" /></a>
<p class="caption"><span class="caption-number">Fig. 27 </span><span class="caption-text">Increasing the resolution of <span class="math notranslate nohighlight">\(16 \times 16\)</span> CelebA to <span class="math notranslate nohighlight">\(128 \times 128\)</span> with a ConvCNP and a baseline bilinear interpolator.</span><a class="headerlink" href="#convcnp-superes-baseline-text" title="Permalink to this image">¶</a></p>
</div>
<p><a class="reference internal" href="#convcnp-superes-baseline-text"><span class="std std-numref">Fig. 27</span></a> demonstrates such an application.
We see that the ConvCNP can indeed be used to increase the resolution of an image better than the baseline bilinear interpolator, even though it was not explicitly trained to do so!</p>
<div class="note admonition">
<p class="admonition-title">Note</p>
<p>Model details, training and more plots are available in the <a class="reference internal" href="../reproducibility/ConvCNP.html"><span class="doc">ConvCNP Notebook</span></a>. We also provide pretrained models to play around with.</p>
</div>
</div>
</div>
<div class="section" id="issues-with-the-cnpf">
<span id="issues-cnpfs"></span><h2>Issues With the CNPF<a class="headerlink" href="#issues-with-the-cnpf" title="Permalink to this headline">¶</a></h2>
<p>Let’s take a step back.
So far, we have seen that we can use the factorisation assumption to construct members of the CNPF, perhaps the simplest of these being the CNP.
Our first observation was that while the CNP can predict simple stochatic processes, it tends to underfit when the processes are more complicated.
We saw that this tendency can be addressed by adding appropriate inductive biases to the model.
Specifically, the AttnCNP significantly improves upon the CNP by adding an attention mechanism to generate target-specific representations of the context set.
However, both the CNP and AttnCNP fail to make meaningful predictions when data is observed outside the training range.
Finally, we saw how including translation equivariance as an inductive bias led to accurate predictions that generalised elegantly to observations outside the training range.</p>
<p>Let’s now consider more closely the implications of the factorisation assumption, along with the Gaussian form of predictive distributions.
One immediate consequence of using a Gaussian likelihood is that we cannot model multi-modal predictive distributions.
To see why this might be an issue, consider making predictions for the MNIST reconstruction experiments.</p>
<div class="figure align-default" id="convcnp-marginal-text">
<a class="reference internal image-reference" href="../_images/ConvCNP_marginal.png"><img alt="Samples from ConvCNP on MNIST and posterior of different pixels" src="../_images/ConvCNP_marginal.png" style="width: 20em;" /></a>
<p class="caption"><span class="caption-number">Fig. 28 </span><span class="caption-text">Predictive distribution of a ConvCNP on an entire MNIST image (left) and marginal predictive distributions of some pixels (right).</span><a class="headerlink" href="#convcnp-marginal-text" title="Permalink to this image">¶</a></p>
</div>
<p>Looking at <a class="reference internal" href="#convcnp-marginal-text"><span class="std std-numref">Fig. 28</span></a>, we might expect that sampling from the predictive distribution for an unobserved pixel would sometimes yield completely white values, and sometimes completely black — depending on whether the sample represents, for example, a 3 or a 5.
However, a Gaussian distribution, which is unimodal (see <a class="reference internal" href="#convcnp-marginal-text"><span class="std std-numref">Fig. 28</span></a> right), cannot model this multimodality.</p>
<!---
One possible solution to this problem might be to employ some other parametric distribution that enables multimodality, for example, a mixture of Gaussians.
While this may solve some issues, we can generalise this point to say that the CNPF requires specifying _some parametric form of distribution_.
Ideally, what we would like is some parametrisation of the NPF that enables us to recover _any_ form of marginal distribution.
-->
<p>The other major restriction is the factorisation assumption itself.
First, CNPF members cannot model any dependencies in the predictive distribution over multiple target points.
For example imagine that we are modelling samples from a GP.
If the model is making predictions at two target locations that are “close” on the <span class="math notranslate nohighlight">\(x\)</span>-axis, it seems reasonable that whenever it predicts the first output to be “high”, it would predict something similar for the second, and vice versa.
Yet the factorisation assumption prevents this type of correlation from occurring.
Another way to view this is that the CNPF cannot produce <em>coherent</em> samples from its predictive distribution.
In fact, sampling from the posterior corresponds to adding independent noise to the mean at each target location, resulting in samples that are discontinuous and look nothing like the underlying process:</p>
<div class="figure align-default" id="convcnp-rbf-samples-text">
<a class="reference internal image-reference" href="../_images/ConvCNP_rbf_samples.png"><img alt="Sampling from ConvCNP on GP with RBF kernel" src="../_images/ConvCNP_rbf_samples.png" style="width: 35em;" /></a>
<p class="caption"><span class="caption-number">Fig. 29 </span><span class="caption-text">Samples form the posterior predictive of a ConvCNP (Blue), and the predictive distribution of the oracle GP (Green) with RBF kernel.</span><a class="headerlink" href="#convcnp-rbf-samples-text" title="Permalink to this image">¶</a></p>
</div>
<p>Similarly, sampled images from a member of the CPNF are not coherent and look like random noise added to a picture:</p>
<div class="figure align-default" id="convcnp-img-sampling-text">
<a class="reference internal image-reference" href="../_images/ConvCNP_img_sampling.png"><img alt="Sampling from ConvCNP on CelebA, MNIST, ZSMM" src="../_images/ConvCNP_img_sampling.png" style="width: 45em;" /></a>
<p class="caption"><span class="caption-number">Fig. 30 </span><span class="caption-text">Samples from the posterior predictive of an ConvCNP for CelebA, MNIST, and ZSMM.</span><a class="headerlink" href="#convcnp-img-sampling-text" title="Permalink to this image">¶</a></p>
</div>
<div class="note dropdown admonition">
<p class="admonition-title">Note<span class="math notranslate nohighlight">\(\qquad\)</span>Thompson Sampling</p>
<p>This inability to sample from the predictive may inhibit the deployment of CNPF members from several application areas for which it might otherwise be potentially well-suited.
One such example is the use of Thompson sampling algorithms for e.g., contextual bandits or Bayesian optimisation, which require a model to produce samples.</p>
</div>
<p>In the next chapter, we will see one approach to solving both these issues by treating the representation as a latent variable. This leads us to the <em>latent</em> Neural Process family (LNPF).</p>
<hr class="docutils" />
<dl class="footnote brackets">
<dt class="label" id="id16"><span class="brackets"><a class="fn-backref" href="#id6">1</a></span></dt>
<dd><p><a class="bibtex reference internal" href="../zbibliography.html#kim2019attentive" id="id17">[KMS+19]</a> only introduced the latent variable model, but one can easily drop the latent variable if not needed.</p>
</dd>
</dl>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./text"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
        </div>
    </div>
    
    
    <div class='prev-next-bottom'>
        
    <a class='left-prev' id="prev-link" href="Intro.html" title="previous page">The Neural Process Family</a>
    <a class='right-next' id="next-link" href="LNPF.html" title="next page">Latent NPFs</a>

    </div>
    <footer class="footer mt-5 mt-md-0">
    <div class="container">
      <p>
        
          By Yann Dubois, Jonathan Gordon, ‪Andrew Foong<br/>
        
            &copy; Copyright 2020.<br/>
      </p>
    </div>
  </footer>
</main>


      </div>
    </div>

    
  <script src="../_static/js/index.30270b6e4c972e43c488.js"></script>


    
  </body>
</html>