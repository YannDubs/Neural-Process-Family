

<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Latent NPFs &#8212; Neural Process Family</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.11.2/css/all.min.css" integrity="sha384-KA6wR/X5RY4zFAHpv/CnoG2UW1uogYfdnP67Uv7eULvTveboZJg0qUpmJZb5VqzN" crossorigin="anonymous">
    <link href="../_static/css/index.css" rel="stylesheet">
    <link rel="stylesheet" href="../_static/sphinx-book-theme.css" type="text/css" />
    <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-dropdown.css" />
    <script id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
    <script src="../_static/sphinx-book-theme.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/language_data.js"></script>
    <script src="../_static/togglebutton.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script src="../_static/sphinx-book-theme.js"></script>
    <script >var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <script type="text/x-mathjax-config">MathJax.Hub.Config({"tex2jax": {"inlineMath": [["\\(", "\\)"]], "displayMath": [["\\[", "\\]"]], "processRefs": false, "processEnvironments": false}})</script>
    <script async="async" src="https://unpkg.com/thebelab@latest/lib/index.js"></script>
    <script >
        const thebe_selector = ".thebe"
        const thebe_selector_input = "pre"
        const thebe_selector_output = ".output"
    </script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <link rel="shortcut icon" href="../_static/favicon.ico"/>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Datasets" href="../reproducibility/Datasets.html" />
    <link rel="prev" title="Conditional NPF" href="CNPF.html" />

    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="docsearch:language" content="en">



  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
<a class="navbar-brand text-wrap" href="../index.html">
  
  <img src="../_static/logo.gif" class="logo" alt="logo">
  
  
  <h1 class="site-logo" id="site-title">Neural Process Family</h1>
  
</a>
</div>

<form class="bd-search d-flex align-items-center" action="../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form>

<nav class="bd-links" id="bd-docs-nav" aria-label="Main navigation">
  <ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="Intro.html">
   The Neural Process Family
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Sub-Families
 </span>
</p>
<ul class="current nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="CNPF.html">
   Conditional NPF
  </a>
 </li>
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   Latent NPF
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Reproducibility
 </span>
</p>
<ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="../reproducibility/Datasets.html">
   Datasets
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../reproducibility/CNP.html">
   CNP
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../reproducibility/AttnCNP.html">
   AttnCNP
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../reproducibility/ConvCNP.html">
   ConvCNP
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../reproducibility/LNP.html">
   LNP
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../reproducibility/AttnLNP.html">
   AttnLNP
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../reproducibility/ConvLNP.html">
   ConvLNP
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../reproducibility/Losses.html">
   LNPF Losses
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Reference
 </span>
</p>
<ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="../zbibliography.html">
   Bibliography
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference external" href="https://github.com/YannDubs/Neural-Process-Family">
   GitHub Repo
   <i class="fas fa-external-link-alt">
   </i>
  </a>
 </li>
</ul>

</nav>

 <!-- To handle the deprecated key -->

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="row topbar fixed-top container-xl">
    <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show">
    </div>
    <div class="col pl-2 topbar-main">
        
        <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
            data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
            aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
            title="Toggle navigation" data-toggle="tooltip" data-placement="left">
            <i class="fas fa-bars"></i>
            <i class="fas fa-arrow-left"></i>
            <i class="fas fa-arrow-up"></i>
        </button>
        
        <div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    
    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="../_sources/text/LNPF.md"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.md</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
            onClick="window.print()" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
    
</div>
        <!-- Source interaction buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Connect with source repository"><i class="fab fa-github"></i></button>
    <div class="dropdown-buttons sourcebuttons">
        <a class="repository-button"
            href="https://github.com/YannDubs/Neural-Process-Family"><button type="button" class="btn btn-secondary topbarbtn"
                data-toggle="tooltip" data-placement="left" title="Source repository"><i
                    class="fab fa-github"></i>repository</button></a>
        
        
    </div>
</div>


        <!-- Full screen (wrap in <a> to have style consistency -->
        <a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
                data-placement="bottom" onclick="toggleFullScreen()" title="Fullscreen mode"><i
                    class="fas fa-expand"></i></button></a>

        <!-- Launch buttons -->

    </div>

    <!-- Table of contents -->
    <div class="d-none d-md-block col-md-2 bd-toc show">
        <div class="tocsection onthispage pt-5 pb-3">
            <i class="fas fa-list"></i> Contents
        </div>
        <nav id="bd-toc-nav">
            <ul class="nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#overview">
   Overview
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#training-lnpf-members">
   Training LNPF members
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#neural-process-maximum-likelihood-npml">
     Neural Process Maximum Likelihood (NPML)
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#neural-process-variational-inference-npvi">
     Neural Process Variational Inference (NPVI)
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#latent-neural-process-lnp">
   Latent Neural Process (LNP)
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#attentive-latent-neural-process-attnlnp">
   Attentive Latent Neural Process (AttnLNP)
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#convolutional-latent-neural-process-convlnp">
   Convolutional Latent Neural Process (ConvLNP)
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#issues-and-discussion">
   Issues and Discussion
  </a>
 </li>
</ul>

        </nav>
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
        
              <div>
                
  <div class="section" id="latent-npfs">
<h1>Latent NPFs<a class="headerlink" href="#latent-npfs" title="Permalink to this headline">¶</a></h1>
<div class="section" id="overview">
<h2>Overview<a class="headerlink" href="#overview" title="Permalink to this headline">¶</a></h2>
<p>We concluded the previous section by noting two important drawbacks of the CNPF:</p>
<ul class="simple">
<li><p>The predictive distribution is factorised across target points, and thus can neither account for correlations in the predictive nor (as a result) produce “coherent” samples from the predictive distribution.</p></li>
<li><p>The predictive distribution requires specification of a particular parametric form (e.g. Gaussian).</p></li>
</ul>
<p>In this section we discuss an alternative parametrisation of <span class="math notranslate nohighlight">\(p_\theta( \mathbf{y}_\mathcal{T} | \mathbf{x}_\mathcal{T}; \mathcal{C})\)</span> that still satisfies our desiredata for NPs, and addresses both of these issues.
The main idea is to introduce a latent variable <span class="math notranslate nohighlight">\(\mathbf{z}\)</span> into the definition of the predictive distribution.
This leads us to the second major branch of the NPF, which we refer to as the Latent Neural Process Sub-family, or LNPF for short.
A graphical representation of the LNPF  is given in <a class="reference internal" href="#graph-model-lnps-text"><span class="std std-numref">Fig. 31</span></a>.</p>
<div class="figure align-default" id="graph-model-lnps-text">
<a class="reference internal image-reference" href="../_images/graph_model_LNPF.svg"><img alt="graphical model LNP" src="../_images/graph_model_LNPF.svg" width="300em" /></a>
<p class="caption"><span class="caption-number">Fig. 31 </span><span class="caption-text">Probabilistic graphical model for LNPs.</span><a class="headerlink" href="#graph-model-lnps-text" title="Permalink to this image">¶</a></p>
</div>
<p>To specify this family of models, we must define a few components:</p>
<ul class="simple">
<li><p>An encoder: <span class="math notranslate nohighlight">\(p_{\theta} \left( \mathbf{z} | \mathcal{C} \right)\)</span>, which provides a <em>distribution</em> over the latent variable <span class="math notranslate nohighlight">\(\mathbf{z}\)</span> having observed the context set <span class="math notranslate nohighlight">\(\mathcal{C}\)</span>. As with other NPF, the encoder needs to be permutation invariant to correctly treat <span class="math notranslate nohighlight">\(\mathcal{C}\)</span> as a set. A typical example is to first have a deterministic representation <span class="math notranslate nohighlight">\(R\)</span> and then use it to output the mean and (log) standard deviations of a Gaussian distribution over <span class="math notranslate nohighlight">\(\mathbf{z}\)</span>.</p></li>
<li><p>A decoder: <span class="math notranslate nohighlight">\(p_{\theta} \left( \mathbf{y}_{\mathcal{T}} | \mathbf{x}_{\mathcal{T}}, \mathbf{z} \right)\)</span>, which provides predictive distributions conditioned on <span class="math notranslate nohighlight">\(\mathbf{z}\)</span> and a target location <span class="math notranslate nohighlight">\(\mathbf{x}_{\mathcal{T}}\)</span>.
The decoder will usually be the same as the CNPF, but using a sample of the latent representation <span class="math notranslate nohighlight">\(\mathbf{z}\)</span> and marginalizing them, rather than a deterministic representation.</p></li>
</ul>
<p>Putting altogether:</p>
<div class="math notranslate nohighlight" id="equation-latent-likelihood">
<span class="eqno">(4)<a class="headerlink" href="#equation-latent-likelihood" title="Permalink to this equation">¶</a></span>\[\begin{split}\begin{align}
p_{\theta}(\mathbf{y}_\mathcal{T} | \mathbf{x}_\mathcal{T}; \mathcal{C})
&amp;= \int p_{\theta} \left( \mathbf{z} | \mathcal{C} \right) p_{\theta} \left( \mathbf{y}_{\mathcal{T}} | \mathbf{x}_{\mathcal{T}}, \mathbf{z} \right) \mathrm{d}\mathbf{z} &amp; \text{Marginalisation}  \\
&amp;= \int p_{\theta} \left( \mathbf{z} | \mathcal{C} \right) \prod_{t=1}^{T} p_{\theta}(y^{(t)} |  x^{(t)}, \mathbf{z}) \, \mathrm{d}\mathbf{z}  &amp; \text{Factorisation}\\
&amp;= \int p_{\theta} \left( \mathbf{z} | \mathcal{C} \right)  \prod_{t=1}^{T} \mathcal{N} \left( y^{(t)};  \mu^{(t)}, \sigma^{2(t)} \right) \mathrm{d}\mathbf{z} &amp; \text{Gaussianity}
\end{align}\end{split}\]</div>
<div class="hint dropdown admonition">
<p class="admonition-title">Advanced<span class="math notranslate nohighlight">\(\qquad\)</span>Latent variable <span class="math notranslate nohighlight">\(\implies\)</span> consistency</p>
<p>We show that like the CNPF, members of the LNPF also specify consistent stochastic processes conditioned on a fixed context set <span class="math notranslate nohighlight">\(\mathcal{C}\)</span>.</p>
<p>(Consistency under permutation) Let <span class="math notranslate nohighlight">\(\mathbf{x}_{\mathcal{T}} = \{ x^{(t)} \}_{t=1}^T\)</span> be the target inputs and <span class="math notranslate nohighlight">\(\pi\)</span> be any permutation of <span class="math notranslate nohighlight">\(\{1, ..., T\}\)</span>. Then the predictive density is:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align}
    p_\theta(y^{(1)}, ..., y^{(T)} | x^{(1)}, ..., x^{(T)}; \mathcal{C}) &amp;= \int p_{\theta} ( \mathbf{z}) \prod_{t=1}^{T} p_{\theta}(y^{(t)} |  x^{(t)}, \mathbf{z}) \, \mathrm{d}\mathbf{z} \\
    &amp;= \int p_{\theta} ( \mathbf{z}) \prod_{t=1}^{T} p_{\theta}(y^{(\pi(t))} |  y^{(\pi(t))}, \mathbf{z}) \, \mathrm{d}\mathbf{z} \\
    &amp;= p_\theta(y^{(\pi(1))}, ..., y^{(\pi(T))} | x^{(\pi(1))}, ..., x^{(\pi(T))}; \mathcal{C})
\end{align}
\end{split}\]</div>
<p>since multiplication is commutative.</p>
<p>(Consistency under marginalisation)  Consider two target inputs, <span class="math notranslate nohighlight">\(x^{(1)}, x^{(2)}\)</span>. Then by marginalising out the second target output, we get:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align}
    \int p_\theta(y^{(1)}, y^{(2)}| x^{(1)}, x^{(2)}; \mathcal{C}) \, \mathrm{d}y^{(2)} &amp;= \int \int p_\theta(y^{(1)}| x^{(1)}, \mathbf{z})p_\theta(y^{(2)}| x^{(2)}, \mathbf{z}) p_\theta(\mathbf{z} | \mathcal{C}) \, \mathrm{d}\mathbf{z} \mathrm{d}y^{(2)} \\
    &amp;= \int  p_\theta(y^{(1)}| x^{(1)}, \mathbf{z}) p_\theta(\mathbf{z}| \mathcal{C}) \int p_\theta(y^{(2)}| x^{(2)}, \mathbf{z})  \, \mathrm{d}y^{(2)} \mathrm{d}\mathbf{z}  \\
    &amp;= \int  p_\theta(y^{(1)}| x^{(1)}, \mathbf{z}) p_\theta(\mathbf{z}| \mathcal{C}) \mathrm{d}\mathbf{z}  \\
    &amp;= p_\theta(y^{(1)}| x^{(1)}; \mathcal{C})
\end{align}
\end{split}\]</div>
<p>which shows that the predictive distribution obtained by querying an LNPF member at <span class="math notranslate nohighlight">\(x^{(1)}\)</span> is the same as that obtained by querying it at <span class="math notranslate nohighlight">\(x^{(1)}, x^{(2)}\)</span> and then marginalising out the second target point. Of course, the same idea works with collections of any size, and marginalising any subset of the variables.</p>
</div>
<p>Now, you might be worried that we have still made both the factorisation and Gaussian assumptions!
However, while the decoder likelihood <span class="math notranslate nohighlight">\(p_{\theta} \left( \mathbf{y}_{\mathcal{T}} | \mathbf{x}_{\mathcal{T}}, \mathbf{z} \right)\)</span> is still factorised, the predictive distribution we are actually interested in, <span class="math notranslate nohighlight">\(p_\theta( \mathbf{y}_\mathcal{T} | \mathbf{x}_\mathcal{T}; \mathcal{C})\)</span>, is no longer due to the marginalisation of <span class="math notranslate nohighlight">\(\mathbf{z}\)</span>, thus addressing the first problem we associated with the CNPF.
Moreover, the predictive distribution is no longer Gaussian either.
In fact, since the predictive now has the form of an <em>infinite mixture of Gaussians</em>, potentially <em>any</em> predictive density can be represented (i.e. learned) by this form.
This is great news, as it (conceptually) relieves us of the burden of choosing / designing a bespoke likelihood function when deploying the NPF for a new application!</p>
<p>However, there is an important drawback. The key difficulty with the LNPF is that the likelihood we defined in Eq.<a class="reference internal" href="#equation-latent-likelihood">(4)</a> is no longer <em>analytically tractable</em>.
We now discuss how to train members of the LNPF in general.
After discussing several training procedures, we’ll introduce extensions of each of the CNPF members discussed in the previous chapter to their corresponding member of the LNPF.</p>
</div>
<div class="section" id="training-lnpf-members">
<span id="training-lnpf"></span><h2>Training LNPF members<a class="headerlink" href="#training-lnpf-members" title="Permalink to this headline">¶</a></h2>
<p>Ideally, we would like to directly maximize the likelihood defined in Eq.<a class="reference internal" href="#equation-latent-likelihood">(4)</a> to optimise the parameters of the model.
However, the integral over <span class="math notranslate nohighlight">\(\mathbf{z}\)</span> renders this quantity intractable, so we must consider alternatives.
In fact, this story is not new, and the same issues arise when considering other latent variable models, such as variational auto-encoders (VAEs).</p>
<p>The question of how best to train LNPF members is still open, and there is ongoing research in this area.
In this section, we will cover two methods for training LNPF models, but each have their flaws, and deciding which is the preferred training method must often be answered empirically.
Here is a brief summary / preview of both methods which are described in details in the following sections:</p>
<table class="table" id="summary-training">
<caption><span class="caption-number">Table 3 </span><span class="caption-text">Summary of training methods for LNPF models</span><a class="headerlink" href="#summary-training" title="Permalink to this table">¶</a></caption>
<colgroup>
<col style="width: 20%" />
<col style="width: 20%" />
<col style="width: 20%" />
<col style="width: 20%" />
<col style="width: 20%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head stub"><p>Training Method</p></th>
<th class="head"><p>Approximation of Log Likelihood</p></th>
<th class="head"><p>Biased</p></th>
<th class="head"><p>Variance</p></th>
<th class="head"><p>Empirical Performance</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><th class="stub"><p><a class="reference internal" href="#npml"><span class="std std-ref">NPML</span></a> <a class="bibtex reference internal" href="../zbibliography.html#foong2020convnp" id="id1">[FBG+20]</a></p></th>
<td><p>Sample Estimate</p></td>
<td><p>Yes</p></td>
<td><p>Large</p></td>
<td><p>Usually better</p></td>
</tr>
<tr class="row-odd"><th class="stub"><p><a class="reference internal" href="#npml"><span class="std std-ref">NPVI</span></a> <a class="bibtex reference internal" href="../zbibliography.html#garnelo2018neural" id="id2">[GSR+18]</a></p></th>
<td><p>Variational Inference</p></td>
<td><p>Yes</p></td>
<td><p>Small</p></td>
<td><p>Usually worst</p></td>
</tr>
</tbody>
</table>
<div class="caution dropdown admonition">
<p class="admonition-title">Disclaimer<span class="math notranslate nohighlight">\(\qquad\)</span> Chronology</p>
<p>In this tutorial, the order in which we introduce the objective functions does not follow the chronology in which they were originally introduced in the literature.
We begin by describing an approximate maximum-likelihood procedure, which was recently introduced by <a class="bibtex reference internal" href="../zbibliography.html#foong2020convnp" id="id3">[FBG+20]</a> due to its simplicity, and its relation to the CNPF training procedure.
Following this, we introduce a variational inference-inspired approach, which was proposed earlier by <a class="bibtex reference internal" href="../zbibliography.html#garnelo2018neural" id="id4">[GSR+18]</a> to train members of the LNPF.</p>
</div>
<div class="section" id="neural-process-maximum-likelihood-npml">
<span id="npml"></span><h3>Neural Process Maximum Likelihood (NPML)<a class="headerlink" href="#neural-process-maximum-likelihood-npml" title="Permalink to this headline">¶</a></h3>
<p>First, let’s consider a direct approach to optimising the log-marginal predictive likelihood of LNPF members.
While this quantity is no longer tractable (as it was with members of the CNPF), we can derive an estimator using Monte-Carlo sampling:</p>
<div class="math notranslate nohighlight" id="equation-npml">
<span class="eqno">(5)<a class="headerlink" href="#equation-npml" title="Permalink to this equation">¶</a></span>\[\begin{split}\begin{align}
\log p_{\theta}(\mathbf{y}_\mathcal{T} | \mathbf{x}_\mathcal{T}; \mathcal{C})
&amp;= \log \int p_{\theta} \left( \mathbf{z} | \mathcal{C} \right) \prod_{t=1}^{T} p_{\theta} \left( y^{(t)} | x^{(t)}, \mathbf{z} \right) \mathrm{d}\mathbf{z} &amp; \text{Marginalisation} \\
&amp; \approx \log \left( \frac{1}{L} \sum_{l=1}^{L} \prod_{t=1}^{T} p_{\theta} \left( y^{(t)} | x^{(t)}, \mathbf{z}_l \right) \right) &amp; \text{Monte-Carlo approximation} \\
&amp; = \hat{\mathcal{L}}_{ML}
\end{align}\end{split}\]</div>
<p>where each <span class="math notranslate nohighlight">\(\mathbf{z}_l \sim p_{\theta} \left( \mathbf{z} | \mathcal{C} \right)\)</span>.</p>
<div class="dropdown admonition">
<p class="admonition-title">Implementation<span class="math notranslate nohighlight">\(\qquad\)</span>LogSumExp</p>
<p>In practice, manipulating directly probabilities is prone to numerical instabilities, e.g., multiplying probabilities as in Eq.<a class="reference internal" href="#equation-npml">(5)</a> will often underflow.
As a result one should manipulate probabilities in the log domain:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{align}
\log p_{\theta}(\mathbf{y}_\mathcal{T} | \mathbf{x}_\mathcal{T}; \mathcal{C})
&amp;\approx \log \left( \frac{1}{L} \sum_{l=1}^{L} \prod_{t=1}^{T} p_{\theta} \left( y^{(t)} | x^{(t)}, \mathbf{z}_l \right) \right) &amp; \text{Monte-Carlo} \\
&amp; = \log \left( \sum_{l=1}^{L} \exp \left(  \sum_{t=1}^{T} \log p_{\theta} \left( y^{(t)} | x^{(t)}, \mathbf{z}_l \right) \right) \right) - \log L &amp; \text{LogSumExp trick}\\
&amp; = \text{LogSumExp}_{l=1}^{L} \left( \sum_{t=1}^{T} \log p_{\theta} \left( y^{(t)} | x^{(t)}, \mathbf{z}_l \right) \right) - \log L,
\end{align}\end{split}\]</div>
<p>where numerical stable implementations of <a class="reference external" href="https://en.wikipedia.org/wiki/LogSumExp#log-sum-exp_trick_for_log-domain_calculations">LogSumExp</a> can be found in most frameworks.</p>
</div>
<p>Eq.<a class="reference internal" href="#equation-npml">(5)</a> provides a simple-to-compute objective function for training LNPF-members, which we can then use with standard optimisers to learn the model parameters <span class="math notranslate nohighlight">\(\theta\)</span>.
The final (numerically stable) pseudo-code for NPML is given in <a class="reference internal" href="#npml-pseudocode"><span class="std std-numref">Fig. 32</span></a>:</p>
<div class="figure align-default" id="npml-pseudocode">
<a class="reference internal image-reference" href="../_images/alg_npml.png"><img alt="Pseudo-code NPML." src="../_images/alg_npml.png" style="width: 35em;" /></a>
<p class="caption"><span class="caption-number">Fig. 32 </span><span class="caption-text">Pseudo-code for a single training step of a LNPF member with NPML.</span><a class="headerlink" href="#npml-pseudocode" title="Permalink to this image">¶</a></p>
</div>
<p>NPML is conceptually very simple as it directly approximates the training procedure of the CNPF, in the sense that it targets the same predictive likelihood during training.
Moreover, it tends to work well in practice, typically leading to models achieving good performance.
However, it suffers from two important drawbacks:</p>
<ol class="simple">
<li><p><em>Bias</em>: When applying the Monte-Carlo approximation, we have employed an unbiased estimator to the predictive likelihood. However, in practice we are interested in the <em>log</em> likelihood. Unfortunately, the log of an unbiased estimator is not itself unbiased. As a result, NPML is a <em>biased</em> (conservative) estimator of the true log-likelihood.</p></li>
<li><p><em>High Variance</em>: In practice it turns out that NPML is quite sensitive to the number of samples <span class="math notranslate nohighlight">\(L\)</span> used to approximate it. In both our GP and image experiments, we find that on the order of 20 samples are required to achieve “good” performance. Of course, the computational and memory costs of training scale linearly with <span class="math notranslate nohighlight">\(L\)</span>, often limiting the number of samples that can be used in practice.</p></li>
</ol>
<p>Unfortunately, decreasing the number of samples <span class="math notranslate nohighlight">\(L\)</span> needed to perform well turns out to be quite difficult, and is an open question in training latent variable models in general.
However, we next describe an alternative training procedure that typically works well with fewer samples.</p>
</div>
<div class="section" id="neural-process-variational-inference-npvi">
<h3>Neural Process Variational Inference (NPVI)<a class="headerlink" href="#neural-process-variational-inference-npvi" title="Permalink to this headline">¶</a></h3>
<p>NPVI is a training procedure proposed by <a class="bibtex reference internal" href="../zbibliography.html#garnelo2018neural" id="id5">[GSR+18]</a>, which takes inspiration from the literature on variational inference (VI).
The central idea behind this objective function is to use <em>posterior sampling</em> to reduce the variance of NPML.
For a better intuition regarding this, note that NPML is defined using an expectation against <span class="math notranslate nohighlight">\(p_{\theta}(\mathbf{z} | \mathcal{C})\)</span>.
The idea in posterior sampling is to use the whole task, including the target set, <span class="math notranslate nohighlight">\(\mathcal{D} = \mathcal{C} \cup \mathcal{T}\)</span> to produce the distribution over <span class="math notranslate nohighlight">\(\mathbf{z}\)</span>, thus leading to more informative samples and lower variance objectives.</p>
<p>In our case, the posterior distribution from which we would like to sample is <span class="math notranslate nohighlight">\(p(\mathbf{z} | \mathcal{C}, \mathcal{T})\)</span>, i.e., the distribution of the latent variable having observed <em>both</em> the context and target sets.
Unfortunately, this posterior is intractable.
To address this, <a class="bibtex reference internal" href="../zbibliography.html#garnelo2018neural" id="id6">[GSR+18]</a> propose to replace the true posterior with simply passing both the context and target sets through the encoder, i.e.</p>
<div class="math notranslate nohighlight" id="equation-approximate-posterior">
<span class="eqno">(6)<a class="headerlink" href="#equation-approximate-posterior" title="Permalink to this equation">¶</a></span>\[\begin{align}
p \left( \mathbf{z} | \mathcal{C}, \mathcal{T} \right) \approx p_{\theta} \left( \mathbf{z} | \mathcal{D} \right)
\end{align}\]</div>
<div class="note dropdown admonition">
<p class="admonition-title">Note<span class="math notranslate nohighlight">\(\qquad p \left( \mathbf{z} | \mathcal{C}, \mathcal{T} \right) \neq p_{\theta} \left( \mathbf{z} | \mathcal{D} \right)\)</span></p>
<p>Note that these two distributions are different.
We can compute <span class="math notranslate nohighlight">\(p_{\theta} \left( \mathbf{z} | \mathcal{D} \right)\)</span> by simply passing both <span class="math notranslate nohighlight">\(\mathcal{C}\)</span> and <span class="math notranslate nohighlight">\(\mathcal{T}\)</span> through the model encoder.
One the other hand, the posterior <span class="math notranslate nohighlight">\(p \left( \mathbf{z} | \mathcal{C}, \mathcal{T} \right)\)</span> is computed using Bayes’ rule and our model definition:</p>
<div class="math notranslate nohighlight">
\[
\begin{align}
p \left( \mathbf{z} | \mathcal{C}, \mathcal{T} \right) = \frac{p_{\theta} \left( \mathbf{z} | \mathcal{C} \right) \prod_{t=1}^{T} p_{\theta} \left( y^{(t)} | x^{(t)}, \mathbf{z} \right)}{ \int p_{\theta} \left( \mathbf{z} | \mathcal{C} \right) \prod_{t=1}^{T} p_{\theta} \left( y^{(t)} | x^{(t)}, \mathbf{z} \right) \mathrm{d} \mathbf{z}}.
\end{align}
\]</div>
<p>Recalling that our decoder is defined by a complicated, non-linear neural network, we can see that this posterior is intractable, as it involves an integration against complicated likelihoods.</p>
</div>
<p>We can now derive the final objective function, which is a <em>lower bound</em> to the log marginal likelihood, by (i) introducing the approximate posterior as a sampling distribution, and (ii) employing a straightforward application of <a class="reference external" href="https://en.wikipedia.org/wiki/Jensen%27s_inequality">Jensen’s inequality</a>.</p>
<div class="math notranslate nohighlight" id="equation-npvi">
<span class="eqno">(7)<a class="headerlink" href="#equation-npvi" title="Permalink to this equation">¶</a></span>\[\begin{split}\begin{align}
\log p_{\theta}(\mathbf{y}_\mathcal{T} | \mathbf{x}_\mathcal{T}, \mathcal{C})
&amp;= \log \int p_{\theta} \left( \mathbf{z} | \mathcal{C} \right)  p_{\theta} \left( \mathbf{y}_{\mathcal{T}} | \mathbf{x}_{\mathcal{T}}, \mathbf{z} \right) \mathrm{d}\mathbf{z} &amp; \text{Marginalisation} \\
&amp; = \log \int p_{\theta} \left( \mathbf{z} | \mathcal{D} \right) \frac{p_{\theta} \left( \mathbf{z} | \mathcal{C} \right)}{p_{\theta} \left( \mathbf{z} | \mathcal{D} \right)} p_{\theta} \left( \mathbf{y}_{\mathcal{T}} | \mathbf{x}_{\mathcal{T}}, \mathbf{z} \right) \mathrm{d}\mathbf{z} &amp; \text{Importance Weight} \\
&amp; \geq \int p_{\theta} \left( \mathbf{z} | \mathcal{D} \right) \left( \log p_{\theta} \left( \mathbf{y}_{\mathcal{T}} | \mathbf{x}_{\mathcal{T}}, \mathbf{z} \right) + \log \frac{p_{\theta} \left( \mathbf{z} | \mathcal{C} \right)}{p_{\theta} \left( \mathbf{z} | \mathcal{D} \right)} \right) &amp; \text{Jensen's inequality} \\
&amp; = \mathbb{E}_{\mathbf{z} \sim p_{\theta} \left( \mathbf{z} | \mathcal{D} \right)} \left[ \log p_{\theta} \left( \mathbf{y}_{\mathcal{T}} | \mathbf{x}_{\mathcal{T}}, \mathbf{z} \right) \right] - \mathrm{KL} \left( p_{\theta} \left( \mathbf{z} | \mathcal{D} \right) \| p_{ \theta} \left( \mathbf{z} | \mathcal{C} \right) \right) \\
&amp; = \mathcal{L}_{VI}
\end{align}\end{split}\]</div>
<p>where <span class="math notranslate nohighlight">\(\mathrm{KL}(p \| q)\)</span> is the Kullback-Liebler (KL) divergence between two distributions <span class="math notranslate nohighlight">\(p\)</span> and <span class="math notranslate nohighlight">\(q\)</span>, and we have used the shorthand <span class="math notranslate nohighlight">\(p_{\theta} \left( \mathbf{y}_{\mathcal{T}} | \mathbf{x}_{\mathcal{T}}, \mathbf{z} \right) = \prod_{t=1}^{T} p_{\theta} \left( y^{(t)} | x^{(t)}, \mathbf{z} \right)\)</span> to ease notation.
Let’s consider what we have achieved in Eq. <a class="reference internal" href="#equation-npvi">(7)</a>.
We have taken our original objective, and re-expressed it as an expectation with respect to <span class="math notranslate nohighlight">\(p_{\theta} \left( \mathbf{z} | \mathcal{D} \right)\)</span>, which now has access to all of <span class="math notranslate nohighlight">\(\mathcal{D}\)</span>, as opposed to the NPML objective, where the expectation was only with respect to <span class="math notranslate nohighlight">\(p_{\theta} \left( \mathbf{z} | \mathcal{C} \right)\)</span>.</p>
<div class="attention admonition">
<p class="admonition-title">Test Time</p>
<p>Of course, we can only sample from this approximate posterior during <em>training</em>, when we have access to both the context <em>and</em> target sets.
At test time, we will only have access to the context set, and so the forward pass through the model will be equivalent to that of the model when trained with NPML, i.e., we will only pass the context set through the encoder.
This is an important detail of NPVI: forward passes at meta-train time look different than they do at meta-test time!</p>
</div>
<div class="dropdown hint admonition">
<p class="admonition-title">Advanced<span class="math notranslate nohighlight">\(\qquad\)</span>LNPF as Amortised VI</p>
<p>The above procedure can be seen as a form of <em>amortised</em> VI.
Amortised VI is a method for performing approximate inference and learning in probabilistic latent variable models, where an <em>inference</em> network is trained to approximate the true posterior distributions.</p>
<p>There are many resources available on (amortised) VI (e.g., <a class="reference external" href="https://jaan.io/what-is-variational-autoencoder-vae-tutorial/">Jaan Altosaar’s VAE tutorial</a>, <a class="reference external" href="http://blog.shakirm.com/2015/01/variational-inference-tricks-of-the-trade/">the Spectator’s</a> take, or this <a class="reference external" href="https://arxiv.org/pdf/1711.05597.pdf">review paper on modern VI</a>), and we encourage readers unfamiliar with the concept to take the time to go through some of these.
For our purposes, the following intuitions should suffice:</p>
<p>Assume that we have a latent variable model with a prior <span class="math notranslate nohighlight">\(p(\mathbf{z})\)</span>, and a conditional likelihood for observations <span class="math notranslate nohighlight">\(y\)</span>, written <span class="math notranslate nohighlight">\(p_{\theta} \left(y | \mathbf{z} \right)\)</span>.
The central idea in amortised VI is to introduce an <em>inference network</em>, denoted <span class="math notranslate nohighlight">\(q_{\phi}(\mathbf{z} | y)\)</span>, which maps observations to distributions over <span class="math notranslate nohighlight">\(\mathbf{z}\)</span>.
We can then use <span class="math notranslate nohighlight">\(q_{\phi}\)</span> to derive a lower bound to the log-marginal likelihood, just as we did above:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align}
\log p_{\theta}(y)
&amp;= \log \int p_{\theta} \left( \mathbf{z} , y \right)\mathrm{d}\mathbf{z} \\
&amp; = \log \int q_{\phi} \left( \mathbf{z} | y \right) \frac{p_{\theta} \left( \mathbf{z} , y \right)}{q_{\phi} \left( \mathbf{z} | y \right)} \mathrm{d}\mathbf{z} \\
&amp; \geq \mathbb{E}_{\mathbf{z} \sim q_{\phi} \left( \mathbf{z} | y \right)} \left[ \log p_{\theta} \left( y | \mathbf{z} \right) \right] - \mathrm{KL} \left( q_{\phi} \left( \mathbf{z} | y \right) \| p_{ \theta} \left( \mathbf{z} \right) \right).
\end{align}
\end{split}\]</div>
<p>In the VI terminology, this lower bound is commonly known as the <em>evidence lower bound</em> (ELBO).
So maximising the ELBO with respect to <span class="math notranslate nohighlight">\(\theta\)</span> trains the model to optimise a lower bound on the log-marginal likelihood, which is a sensible thing to do.
Moreover, it turns out that maximising the ELBO with respect to <span class="math notranslate nohighlight">\(\phi\)</span> minimises the KL divergence between <span class="math notranslate nohighlight">\(q_{\phi} \left( \mathbf{z} | y \right)\)</span> and the true posterior <span class="math notranslate nohighlight">\(p_\theta(\mathbf{z} | y) = p_\theta(y | \mathbf{z}) p_\theta(\mathbf{z}) / p_\theta(y)\)</span>, so we can think of <span class="math notranslate nohighlight">\(q_{\phi}\)</span> as approximating the true posterior in a meaningful way.</p>
<p>In the NPF, to approximate the desired posterior, we can introduce a network that maps datasets to distributions over the latent variable.
We already know how to define such networks in the NPF – that’s exactly what the encoder <span class="math notranslate nohighlight">\(p_{\theta}(\mathbf{z} | \mathcal{C})\)</span> does!
In fact, NPVI proposes to use the encoder as the inference network when training LNPF members.</p>
<p>So we can view Eq.<a class="reference internal" href="#equation-npvi">(7)</a> as performing amortised VI for any member of the LNPF.
The twist on standard amortised VI is that here, we are sharing <span class="math notranslate nohighlight">\(q_{\phi}\)</span> with a part of the model itself, since <span class="math notranslate nohighlight">\(p_{\theta}(\mathbf{z} | \mathcal{D})\)</span> plays the dual role of being an approximate posterior <span class="math notranslate nohighlight">\(q_{\phi}\)</span>, and also defining the <em>conditional prior</em> having observed <span class="math notranslate nohighlight">\(\mathcal{D}\)</span>. This somewhat complicates our understanding of the procedure, and could lead to unintended consequences.</p>
</div>
<p>When both the encoder and inference network parameterise Gaussian distributions over <span class="math notranslate nohighlight">\(\mathbf{z}\)</span> (as is standard), the KL-term can be computed analytically.
Hence we can derive an unbiased estimator to Eq.<a class="reference internal" href="#equation-npvi">(7)</a> by taking samples from <span class="math notranslate nohighlight">\(p_{\theta} \left( \mathbf{z} | \mathcal{D} \right)\)</span> to estimate the first term on the RHS.
<a class="reference internal" href="#npvi-pseudocode"><span class="std std-numref">Fig. 33</span></a> provides the pseudo-code for a single training iteration for a LNPF member, using NPVI as the target objective.</p>
<div class="figure align-default" id="npvi-pseudocode">
<a class="reference internal image-reference" href="../_images/alg_npvi.png"><img alt="Pseudo-code NPVI." src="../_images/alg_npvi.png" style="width: 35em;" /></a>
<p class="caption"><span class="caption-number">Fig. 33 </span><span class="caption-text">Pseudo-code for a single training step of a LNPF member with NPVI.</span><a class="headerlink" href="#npvi-pseudocode" title="Permalink to this image">¶</a></p>
</div>
<div class="warning dropdown admonition">
<p class="admonition-title">Warning<span class="math notranslate nohighlight">\(\qquad\)</span>Biased estimators</p>
<p>Importantly, it is not the case that NPVI avoids the “biased estimator” issue of NPML.
Rather, NPML defines a biased (conservative) estimator of the desired quantity, and NPVI produces an unbiased estimator of a strict lower-bound of the same quantity.
In both cases, unfortunately, we do not have unbiased estimators of the quantity we really care about — <span class="math notranslate nohighlight">\(\log p_{\boldsymbol\theta}(\mathbf{y}_\mathcal{T} | \mathbf{x}_\mathcal{T}, \mathcal{C})\)</span>.</p>
<p>Another consequence of this is that it is challenging to evaluate the models quantitatively, since our performance metrics are only ever lower bounds to the true model performance, and quantifying how tight those bounds are is quite challenging.</p>
</div>
<p>To better understand the NPVI objective, it is important to note — see the box below — that it can be rewritten as the difference between the desired log marginal likelihood and a KL divergence between approximate and true posterior:</p>
<div class="math notranslate nohighlight" id="equation-npvi2">
<span class="eqno">(8)<a class="headerlink" href="#equation-npvi2" title="Permalink to this equation">¶</a></span>\[\mathcal{L}_{VI} = \log p_{\theta}(\mathbf{y}_\mathcal{T} | \mathbf{x}_\mathcal{T}, \mathcal{C}) - \mathrm{KL} \left( p_{\theta} \left( \mathbf{z} | \mathcal{D} \right) \| p \left( \mathbf{z} | \mathcal{C}, \mathcal{T} \right) \right) \]</div>
<p>The NPVI can thus be seen as maximizing the desired log marginal likelihood as well as forcing the approximate and true posterior to be similar.</p>
<div class="hint dropdown admonition">
<p class="admonition-title">Advanced<span class="math notranslate nohighlight">\(\qquad\)</span>Relationship between NPML and NPVI</p>
<p>There is a close relationship between the NPML and NPVI objectives.
To see this, we denote the <em>normalising constant</em> for the distribution <span class="math notranslate nohighlight">\(p_{\theta} \left( \mathbf{y}_{\mathcal{T}}, \mathbf{z} | \mathbf{x}_{\mathcal{T}}, \mathcal{C} \right)\)</span> as</p>
<div class="math notranslate nohighlight">
\[
\begin{align}
  Z = \int p_{\theta} \left( \mathbf{y}_{\mathcal{T}} | \mathbf{x}_{\mathcal{T}}, \mathbf{z} \right) p_{\theta} \left( \mathbf{z} | \mathcal{C} \right) \mathrm{d}\mathbf{z}.
\end{align}
\]</div>
<p>Now, we can rewrite the NPVI objective as</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align}
\mathcal{L}_{VI} &amp; (\theta ; D) = \mathbb{E}_{p_{\theta}(\mathbf{z} | \mathcal{D})} \left[ \log p_{\theta} \left( \mathbf{y}_{\mathcal{T}} | \mathbf{x}_{\mathcal{T}}, \mathbf{z} \right) \right] - \mathrm{KL} \left( p_{\theta} \left( \mathbf{z} | \mathcal{D} \right) \| p_{ \theta} \left( \mathbf{z} | \mathcal{C} \right) \right) \\
&amp; = \mathbb{E}_{p_{\theta}(\mathbf{z} | \mathcal{D})} \left[ \log p_{\theta} \left( \mathbf{y}_{\mathcal{T}} | \mathbf{x}_{\mathcal{T}}, \mathbf{z} \right) + \log p_{\theta} \left( \mathbf{z} | \mathcal{C} \right) - \log p_{\theta} \left( \mathbf{z} | \mathcal{D} \right) \right] \\
&amp; = \mathbb{E}_{p_{\theta}(\mathbf{z} | \mathcal{D})} \left[ \log Z + \log \frac{1}{Z} p_{\theta} \left( \mathbf{y}_{\mathcal{T}}, \mathbf{z} | \mathbf{x}_{\mathcal{T}}, \mathcal{C} \right) - \log p_{\theta} \left( \mathbf{z} | \mathcal{D} \right) \right] \\
&amp; = \log p_{\theta}(\mathbf{y}_\mathcal{T} | \mathbf{x}_\mathcal{T}, \mathcal{C}) - \mathrm{KL} \left( p_{\theta} \left( \mathbf{z} | \mathcal{D} \right) \| \frac{1}{Z} p_{\theta} \left( \mathbf{y}_{\mathcal{T}}, \mathbf{z} | \mathbf{x}_{\mathcal{T}}, \mathcal{C} \right) \right) \\
&amp; = \mathcal{L}_{ML}(\theta; \mathcal{D}) - \mathrm{KL} \left( p_{\theta} \left( \mathbf{z} | \mathcal{D} \right) \| p \left( \mathbf{z} | \mathcal{C}, \mathcal{T} \right) \right) .
\end{align}
\end{split}\]</div>
<p>Thus, we can see that <span class="math notranslate nohighlight">\(\mathcal{L}_{VI}\)</span> is equal to <span class="math notranslate nohighlight">\(\mathcal{L}_{ML}\)</span> (with infinitely many samples) up to an additional KL term.
This KL term has a nice interpretation as encouraging consistency among predictions with different context sets, which is the kind of consistency <em>not</em> baked into the NPF.</p>
<p>However, this term can also be a <em>distractor</em>.
When dealing with NPF members, we are typically not interested in the latent variable <span class="math notranslate nohighlight">\(\mathbf{z}\)</span>, and most considered tasks require only a “good” approximation to the predictive distribution over <span class="math notranslate nohighlight">\(\mathbf{y}_{\mathcal{T}}\)</span>.
Therefore, given only finite model capacity, and finite data, it may be preferable to focus all the model capacity on achieving the best possible predictive distribution (which is what <span class="math notranslate nohighlight">\(\mathcal{L}_{ML}\)</span> focuses on), rather than focusing on the distribution over <span class="math notranslate nohighlight">\(\mathbf{z}\)</span>, as encouraged by the KL term introduced by <span class="math notranslate nohighlight">\(\mathcal{L}_{VI}\)</span>.</p>
<p>Notice that if our approximate posterior can recover the true posterior, then the KL term is zero, and we are exactly optimising the log-marginal likelihood.
In practice, however, the inequality holds, meaning that we are only optimising a lower-bound to the quantity we actually care about. Moreover, it is often quite difficult to know how tight this bound may be.</p>
</div>
<p>As we have discussed, the most appealing property of NPVI is that it utilises <em>posterior sampling</em> to reduce the variance of the Monte-Carlo estimator of the intractable expectation.
This means that often we can get away with training models taking just a single sample, resulting in computationally and memory efficient training procedures.
However, it also comes with several drawbacks, which can be roughly summarised as follows:</p>
<ul class="simple">
<li><p>NPVI is focused on approximating the posterior distribution (see Eq.<a class="reference internal" href="#equation-npvi2">(8)</a> ). However, in the NPF setting, we are typically only interested in the predictive distribution <span class="math notranslate nohighlight">\(p(\mathbf{y}_T | \mathbf{x}_T, \mathcal{C})\)</span>, and it is unclear whether focusing our efforts on <span class="math notranslate nohighlight">\(\mathbf{z}\)</span> is beneficial to achieving higher quality predictive distributions.</p></li>
<li><p>In NPVI, the encoder plays a dual role: it is both part of the model, and used for posterior sampling. This fact introduces additional complexities in the training procedure, and it may be that using the encoder as an approximate posterior has a detrimental effect on the resulting predictive distributions.</p></li>
</ul>
<p>As we shall see below, it is often the case that models trained with NPML produce better fits than equivalent models trained with NPVI, at the cost of additional computational and memory costs of training.
Moreover, using NPVI often requires additional tricks and contraints to get good descent performance.</p>
<p>Armed with procedures for training LNPF-members, we turn our attention to the models themselves.
In particular, we next introduce the latent-variable variant of each of the conditional models introduced in the previous section, and we shall see that having addressed the training procedures, the extension to latent variables is quite straightforward from a practical perspective.</p>
</div>
</div>
<div class="section" id="latent-neural-process-lnp">
<h2>Latent Neural Process (LNP)<a class="headerlink" href="#latent-neural-process-lnp" title="Permalink to this headline">¶</a></h2>
<div class="figure align-default" id="computational-graph-lnps-text">
<a class="reference internal image-reference" href="../_images/computational_graph_LNPs1.svg"><img alt="Computational graph LNP" src="../_images/computational_graph_LNPs1.svg" width="400em" /></a>
<p class="caption"><span class="caption-number">Fig. 34 </span><span class="caption-text">Computational graph for LNPS.</span><a class="headerlink" href="#computational-graph-lnps-text" title="Permalink to this image">¶</a></p>
</div>
<p>The latent neural process <a class="bibtex reference internal" href="../zbibliography.html#garnelo2018neural" id="id7">[GSR+18]</a> is the latent counterpart of the CNP, and the first member of the LNPF proposed in the literature.
Given the vector <span class="math notranslate nohighlight">\(R\)</span>, which is computed as in the CNP, we simply pass it through an additional MLP to predict the mean and variance of the latent representation <span class="math notranslate nohighlight">\(\mathbf{z}\)</span>, from which we can produce samples.
The decoder then has the same architecture as that of the CNP, and we can simply pass samples of <span class="math notranslate nohighlight">\(\mathbf{z}\)</span>, together with desired target locations, to produce our predictive distributions.
<a class="reference internal" href="#computational-graph-lnps-text"><span class="std std-numref">Fig. 34</span></a> illustrates the computational graph of the LNP.</p>
<div class="note dropdown admonition">
<p class="admonition-title">Implementation<span class="math notranslate nohighlight">\(\qquad\)</span>Parameterising the Observation Noise</p>
<p>An important detail in the parameterisation of LNPF members is how the predictive standard deviation is handled, there are 2 possibilities</p>
<ul class="simple">
<li><p><em>Heteroskedastic noise</em>: As with CNPF members, the decoder parameterises a mean <span class="math notranslate nohighlight">\(\mu^{(t)}\)</span> and a standard deviation <span class="math notranslate nohighlight">\(\sigma^{(t)}\)</span> for each target location <span class="math notranslate nohighlight">\(x^{(t)}\)</span>.</p></li>
<li><p><em>Homoskedastic noise</em>: The decoder only parameterises a mean <span class="math notranslate nohighlight">\(\mu^{(t)}\)</span>, while the standard deviation <span class="math notranslate nohighlight">\(\sigma\)</span> is a global (learned) observation noise parameter, i.e., it is shared for all target locations.
Note that the uncertainty of the posterior predictive at two different target locations can still be very different.
Indeed, it is influenced by the variance in the <em>means</em> <span class="math notranslate nohighlight">\(\mu^{(t)}\)</span>, which arises from the uncertainty in the latent variable.</p></li>
</ul>
<p>In practice, a heteroskedastic noise usually performs better than the homoskedastic variant.</p>
</div>
<div class="dropdown note admonition">
<p class="admonition-title">Implementation<span class="math notranslate nohighlight">\(\qquad\)</span>”Lower Bounding” Standard Deviations</p>
<p>In the LNPF literature, it is common to “lower bound” the standard deviations of distributions output by models.
This is often achieved by parameterising the standard deviation as</p>
<div class="math notranslate nohighlight">
\[
\begin{align}
	\sigma = \epsilon + (1 - \epsilon) \ln (1 + \exp (f_\sigma)),
\end{align}
\]</div>
<p>where <span class="math notranslate nohighlight">\(\epsilon\)</span> is a small real number (e.g. 0.001), and <span class="math notranslate nohighlight">\(f_\sigma\)</span> is the log standard deviation output by the model.
This “lower bounding” is often used in practice for the standard deviations of both the latent and predictive distributions.
In fact Le et al.<a class="bibtex reference internal" href="../zbibliography.html#le2018empirical" id="id8">[LKG+18]</a> find that this consistently improves performance of the AttnLNP, and recommend it as best practice.
Following the literature, the results displayed in this tutorial all employ such lower bounds.
However, we note that, in our opinion, there is no conceptual justification for this trick, and it is indicative of flaws in the models and or training procedures.</p>
</div>
<p>Throughout the section we train LNPs with NPVI as in the original paper.
Below, we show the predictive distribution of an LNP trained on samples from the RBF-kernel GP, as the number of observed context points from the underlying function increases.</p>
<div class="figure align-default" id="lnp-rbf-text">
<a class="reference internal image-reference" href="../_images/LNP_rbf.gif"><img alt="LNP on GP with RBF kernel" src="../_images/LNP_rbf.gif" style="width: 35em;" /></a>
<p class="caption"><span class="caption-number">Fig. 35 </span><span class="caption-text">Samples from posterior predictive of LNPs (Blue) and the oracle GP (Green) with RBF kernel.</span><a class="headerlink" href="#lnp-rbf-text" title="Permalink to this image">¶</a></p>
</div>
<p><a class="reference internal" href="#graph-model-lnps-text"><span class="std std-numref">Fig. 31</span></a> shows that the latent variable indeed enables coherent sampling from the posterior predictive.
In fact, within the range <span class="math notranslate nohighlight">\([-1, 1]\)</span> the model produces very nice samples, that seem to properly mimic those from the underlying process.
Nevertheless, here too we see that the LNP suffers from the same underfitting issue as discussed with CNPs.
We again see the tendency to overestimate the uncertainty, and often not pass through all the context points with the mean functions.
Moreover, we can observe that beyond the <span class="math notranslate nohighlight">\([-1, 1]\)</span>, the model seems to “give up” on the context points and uncertainty, despite having been trained on the range <span class="math notranslate nohighlight">\([-2, 2]\)</span>.</p>
<div class="dropdown note admonition">
<p class="admonition-title">Note<span class="math notranslate nohighlight">\(\qquad\)</span>NPVI vs NPML</p>
<p>In the main text we trained LNP with NPVI as in the original paper, we compare the results with NPML on different kernels below.</p>
<div class="figure align-default" id="lnp-all-both-objectives-text">
<a class="reference internal image-reference" href="../_images/singlegp_LNP_LatLBTrue_SigLBTrue1.gif"><img alt="LNP on all kernels with both objectives" src="../_images/singlegp_LNP_LatLBTrue_SigLBTrue1.gif" style="width: 35em;" /></a>
<p class="caption"><span class="caption-number">Fig. 36 </span><span class="caption-text">Predictive distributions of LNPs (Blue) and the oracle GP (Green) with (top) RBF, (center) periodic, and (bottom) Noisy Matern kernels. Models were trained with (left) NPML and (right) NPVI.</span><a class="headerlink" href="#lnp-all-both-objectives-text" title="Permalink to this image">¶</a></p>
</div>
<p><a class="reference internal" href="#lnp-all-both-objectives-text"><span class="std std-numref">Fig. 36</span></a> illustrates several interesting points that we tend to see in our experiments.
First, as with the CNP, the model tends to underfit, and in particular fails catastrophically with the periodic kernel.
In some cases, e.g., when trained on the RBF kernel with NPVI, the model seems to collapse the uncertainty arising from the latent variable in certain regions, and rely entirely on the observation noise.
In our experiments, we observe that this tends to occur when training with NPVI, but not with NPML.
Finally, we can see that NPML tends to produce predictive distributions that fit the data better, and tend to have lower uncertainty near the context set points.</p>
</div>
<p>Let us now consider the image experiments as we did with the CNP.</p>
<div class="figure align-default" id="lnp-img-interp-text">
<a class="reference internal image-reference" href="../_images/LNP_img_interp1.gif"><img alt="LNP on CelebA and MNIST" src="../_images/LNP_img_interp1.gif" style="width: 30em;" /></a>
<p class="caption"><span class="caption-number">Fig. 37 </span><span class="caption-text">samples (means conditioned on different samples from the latent) of the posterior predictive of a LNP on CelebA <span class="math notranslate nohighlight">\(32\times32\)</span> and MNIST.
The last row shows the standard deviation of the posterior predictive corresponding to the last sample.</span><a class="headerlink" href="#lnp-img-interp-text" title="Permalink to this image">¶</a></p>
</div>
<p>From <a class="reference internal" href="#lnp-img-interp-text"><span class="std std-numref">Fig. 37</span></a> we see again that the latent variable enables relatively coherent sampling from the posterior predictive.
As with the CNP, the LNP still underfits on images as is best illustrated when the context set is half the image.</p>
<div class="note admonition">
<p class="admonition-title">Details</p>
<p>Model details, training and more plots in <a class="reference internal" href="../reproducibility/LNP.html"><span class="doc">LNP Notebook</span></a></p>
</div>
</div>
<div class="section" id="attentive-latent-neural-process-attnlnp">
<h2>Attentive Latent Neural Process (AttnLNP)<a class="headerlink" href="#attentive-latent-neural-process-attnlnp" title="Permalink to this headline">¶</a></h2>
<p>The Attentive LNPs <a class="bibtex reference internal" href="../zbibliography.html#kim2019attentive" id="id9">[KMS+19]</a> is the latent counterpart of AttnCNPs.
Differently from the LNP, the AttnLNP <em>added</em> a “latent path” in addition to (rather than instead of) the deterministic path.
The latent path is implemented with the same method as LNPs, i.e. a mean aggregation followed by a parametrization of a Gaussian.
In other words, even though the deterministic representation <span class="math notranslate nohighlight">\(R^{(t)}\)</span> is target specific, the latent representation <span class="math notranslate nohighlight">\(\mathbf{z}\)</span>  is target independent as seen in the computational graph (<a class="reference internal" href="#computational-graph-attnlnps-text"><span class="std std-numref">Fig. 38</span></a>).</p>
<div class="figure align-default" id="computational-graph-attnlnps-text">
<a class="reference internal image-reference" href="../_images/computational_graph_AttnLNPs1.svg"><img alt="Computational graph AttnLNP" src="../_images/computational_graph_AttnLNPs1.svg" width="400em" /></a>
<p class="caption"><span class="caption-number">Fig. 38 </span><span class="caption-text">Computational graph for AttnLNPS.</span><a class="headerlink" href="#computational-graph-attnlnps-text" title="Permalink to this image">¶</a></p>
</div>
<p>Throughout the section we train AttnLNPs with NPVI as in the original paper.
Below, we show the predictive distribution of an AttnLNP trained on samples from RBF, periodic, and noisy Matern kernel GPs, again viewing the predictive as the number of observed context points is increased.</p>
<div class="figure align-default" id="attnlnp-single-gp-text">
<a class="reference internal image-reference" href="../_images/AttnLNP_single_gp1.gif"><img alt="AttnLNP on single GP" src="../_images/AttnLNP_single_gp1.gif" style="width: 35em;" /></a>
<p class="caption"><span class="caption-number">Fig. 39 </span><span class="caption-text">Samples Posterior predictive of AttnLNPs (Blue) and the oracle GP (Green) with RBF,periodic, and noisy Matern kernel.</span><a class="headerlink" href="#attnlnp-single-gp-text" title="Permalink to this image">¶</a></p>
</div>
<p><a class="reference internal" href="#attnlnp-single-gp-text"><span class="std std-numref">Fig. 39</span></a> paints an interesting picture regarding the AttnLNP.
On the one hand, we see that it is able to do a significantly better job in modelling the marginals than the LNP.
However, on closer inspection, we can see several issues with the resulting distributions:</p>
<ul class="simple">
<li><p><em>Kinks</em>: The samples do not seem to be smooth, and we see “kinks” that are similar (though even more pronounced) than in <a class="reference internal" href="CNPF.html#attncnp-single-gp-text"><span class="std std-numref">Fig. 14</span></a>.</p></li>
<li><p><em>Collapse to AttnCNP</em>: In many places the AttnLNP seems to collpase the distribution around the latent variable, and express all its uncertainty via the observation noise. This tends to occur more often for the AttnLNP when trained with NPVI rather than NPML.</p></li>
</ul>
<div class="dropdown note admonition">
<p class="admonition-title">Note<span class="math notranslate nohighlight">\(\qquad\)</span>NPVI vs NPML</p>
<p>As with the LNP, we compare the performance of the AttnLNP when trained with NPVI and NPML.</p>
<div class="figure align-default" id="attnlnp-all-both-objectives-text">
<a class="reference internal image-reference" href="../_images/singlegp_AttnLNP_LatLBTrue_SigLBTrue1.gif"><img alt="AttnLNP on all kernels with both objectives" src="../_images/singlegp_AttnLNP_LatLBTrue_SigLBTrue1.gif" style="width: 35em;" /></a>
<p class="caption"><span class="caption-number">Fig. 40 </span><span class="caption-text">Predictive distributions of AttnLNPs (Blue) and the oracle GP (Green) with (top) RBF, (center) periodic, and (bottom) Noisy Matern kernels. Models were trained with (left) NPML and (right) NPVI.</span><a class="headerlink" href="#attnlnp-all-both-objectives-text" title="Permalink to this image">¶</a></p>
</div>
<p>Here we see that the AttnLNP tends to “collapse” for both the RBF and noisy Matern kernels when trained with NPVI.
In contrast, when trained with NPML it tends to avoid this behaviour.
This is consistent with what we observe more generally in our experiments.</p>
</div>
<p>Let us now consider the image experiments as we did with the AttnCNP.</p>
<div class="figure align-default" id="attnlnp-img-text">
<a class="reference internal image-reference" href="../_images/AttnLNP_img1.gif"><img alt="AttnLNP on CelebA, MNIST, ZSMM" src="../_images/AttnLNP_img1.gif" style="width: 45em;" /></a>
<p class="caption"><span class="caption-number">Fig. 41 </span><span class="caption-text">Samples from posterior predictive of an AttnCNP for CelebA <span class="math notranslate nohighlight">\(32\times32\)</span>, MNIST, ZSMM.</span><a class="headerlink" href="#attnlnp-img-text" title="Permalink to this image">¶</a></p>
</div>
<p>From <a class="reference internal" href="#attnlnp-img-text"><span class="std std-numref">Fig. 41</span></a> we see that AttnLNP generates quite impressive samples, and exhibits descent sampling and good performances when the model does not require generalisation (CelebA <span class="math notranslate nohighlight">\(32\times32\)</span>, MNIST).
However, as expected, the model “breaks” for ZSMM as it still cannot extrapolate.</p>
<div class="note admonition">
<p class="admonition-title">Details</p>
<p>Model details, training and more plots in <a class="reference internal" href="../reproducibility/AttnLNP.html"><span class="doc">AttnLNP Notebook</span></a></p>
</div>
</div>
<div class="section" id="convolutional-latent-neural-process-convlnp">
<h2>Convolutional Latent Neural Process (ConvLNP)<a class="headerlink" href="#convolutional-latent-neural-process-convlnp" title="Permalink to this headline">¶</a></h2>
<p>The Convolutional LNP <a class="bibtex reference internal" href="../zbibliography.html#foong2020convnp" id="id10">[FBG+20]</a> is the latent counterpart of the ConvCNP.
In contrast with the AttnLNP, the latent path <em>replaces</em> the deterministic one (as with LNP), resulting in a latent functional representation (a latent stochastic process) instead of a latent vector valued variable.</p>
<p>Another way of viewing the ConvLNP, which is useful in gaining an intuitive understanding of the computational graph (see <a class="reference internal" href="#computational-graph-convlnps-cpnvcnps-text"><span class="std std-numref">Fig. 42</span></a>) is as consisting of two stacked ConvCNPs: the first takes in context sets and outputs a latent stochastic process.
The second takes as input a sample from the latent process and models the posterior predictive conditioned on that sample.</p>
<div class="figure align-default" id="computational-graph-convlnps-cpnvcnps-text">
<a class="reference internal image-reference" href="../_images/computational_graph_ConvLNPs_ConvCNPs.svg"><img alt="Computational graph ConvLNP using two ConvCNPs" src="../_images/computational_graph_ConvLNPs_ConvCNPs.svg" width="400em" /></a>
<p class="caption"><span class="caption-number">Fig. 42 </span><span class="caption-text">Computational graph for ConvLNPs.</span><a class="headerlink" href="#computational-graph-convlnps-cpnvcnps-text" title="Permalink to this image">¶</a></p>
</div>
<div class="note dropdown admonition">
<p class="admonition-title">Implementation<span class="math notranslate nohighlight">\(\qquad\)</span>ConvLNP without ConvCNP</p>
<p>In our implementation we do not use two ConvCNPs to implement the ConvLNP.
Instead, we use the same computational graph than for ConvCNPs but split the CNN into two.
The output of the first CNN is a mean and a (log) standard deviation at every position of the (discrete) signal, which are then used to parametrize independent Gaussian disitrubtions each positions.
The second CNN gets as input a sample from the first one, i.e., a discrete signal.</p>
<p>The computational graph for this implementation is shown in <a class="reference internal" href="#computational-graph-convlnps-text"><span class="std std-numref">Fig. 43</span></a>.
Importantly both are mathematically equivalent, but using two ConvCNPs can be easier to understand and more modular, while using two CNNs avoids unecessary computations.</p>
<div class="figure align-default" id="computational-graph-convlnps-text">
<a class="reference internal image-reference" href="../_images/computational_graph_ConvLNPs1.svg"><img alt="Computational graph ConvLNP" src="../_images/computational_graph_ConvLNPs1.svg" width="400em" /></a>
<p class="caption"><span class="caption-number">Fig. 43 </span><span class="caption-text">Computational graph for ConvLNPs without using ConvCNPs.</span><a class="headerlink" href="#computational-graph-convlnps-text" title="Permalink to this image">¶</a></p>
</div>
</div>
<p>One difficulty arises in training the ConvLNP with the NPVI objective, as it requires evaluating the KL divergence between two stochastic processes, which is a tricky proposition.
<a class="bibtex reference internal" href="../zbibliography.html#foong2020convnp" id="id11">[FBG+20]</a> propose a simple approach, that approximates this quantity by instead summing the KL divergences at each discretisation location.
However, as they note, the ConvLNP performs significantly better in most cases when trained with NPML rather than NPVI.
Throughout this section we will thus use NPML instead of NPVI.</p>
<div class="note dropdown admonition">
<p class="admonition-title">Note<span class="math notranslate nohighlight">\(\qquad\)</span>Global Latent Representations</p>
<p>In this tutorial, we consider a simple extension to the ConvLNP proposed by Foong et. al <a class="bibtex reference internal" href="../zbibliography.html#foong2020convnp" id="id12">[FBG+20]</a>, which includes a <em>global latent representation</em> as well.
The global representation is computed by average-pooling half of the channels in the latent function, resulting in a translation <em>invariant</em> latent representation, in addition to the translation equivariant one.
The intuition behind such a representation is that it may help to capture aspects of the underlying function that are global, allowing the functional representation to represent more localised information.
This intuition is clearest when considering the mixture of GPs experiments discussed below.</p>
</div>
<div class="figure align-default" id="convlnp-single-gp-extrap-text">
<a class="reference internal image-reference" href="../_images/ConvLNP_single_gp_extrap1.gif"><img alt="ConvLNP on GPs with RBF, periodic, Matern kernel" src="../_images/ConvLNP_single_gp_extrap1.gif" style="width: 35em;" /></a>
<p class="caption"><span class="caption-number">Fig. 44 </span><span class="caption-text">Samples Posterior predictive of ConvLNPs (Blue) and the oracle GP (Green) with RBF,periodic, and noisy Matern kernel.</span><a class="headerlink" href="#convlnp-single-gp-extrap-text" title="Permalink to this image">¶</a></p>
</div>
<p>From <a class="reference internal" href="#convlnp-single-gp-extrap-text"><span class="std std-numref">Fig. 44</span></a> we see that ConvLNP performs very well and the samples are reminiscent of those from a GP, i.e., with much richer variability compared to <a class="reference internal" href="#attnlnp-single-gp-text"><span class="std std-numref">Fig. 39</span></a>.
Further, as in the case of the ConvCNP, we see that the ConvLNP elegantly generalises beyond the range in <span class="math notranslate nohighlight">\(X\)</span>-space on which it was trained.</p>
<p>Next, we consider the more challenging problem of having the ConvLNP model a stochastic process whose posterior predictive is non Gaussian.
We do so by having the following underlying generative process: first, sample one of the 3 kernels discussed above, and second, sample a function from the sampled kernel.
Importantly, the data generating process is a mixture of GPs, and the true posterior predictive process (achieved by marginalising over the different kernels) is non-Gaussian.</p>
<!-- Theoretically, this could still be modelled by a LNPF as the latent variables could represent the both the current kernel and its functions, but adding the global representation corresponds (intuitively) to allowing the ConvLNP to "switch" between kernels. -->
<div class="figure align-default" id="convlnp-kernel-gp-text">
<a class="reference internal image-reference" href="../_images/ConvLNP_kernel_gp1.gif"><img alt="ConvLNP trained on GPs with RBF,Matern,periodic kernel" src="../_images/ConvLNP_kernel_gp1.gif" style="width: 35em;" /></a>
<p class="caption"><span class="caption-number">Fig. 45 </span><span class="caption-text">Similar to <a class="reference internal" href="#convlnp-single-gp-extrap-text"><span class="std std-numref">Fig. 44</span></a> but the training was performed on all data simultaneously.</span><a class="headerlink" href="#convlnp-kernel-gp-text" title="Permalink to this image">¶</a></p>
</div>
<p><a class="reference internal" href="#convlnp-kernel-gp-text"><span class="std std-numref">Fig. 45</span></a> demonstrates that ConvLNP performs quite well in this harder setting.
Indeed, it seems to model the predictive process using the periodic kernel when the number of context points is small but quickly (around 10 context points) recovers the correct underlying kernel.
Note how in the middle plot, the ConvLNP becomes progressively more and more “confident” that the process is periodic as more data is observed.
Note that in <a class="reference internal" href="#convlnp-kernel-gp-text"><span class="std std-numref">Fig. 45</span></a> we are plotting the posterior predictive for the sampled GP, rather than the actual, non-GP posterior predictive process.</p>
<p>Again, we consider the performance of the ConvLNP in the image setting.
In <a class="reference internal" href="#convlnp-img-text"><span class="std std-numref">Fig. 46</span></a> we see that the ConvLNP does a reasonable job producing samples when the context sets are uniformly subsampled from images, but struggles with the “structured” context sets, e.g. when the left or bottom halves of the image are missing.
Moreover, the ConvLNP is able to produce samples in the generalisation setting (ZSMM), but these are not always coherent, and include some strange artifacts that seem more similar to sampling the MNIST “texture” than coherent digits.</p>
<div class="figure align-default" id="convlnp-img-text">
<a class="reference internal image-reference" href="../_images/ConvLNP_img1.gif"><img alt="ConvLNP on CelebA, MNIST, ZSMM" src="../_images/ConvLNP_img1.gif" style="width: 45em;" /></a>
<p class="caption"><span class="caption-number">Fig. 46 </span><span class="caption-text">Samples from posterior predictive of an ConvCNP for CelebA <span class="math notranslate nohighlight">\(32\times32\)</span>, MNIST, ZSMM.</span><a class="headerlink" href="#convlnp-img-text" title="Permalink to this image">¶</a></p>
</div>
<p>As discussed in  <a class="reference internal" href="CNPF.html#issues-cnpfs"><span class="std std-ref">the ‘Issues with the CNPF’ section</span></a>, members of the CNPF could not be used to generate coherent samples, nor model non-Gaussian posterior predictive distributions.
In contrast, <a class="reference internal" href="#convlnp-marginal-text"><span class="std std-numref">Fig. 47</span></a> (right) demonstrates that, as expected, ConvLNP is able to produce non-Gaussian predictives for pixels, with interesting bi-modal and heavy-tailed behaviours.</p>
<div class="figure align-default" id="convlnp-marginal-text">
<a class="reference internal image-reference" href="../_images/ConvLNP_marginal.png"><img alt="Samples from ConvLNP on MNIST and posterior of different pixels" src="../_images/ConvLNP_marginal.png" style="width: 20em;" /></a>
<p class="caption"><span class="caption-number">Fig. 47 </span><span class="caption-text">Samples form the posterior predictive of ConvCNPs on MNIST (left) and posterior predictive of some pixels (right).</span><a class="headerlink" href="#convlnp-marginal-text" title="Permalink to this image">¶</a></p>
</div>
<p>From <a class="reference internal" href="#attnlnp-img-text"><span class="std std-numref">Fig. 41</span></a> we see that AttnLNP generates quite impressive samples, and exhibits descent sampling and good performances when the model does not require generalisation (CelebA <span class="math notranslate nohighlight">\(32\times32\)</span>, MNIST).
However, as expected, the model “breaks” for ZSMM as it still cannot extrapolate.</p>
<div class="note admonition">
<p class="admonition-title">Details</p>
<p>Model details, training and more plots in <a class="reference internal" href="../reproducibility/AttnLNP.html"><span class="doc">AttnLNP Notebook</span></a></p>
</div>
</div>
<div class="section" id="issues-and-discussion">
<h2>Issues and Discussion<a class="headerlink" href="#issues-and-discussion" title="Permalink to this headline">¶</a></h2>
<p>We have seen how members of the LNPF utilise a latent variable to define a predictive distribution, thus achieving structured and expressive predictive distributions over target sets.
Despite these advantages, the LNPF suffers from important drawbacks:</p>
<ul class="simple">
<li><p>The training procedure only optimises a biased objective or a lower bound to the true objective.</p></li>
<li><p>Approximating the objective function requires sampling, which can lead to high variance during training.</p></li>
<li><p>They are more memory and computationally demanding, requiring many samples to estimate the objective for NPML.</p></li>
<li><p>It is difficult to quantitatively evaluate and compare different models, since only lower bounds to the log predictive likelihood can be estimated.</p></li>
</ul>
<p>Despite these challenges, the LNPF defines a useful and powerful class of models.
Whether to deploy a member of the CNPF or LNPF depends on the task at hand.
For example, if samples are not required for a particular application, and we have reason to believe a parametric distribution may be a good description of the likelihood, it may well be that a CNPF would be preferable.
Conversely, it is crucial to use the LNPF if sampling or dependencies in the predictive are required, for example in Thompson sampling.</p>
<!-- Finally, we believe there is an exciting area of research in developing NPF members that enjoy the best of both worlds.
Some examples of potential avenues for investigation are:
1. Improving training procedures for the LNPF using ideas from importance sampling and unbiased estimators of the marginal likelihood.
2. Considering flow-based parameterisations that admit closed form likelihood computations, while potentially maintaining structure in the predictive distribution for the target set. -->
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./text"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
        </div>
    </div>
    
    
    <div class='prev-next-bottom'>
        
    <a class='left-prev' id="prev-link" href="CNPF.html" title="previous page">Conditional NPF</a>
    <a class='right-next' id="next-link" href="../reproducibility/Datasets.html" title="next page">Datasets</a>

    </div>
    <footer class="footer mt-5 mt-md-0">
    <div class="container">
      <p>
        
          By Yann Dubois, Jonathan Gordon, ‪Andrew Foong<br/>
        
            &copy; Copyright 2020.<br/>
      </p>
    </div>
  </footer>
</main>


      </div>
    </div>

    <script src="../_static/js/index.js"></script>
    
  </body>
</html>