

<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta charset="utf-8" />
    <title>Latent NPFs &#8212; Neural Process Family</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.11.2/css/all.min.css" integrity="sha384-KA6wR/X5RY4zFAHpv/CnoG2UW1uogYfdnP67Uv7eULvTveboZJg0qUpmJZb5VqzN" crossorigin="anonymous">
    <link href="../_static/css/index.css" rel="stylesheet">
    <link rel="stylesheet" href="../_static/sphinx-book-theme.css" type="text/css" />
    <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../_static/proof.css" />
    <link rel="stylesheet" type="text/css" href="../_static/jupyter-sphinx.css" />
    <script id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
    <script src="../_static/sphinx-book-theme.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/language_data.js"></script>
    <script src="../_static/togglebutton.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script src="../_static/mystnb.js"></script>
    <script src="../_static/proof.js"></script>
    <script src="../_static/sphinx-book-theme.js"></script>
    <script >var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <script src="https://unpkg.com/@jupyter-widgets/html-manager@^0.18.0/dist/embed-amd.js"></script>
    <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <link rel="shortcut icon" href="../_static/favicon.ico"/>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Theory" href="Theory.html" />
    <link rel="prev" title="Conditional NPFs" href="CNPFs.html" />

    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="docsearch:language" content="en">



  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
<a class="navbar-brand text-wrap" href="../index.html">
  
  <img src="../_static/logo.gif" class="logo" alt="logo">
  
  
  <h1 class="site-logo" id="site-title">Neural Process Family</h1>
  
</a>
</div>

<form class="bd-search d-flex align-items-center" action="../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form>

<nav class="bd-links" id="bd-docs-nav" aria-label="Main navigation">
  
  <ul class="nav sidenav_l1">
  <li class="">
    <a href="Intro.html">Neural Process Family</a>
  </li>
  <li class="">
    <a href="CNPFs.html">Conditional NPFs</a>
  </li>
  <li class="active">
    <a href="">Latent NPFs</a>
  </li>
<li class="navbar-special">
<p class="margin-caption">Advanced</p>
</li>
  <li class="">
    <a href="Theory.html">Theory</a>
  </li>
  <li class="">
    <a href="Training.html">Training</a>
  </li>
<li class="navbar-special">
<p class="margin-caption">Reproducibility</p>
</li>
  <li class="">
    <a href="../reproducibility/Datasets.html">Datasets</a>
  </li>
  <li class="">
    <a href="../reproducibility/CNP.html">CNP</a>
  </li>
  <li class="">
    <a href="../reproducibility/AttnCNP.html">AttnCNP</a>
  </li>
  <li class="">
    <a href="../reproducibility/ConvCNP.html">ConvCNP</a>
  </li>
  <li class="">
    <a href="../reproducibility/LNP.html">LNP</a>
  </li>
  <li class="">
    <a href="../reproducibility/AttnLNP.html">AttnLNP</a>
  </li>
  <li class="">
    <a href="../reproducibility/ConvLNP.html">ConvLNP</a>
  </li>
<li class="navbar-special">
<p class="margin-caption">Reference</p>
</li>
  <li class="">
    <a href="Related.html">Related</a>
  </li>
  <li class="">
    <a href="../zbibliography.html">Bibliography</a>
  </li>
  <li class="">
    <a href="https://github.com/YannDubs/Neural-Process-Family">GitHub Repo<i class="fas fa-external-link-alt"></i></a>
  </li>
</ul>
</nav>

 <!-- To handle the deprecated key -->

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="row topbar fixed-top container-xl">
    <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show">
    </div>
    <div class="col pl-2 topbar-main">
        
        <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
            data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
            aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
            title="Toggle navigation" data-toggle="tooltip" data-placement="left">
            <i class="fas fa-bars"></i>
            <i class="fas fa-arrow-left"></i>
            <i class="fas fa-arrow-up"></i>
        </button>
        
        <div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    
    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="../_sources/text/LNPFs.md"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.md</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
            onClick="window.print()" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
    
</div>
        <!-- Source interaction buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Connect with source repository"><i class="fab fa-github"></i></button>
    <div class="dropdown-buttons sourcebuttons">
        <a class="repository-button"
            href="https://github.com/YannDubs/Neural-Process-Family"><button type="button" class="btn btn-secondary topbarbtn"
                data-toggle="tooltip" data-placement="left" title="Source repository"><i
                    class="fab fa-github"></i>repository</button></a>
        
        
    </div>
</div>


        <!-- Full screen (wrap in <a> to have style consistency -->
        <a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
                data-placement="bottom" onclick="toggleFullScreen()" title="Fullscreen mode"><i
                    class="fas fa-expand"></i></button></a>

        <!-- Launch buttons -->

    </div>
    <div class="d-none d-md-block col-md-2 bd-toc show">
<div class="tocsection onthispage pt-5 pb-3">
    <i class="fas fa-list"></i> On this page
</div>

<nav id="bd-toc-nav">
    <ul class="nav section-nav flex-column">
    
        <li class="nav-item toc-entry toc-h2">
            <a href="#overview" class="nav-link">Overview</a>
        </li>
    
        <li class="nav-item toc-entry toc-h2">
            <a href="#training" class="nav-link">Training</a>
        </li>
    
        <li class="nav-item toc-entry toc-h2">
            <a href="#latent-neural-process-lnp" class="nav-link">Latent Neural Process (LNP)</a>
        </li>
    
        <li class="nav-item toc-entry toc-h2">
            <a href="#attentive-latent-neural-process-attnlnp" class="nav-link">Attentive Latent Neural Process (AttnLNP)</a>
        </li>
    
        <li class="nav-item toc-entry toc-h2">
            <a href="#convolutional-latent-neural-process-convlnp" class="nav-link">Convolutional Latent Neural Process (ConvLNP)</a><ul class="nav section-nav flex-column">
                
        <li class="nav-item toc-entry toc-h3">
            <a href="#issues-of-lnpfs" class="nav-link">Issues of LNPFs</a>
        </li>
    
            </ul>
        </li>
    
    </ul>
</nav>


    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
        
              <div>
                
  <div class="section" id="latent-npfs">
<h1>Latent NPFs<a class="headerlink" href="#latent-npfs" title="Permalink to this headline">¶</a></h1>
<div class="section" id="overview">
<h2>Overview<a class="headerlink" href="#overview" title="Permalink to this headline">¶</a></h2>
<p>In the previous chapter we discussed about CNPFs and argued that although veru useful for arg maximizing the posterior predictive it cannot model arbitrarily complex posterior predictives nor can it be used to sample coherent functions from the posterior predictive.
Here we will show how to tackle these issues by treating the representation as a latent variable.</p>
<p>The three previously discussed CNPFs can have a corresponding latent variable model (LNPF)<a class="footnote-reference brackets" href="#lnps" id="id1">1</a>, essentially instead of generating a deterministic representation of the context set, the encoder parametrizes a distribution over representations from which we will sample to give rise to different coherent sampled functions from the posterior predictive. By marginalizing over the latent varable LNPFs can also model the arbitrarily complex posterior predictive</p>
<ul class="simple">
<li><p>equations</p></li>
<li><p>more intuition</p></li>
</ul>
</div>
<div class="section" id="training">
<h2>Training<a class="headerlink" href="#training" title="Permalink to this headline">¶</a></h2>
<p>For training it is the same discussion as usual latent variable models</p>
<ol class="simple">
<li><p>ideally max likelihood (<a class="reference internal" href="Theory.html"><span class="doc">Additional theory</span></a>). Sampling is possible due to reparametrixation trick [cite], but still computationnaly intractable due to the marginalization so use monte carlo estimation. Problem:  biased, requires many samples since we are in a regime similar to prior sampling (<a class="reference internal" href="Theory.html"><span class="doc">Additional theory</span></a>).</p></li>
<li><p>An alternative route is to take a VI-inspired approach. Here, we move to “posterior sampling”, so may expect to work better with fewer samples. However (i) the interpretation of the approximate posterior is quite troublesome ((<a class="reference internal" href="Theory.html"><span class="doc">Additional theory</span></a>), and in fact we often don’t care about the latent distribution. (ii) This can be viewed as an odd regularization procedure on ML training, and is not clearly justified. […equation and approx…]</p></li>
</ol>
<p>In the end, determining the “better” approach may be task-dependent, and require empirical evaluation.</p>
<div class="tip admonition">
<p class="admonition-title">Details</p>
<p>Model details in <a class="reference internal" href="Training.html"><span class="doc">Additional Training</span></a>)</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>If you are familiar with the literature on VAE, the disctinction is essentially the same as training with ELBO or IWAE […]
With the additional issue that the conditional prior cannot is unkown so we replace it with the variational distribution =&gt; not proper lower bound any more</p>
</div>
</div>
<div class="section" id="latent-neural-process-lnp">
<h2>Latent Neural Process (LNP)<a class="headerlink" href="#latent-neural-process-lnp" title="Permalink to this headline">¶</a></h2>
<div class="figure align-default" id="graph-model-lnps-text">
<a class="reference internal image-reference" href="../_images/graph_model_LNPs.svg"><img alt="graphical model LNP" src="../_images/graph_model_LNPs.svg" width="200em" /></a>
<p class="caption"><span class="caption-number">Fig. 22 </span><span class="caption-text">Graphical model for LNPs.</span><a class="headerlink" href="#graph-model-lnps-text" title="Permalink to this image">¶</a></p>
</div>
<div class="figure align-default" id="computational-graph-lnps-text">
<a class="reference internal image-reference" href="../_images/computational_graph_LNPs1.svg"><img alt="Computational graph LNP" src="../_images/computational_graph_LNPs1.svg" width="300em" /></a>
<p class="caption"><span class="caption-number">Fig. 23 </span><span class="caption-text">Computational graph for LNPS. [drop?]</span><a class="headerlink" href="#computational-graph-lnps-text" title="Permalink to this image">¶</a></p>
</div>
<p>The latent neural process <a class="bibtex reference internal" href="../zbibliography.html#garnelo2018neural" id="id2">[GSR+18]</a> is the latent counterpart of CNP, i.e. once we have a representatoin <span class="math notranslate nohighlight">\(R\)</span> we will pass it through a MLP to predict the mean and variance of the latent representation from which to sample.
See <a class="reference internal" href="#graph-model-lnps-text"><span class="std std-numref">Fig. 22</span></a> for the graphical model and <a class="reference internal" href="#computational-graph-lnps-text"><span class="std std-numref">Fig. 23</span></a> for the computational graph.</p>
<div class="dropdown caution admonition">
<p class="admonition-title">Advanced</p>
<p>Theoretical gains of using a latent variable
More information in <a class="reference internal" href="Theory.html"><span class="doc">Additional Theory</span></a></p>
</div>
<p>Note on heteroskedastic noise which is strange and as a result in the case of modeling GPs it collapses to CNPs if you do not use a lower bound[]…</p>
<div class="figure align-default" id="lnp-rbf-text">
<a class="reference internal image-reference" href="../_images/LNP_rbf.gif"><img alt="LNP on GP with RBF kernel" src="../_images/LNP_rbf.gif" style="width: 35em;" /></a>
<p class="caption"><span class="caption-number">Fig. 24 </span><span class="caption-text">Samples from posterior predictive of LNPs (Blue) and the oracle GP (Green) with RBF kernel.</span><a class="headerlink" href="#lnp-rbf-text" title="Permalink to this image">¶</a></p>
</div>
<p><a class="reference internal" href="#graph-model-lnps-text"><span class="std std-numref">Fig. 22</span></a> shows that the latent variable indeed enables coherent sampling from the posterior predictive.
It nevertheless suffers from the same underfitting issue as discussed with CNPs.</p>
<div class="tip admonition">
<p class="admonition-title">Details</p>
<p>Model details, training and many more plots in <a class="reference internal" href="../reproducibility/LNP.html"><span class="doc">LNP Notebook</span></a></p>
</div>
</div>
<div class="section" id="attentive-latent-neural-process-attnlnp">
<h2>Attentive Latent Neural Process (AttnLNP)<a class="headerlink" href="#attentive-latent-neural-process-attnlnp" title="Permalink to this headline">¶</a></h2>
<div class="figure align-default" id="graph-model-attnlnps-text">
<a class="reference internal image-reference" href="../_images/graph_model_AttnLNPs.svg"><img alt="graphical model AttnLNP" src="../_images/graph_model_AttnLNPs.svg" width="200em" /></a>
<p class="caption"><span class="caption-number">Fig. 25 </span><span class="caption-text">Graphical model for AttnLNPs.</span><a class="headerlink" href="#graph-model-attnlnps-text" title="Permalink to this image">¶</a></p>
</div>
<div class="figure align-default" id="computational-graph-attnlnps-text">
<a class="reference internal image-reference" href="../_images/computational_graph_AttnLNPs1.svg"><img alt="Computational graph AttnLNP" src="../_images/computational_graph_AttnLNPs1.svg" width="400em" /></a>
<p class="caption"><span class="caption-number">Fig. 26 </span><span class="caption-text">Computational graph for AttnLNPS. [drop?]</span><a class="headerlink" href="#computational-graph-attnlnps-text" title="Permalink to this image">¶</a></p>
</div>
<p>The Attentive LNPs <a class="bibtex reference internal" href="../zbibliography.html#kim2019attentive" id="id3">[KMS+19]</a> is the latent counterpart of AttnCNPs. The way they incorporated the latent variable is a little different than other LNPs, in that they added a “latent path” in addition (not instead) of the deterministic path, giving rise to the (strange?) graphical model depicted in <a class="reference internal" href="#graph-model-attnlnps-text"><span class="std std-numref">Fig. 25</span></a>.
The latent path is implemented with the same method as LNPs, i.e. a mean aggregation followed by a parametrization of a Gaussian.
In other words, even though the deterministic representation is <span class="math notranslate nohighlight">\(R^{(t)}\)</span> is target specific, the latent representation <span class="math notranslate nohighlight">\(\mathrm{Z}\)</span>  is target independent as seen in the computational graph (<a class="reference internal" href="#computational-graph-attnlnps-text"><span class="std std-numref">Fig. 26</span></a>).</p>
<div class="figure align-default" id="attnlnp-single-gp-text">
<a class="reference internal image-reference" href="../_images/AttnLNP_single_gp1.gif"><img alt="AttnLNP on single GP" src="../_images/AttnLNP_single_gp1.gif" style="width: 35em;" /></a>
<p class="caption"><span class="caption-number">Fig. 27 </span><span class="caption-text">Samples Posterior predictive of AttnLNPs (Blue) and the oracle GP (Green) with RBF,periodic, and noisy Matern kernel.</span><a class="headerlink" href="#attnlnp-single-gp-text" title="Permalink to this image">¶</a></p>
</div>
<p>From <a class="reference internal" href="#attnlnp-single-gp-text"><span class="std std-numref">Fig. 27</span></a> we see that although the marginal posterior predictive seem good the samples:</p>
<ol class="simple">
<li><p>are not very smooth (the “kinks” seed in <a class="reference internal" href="CNPFs.html#attncnp-single-gp-text"><span class="std std-numref">Fig. 10</span></a> are even more obvious when sampling);</p></li>
<li><p>lack diversity and seem to be shifted versions of each other. This is probably because having a very expressive deterministic path diminishes the need of a useful latent path.</p></li>
</ol>
<p>Let us now look at images.</p>
<div class="figure align-default" id="attnlnp-img-text">
<a class="reference internal image-reference" href="../_images/AttnLNP_img1.gif"><img alt="AttnLNP on CelebA, MNIST, ZSMM" src="../_images/AttnLNP_img1.gif" style="width: 45em;" /></a>
<p class="caption"><span class="caption-number">Fig. 28 </span><span class="caption-text">Samples from posterior predictive of an AttnCNP for CelebA <span class="math notranslate nohighlight">\(32\times32\)</span>, MNIST, ZSMM.</span><a class="headerlink" href="#attnlnp-img-text" title="Permalink to this image">¶</a></p>
</div>
<p>From <a class="reference internal" href="#attnlnp-img-text"><span class="std std-numref">Fig. 28</span></a> we see that AttnLNP generates nice samples shows descent sampling and good performances when the model does not require generalization (CelebA <span class="math notranslate nohighlight">\(32\times32\)</span>, MNIST) but breaks for ZSMM as it still cannot extrapolate.</p>
<div class="tip admonition">
<p class="admonition-title">Details</p>
<p>Model details, training and many more plots in <a class="reference internal" href="../reproducibility/AttnLNP.html"><span class="doc">AttnLNP Notebook</span></a></p>
</div>
</div>
<div class="section" id="convolutional-latent-neural-process-convlnp">
<h2>Convolutional Latent Neural Process (ConvLNP)<a class="headerlink" href="#convolutional-latent-neural-process-convlnp" title="Permalink to this headline">¶</a></h2>
<div class="figure align-default" id="graph-model-convlnps-text">
<a class="reference internal image-reference" href="../_images/graph_model_ConvLNPs.svg"><img alt="graphical model ConvLNP" src="../_images/graph_model_ConvLNPs.svg" width="200em" /></a>
<p class="caption"><span class="caption-number">Fig. 29 </span><span class="caption-text">Graphical model for ConvLNPs.</span><a class="headerlink" href="#graph-model-convlnps-text" title="Permalink to this image">¶</a></p>
</div>
<div class="figure align-default" id="computational-graph-convlnps-text">
<a class="reference internal image-reference" href="../_images/computational_graph_ConvLNPs1.svg"><img alt="Computational graph ConvLNP" src="../_images/computational_graph_ConvLNPs1.svg" width="400em" /></a>
<p class="caption"><span class="caption-number">Fig. 30 </span><span class="caption-text">Computational graph for ConvLNPS. [simplify ? useful to mention or show induced points ? drop?]</span><a class="headerlink" href="#computational-graph-convlnps-text" title="Permalink to this image">¶</a></p>
</div>
<p>The Convolutional LNPs <a class="bibtex reference internal" href="../zbibliography.html#foong2020convnp" id="id4">[FBG+20]</a> is the latent counterpart of ConvCNPs. A major difference compared to AttnLNP is that the latent path <em>replaces</em> the deterministic, which is done by actually having a latent functional representation (a latent stochastic process) instead of a latent vector valued variable. [intuition …]</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>An other way of viewing the ConvLNP is that it consists of 2 stacked ConvCNP, the first one models the latent stochastic process. The second one takes as input a sample from the latent stochastic process and models the posterior predictive conditioned on that sample.</p>
</div>
<ul class="simple">
<li><p>Mention that better trained using MLE probably because of the functional KL [better explanation ?]</p></li>
<li><p>Link to theory</p></li>
<li><p>talk about global representation ?</p></li>
</ul>
<div class="figure align-default" id="convlnp-single-gp-extrap-text">
<a class="reference internal image-reference" href="../_images/ConvLNP_single_gp_extrap1.gif"><img alt="ConvLNP on GPs with RBF, periodic, Matern kernel" src="../_images/ConvLNP_single_gp_extrap1.gif" style="width: 35em;" /></a>
<p class="caption"><span class="caption-number">Fig. 31 </span><span class="caption-text">Samples Posterior predictive of AttnLNPs (Blue) and the oracle GP (Green) with RBF,periodic, and noisy Matern kernel.</span><a class="headerlink" href="#convlnp-single-gp-extrap-text" title="Permalink to this image">¶</a></p>
</div>
<p>From <a class="reference internal" href="#convlnp-single-gp-extrap-text"><span class="std std-numref">Fig. 31</span></a> we see that ConvLNP performs very well and the samples are reminiscent of those from a GP, i.e., with much richer variability compared to <a class="reference internal" href="#attnlnp-single-gp-text"><span class="std std-numref">Fig. 27</span></a>.</p>
<p>Let us now make the problem harder by having the ConvLNP model a stochastic process whose posterior predictive is non Gaussian. We will do so by having the following underlying generative process: sample one of the 3 previous kernels then sample funvtion. Note that the data generating process is not a GP (when marginalizing over kernel hyperparameters). Theoretically this could still be modeled by a LNPF as the latent variables could model the current kernel hyperparameter. This is where the use of a global representation makes sense.</p>
<div class="figure align-default" id="convlnp-kernel-gp-text">
<a class="reference internal image-reference" href="../_images/ConvLNP_kernel_gp1.gif"><img alt="ConvLNP trained on GPs with RBF,Matern,periodic kernel" src="../_images/ConvLNP_kernel_gp1.gif" style="width: 35em;" /></a>
<p class="caption"><span class="caption-number">Fig. 32 </span><span class="caption-text">Similar to <a class="reference internal" href="#convlnp-single-gp-extrap-text"><span class="std std-numref">Fig. 31</span></a> but the training was performed on all data simultaneously.</span><a class="headerlink" href="#convlnp-kernel-gp-text" title="Permalink to this image">¶</a></p>
</div>
<p>From <a class="reference internal" href="#convlnp-kernel-gp-text"><span class="std std-numref">Fig. 32</span></a> we see that ConvLNP performs quite well in this harder setting. Indeed, it seems to model process using the periodic kernel when the number of context points is small but quickly (around 10 context points) recovers the correct underlying kernel. Note that we plot the posterior predictive of the actual underlying GP but the generating process is highly non Gaussian.</p>
<p>[should we also add the results of <a class="reference internal" href="../reproducibility/ConvLNP.html#convlnp-vary-gp"><span class="std std-numref">Fig. 56</span></a> to show that not great when large/ uncountable number of kernels?]</p>
<div class="figure align-default" id="convlnp-marginal-text">
<a class="reference internal image-reference" href="../_images/ConvLNP_marginal.png"><img alt="Samples from ConvLNP on MNIST and posterior of different pixels" src="../_images/ConvLNP_marginal.png" style="width: 20em;" /></a>
<p class="caption"><span class="caption-number">Fig. 33 </span><span class="caption-text">Samples form the posterior predictive of ConvCNPs on MNIST (left) and posterior predictive of some pixels (right).</span><a class="headerlink" href="#convlnp-marginal-text" title="Permalink to this image">¶</a></p>
</div>
<p>As we discussed in  <a class="reference internal" href="CNPFs.html#issues-cnpfs"><span class="std std-ref">the “CNPG” issue section</span></a>, CNP not only could not be used to generate coherent samples but the posterior predictive is also Gaussian.
<a class="reference internal" href="#convlnp-marginal-text"><span class="std std-numref">Fig. 33</span></a> shows that both of these issues are somewhat alleviated (compare to <a class="reference internal" href="CNPFs.html#convcnp-marginal-text"><span class="std std-numref">Fig. 21</span></a>) […]</p>
<p>Here are more image samples.</p>
<div class="figure align-default" id="convlnp-img-text">
<a class="reference internal image-reference" href="../_images/ConvLNP_img1.gif"><img alt="ConvLNP on CelebA, MNIST, ZSMM" src="../_images/ConvLNP_img1.gif" style="width: 45em;" /></a>
<p class="caption"><span class="caption-number">Fig. 34 </span><span class="caption-text">Samples from posterior predictive of an ConvCNP for CelebA <span class="math notranslate nohighlight">\(32\times32\)</span>, MNIST, ZSMM.</span><a class="headerlink" href="#convlnp-img-text" title="Permalink to this image">¶</a></p>
</div>
<p>[REVERT PLOTS OF CONVLNP, I made a small modificiation which looks bad…]</p>
<div class="section" id="issues-of-lnpfs">
<h3>Issues of LNPFs<a class="headerlink" href="#issues-of-lnpfs" title="Permalink to this headline">¶</a></h3>
<ul class="simple">
<li><p>Do not think that LNPF are necessarily better than CNP</p></li>
<li><p>Cannot easily arg maximize posterior</p></li>
<li><p>More variance when training</p></li>
<li><p>More computationaly demanding to estimate (marginal) posterior predictive</p></li>
</ul>
<hr class="docutils" />
<dl class="footnote brackets">
<dt class="label" id="lnps"><span class="brackets"><a class="fn-backref" href="#id1">1</a></span></dt>
<dd><p>In the literature the latent neural processes are just called neural processes. I use “latent” to distinguish them with the neural process family as a whole.</p>
</dd>
</dl>
</div>
</div>
</div>


              </div>
              
        </div>
    </div>
    
    
    <div class='prev-next-bottom'>
        
    <a class='left-prev' id="prev-link" href="CNPFs.html" title="previous page">Conditional NPFs</a>
    <a class='right-next' id="next-link" href="Theory.html" title="next page">Theory</a>

    </div>
    <footer class="footer mt-5 mt-md-0">
    <div class="container">
      <p>
        
          By Yann Dubois and Jonathan Gordon<br/>
        
            &copy; Copyright 2020.<br/>
      </p>
    </div>
  </footer>
</main>


      </div>
    </div>

    <script src="../_static/js/index.js"></script>
    
  </body>
</html>