

<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta charset="utf-8" />
    <title>Overview &#8212; Neural Process Family</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.11.2/css/all.min.css" integrity="sha384-KA6wR/X5RY4zFAHpv/CnoG2UW1uogYfdnP67Uv7eULvTveboZJg0qUpmJZb5VqzN" crossorigin="anonymous">
    <link href="_static/css/index.css" rel="stylesheet">
    <link rel="stylesheet" href="_static/sphinx-book-theme.css" type="text/css" />
    <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="_static/jupyter-sphinx.css" />
    <script id="documentation_options" data-url_root="./" src="_static/documentation_options.js"></script>
    <script src="_static/sphinx-book-theme.js"></script>
    <script src="_static/jquery.js"></script>
    <script src="_static/underscore.js"></script>
    <script src="_static/doctools.js"></script>
    <script src="_static/language_data.js"></script>
    <script src="_static/togglebutton.js"></script>
    <script src="_static/clipboard.min.js"></script>
    <script src="_static/copybutton.js"></script>
    <script src="_static/mystnb.js"></script>
    <script src="_static/sphinx-book-theme.js"></script>
    <script >var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <script src="https://unpkg.com/@jupyter-widgets/html-manager@^0.18.0/dist/embed-amd.js"></script>
    <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Conditional NPFs" href="CNPFs.html" />

    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="docsearch:language" content="en">



  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
<a class="navbar-brand text-wrap" href="index.html">
  
  <img src="_static/logo.png" class="logo" alt="logo">
  
  
  <h1 class="site-logo" id="site-title">Neural Process Family</h1>
  
</a>
</div>

<form class="bd-search d-flex align-items-center" action="search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form>

<nav class="bd-links" id="bd-docs-nav" aria-label="Main navigation">
  
  <ul class="nav sidenav_l1">
  <li class="">
    <a href="CNPFs.html">Conditional NPFs</a>
  </li>
  <li class="">
    <a href="LNPFs.html">Latent NPFs</a>
  </li>
  <li class="">
    <a href="Additional.html">Additional</a>
  </li>
  <li class="">
    <a href="zbibliography.html">Bibliography</a>
  </li>
</ul>
</nav>

 <!-- To handle the deprecated key -->

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="row topbar fixed-top container-xl">
    <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show">
    </div>
    <div class="col pl-2 topbar-main">
        
        <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse" data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu" aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation" title="Toggle navigation" data-toggle="tooltip" data-placement="left">
            <i class="fas fa-bars"></i>
            <i class="fas fa-arrow-left"></i>
            <i class="fas fa-arrow-up"></i>
        </button>
        
        <div class="dropdown-buttons-trigger">
            <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i class="fas fa-download"></i></button>

            
            <div class="dropdown-buttons">
                <!-- ipynb file if we had a myst markdown file -->
                
                <!-- Download raw file -->
                <a class="dropdown-buttons" href="_sources/intro.md"><button type="button" class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip" data-placement="left">.md</button></a>
                <!-- Download PDF via print -->
                <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF" onClick="window.print()" data-toggle="tooltip" data-placement="left">.pdf</button>
            </div>
            
        </div>

        <!-- Source interaction buttons -->
        

        <!-- Full screen (wrap in <a> to have style consistency -->
        <a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip" data-placement="bottom" onclick="toggleFullScreen()" title="Fullscreen mode"><i class="fas fa-expand"></i></button></a>

        <!-- Launch buttons -->
        
    </div>
    <div class="d-none d-md-block col-md-2 bd-toc show">
<div class="tocsection onthispage pt-5 pb-3">
    <i class="fas fa-list"></i> On this page
</div>

<nav id="bd-toc-nav">
    <ul class="nav section-nav flex-column">
    
        <li class="nav-item toc-entry toc-h2">
            <a href="#modeling-stochastic-processes" class="nav-link">Modeling Stochastic Processes</a>
        </li>
    
        <li class="nav-item toc-entry toc-h2">
            <a href="#training" class="nav-link">Training</a>
        </li>
    
        <li class="nav-item toc-entry toc-h2">
            <a href="#properties" class="nav-link">Properties</a>
        </li>
    
        <li class="nav-item toc-entry toc-h2">
            <a href="#usecases" class="nav-link">Usecases</a>
        </li>
    
        <li class="nav-item toc-entry toc-h2">
            <a href="#members-of-npfs" class="nav-link">Members of NPFs</a>
        </li>
    
    </ul>
</nav>


    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
        
              <div>
                
  <div class="section" id="overview">
<h1>Overview<a class="headerlink" href="#overview" title="Permalink to this headline">¶</a></h1>
<p>The aim of this section is to :</p>
<ul class="simple">
<li><p>Overview of Neural Process Family</p></li>
<li><p>Overview of the framework I will use to try unifying different models from the neural process family</p></li>
</ul>
<div class="section" id="modeling-stochastic-processes">
<h2>Modeling Stochastic Processes<a class="headerlink" href="#modeling-stochastic-processes" title="Permalink to this headline">¶</a></h2>
<p><strong>Neural Processes Family (NPFs)</strong> are a family of models which aim to model stochastic processes (distributions over functions) using neural networks.
If you think of neural networks as a way of approximating a function <span class="math notranslate nohighlight">\(f : \mathcal{X} \to \mathcal{Y}\)</span>, then you can think of NPFs as a way of modeling a distribution over functions conditioned on a certain set of points.</p>
<p>Specifically, we want to model a distribution over <strong>target</strong> values <span class="math notranslate nohighlight">\(\mathbf{y}_{\mathcal{T}} := \{y^{(t)}\}_{t=1}^T\)</span> conditioned on a set of corresponding target features <span class="math notranslate nohighlight">\(\mathbf{x}_{\mathcal{T}} := \{x^{(t)}\}_{t=1}^T\)</span> and a <strong>context</strong> set of feature-value pairs <span class="math notranslate nohighlight">\(\mathcal{C} := \{(x^{(c)}, y^{(c)})\}_{c=1}^C\)</span>.
We call such distribution <span class="math notranslate nohighlight">\(p(\mathbf{y}_{\mathcal{T}}|\mathbf{x}_{\mathcal{T}}, \mathcal{C})\)</span> the <strong>posterior predictive</strong>.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>If you sampled <span class="math notranslate nohighlight">\(\mathbf{y}_{\mathcal{T}}\)</span> according to <span class="math notranslate nohighlight">\(p(\mathbf{y}_{\mathcal{T}}|\mathbf{x}_{\mathcal{T}}, \mathcal{C})\)</span>, for all possible features <span class="math notranslate nohighlight">\(\mathbf{x}_{\mathcal{T}}=\mathcal{X}\)</span> then you effectively sampled an entire function.</p>
</div>
<p>Prior to NPFs, one typically modelled stochastic processes by choosing the form of the distribution <span class="math notranslate nohighlight">\(p(\mathbf{y}|\mathbf{x})\)</span> and then using rules of probabilities to infer different terms such as the posterior predictive <span class="math notranslate nohighlight">\(p(\mathbf{y}_{\mathcal{T}}|\mathbf{x}_{\mathcal{T}}, \mathcal{C}) = \frac{p(\mathbf{y}_{\mathcal{T}},\mathbf{y}_{\mathcal{C}}|\mathbf{x}_{\mathcal{T}}, \mathbf{x}_\mathcal{C})}{p(\mathbf{y}_\mathcal{C}|\mathbf{x}_\mathcal{C})}\)</span>.
For example, this is the approach of <a class="reference external" href="https://distill.pub/2019/visual-exploration-gaussian-processes/">Gaussian Processes</a> (GPs), which uses a multivariate normal distribution for any <span class="math notranslate nohighlight">\(p(\mathbf{y}|\mathbf{x})\)</span>.
To define a proper stochastic process using <span class="math notranslate nohighlight">\(p(\mathbf{y}|\mathbf{x})\)</span>, it essentially needs to satisfy two consistency conditions (<a class="reference external" href="https://en.wikipedia.org/wiki/Kolmogorov_extension_theorem">Kolmogorov existence theorem</a>) that can informally be summarized as follows :</p>
<ul class="simple">
<li><p><strong>Permutation invariance</strong>
<span class="math notranslate nohighlight">\(p(\mathbf{y}|\mathbf{x})=p(\pi(\mathbf{y})|\pi(\mathbf{x}))\)</span> for all permutations <span class="math notranslate nohighlight">\(\pi\)</span> on <span class="math notranslate nohighlight">\(1, ..., |\mathbf{y}|\)</span>. <span class="math notranslate nohighlight">\(\mathbf{x}\)</span> and <span class="math notranslate nohighlight">\(\mathbf{y}\)</span> are sets and should thus be unordered.</p></li>
<li><p><strong>Consistent under marginalizaion</strong>
<span class="math notranslate nohighlight">\(p(\mathbf{y}|\mathbf{x})= \int p(\mathbf{y}'|\mathbf{x}') p(\mathbf{y}|\mathbf{x},\mathbf{y}',\mathbf{x}') d\mathbf{y}'\)</span>, for all <span class="math notranslate nohighlight">\(\mathbf{x}'\)</span>.
<span class="math notranslate nohighlight">\(\mathbf{x}\)</span> and <span class="math notranslate nohighlight">\(\mathbf{y}\)</span> are sets and should thus be unordered.</p></li>
</ul>
<p>By ensuring that these two condition hold, one can use standard probability rules and inherits nice mathematical properties.
Unfortunately stochastic processes are usually computationally inefficient.
For example predicting values of a target set using GPs takes time which is cubic in the context set <span class="math notranslate nohighlight">\(\mathcal{O}(|\mathcal{C}|^3)\)</span>.
In contrast, the core idea of NPFs is to model directly the posterior predictive using neural networks <span class="math notranslate nohighlight">\(p( \mathbf{y}_{\mathcal{T}} | \mathbf{x}_{\mathcal{T}}, \mathcal{C}) \approx q_{\boldsymbol \theta}(\mathbf{y}_{\mathcal{T}}  | \mathbf{x}_{\mathcal{T}}, \mathcal{C})\)</span> in the following way (sacrificing nice mathematical properties of stochastic processes for computational gains):</p>
<div class="math notranslate nohighlight" id="equation-formal">
<span class="eqno">(1)<a class="headerlink" href="#equation-formal" title="Permalink to this equation">¶</a></span>\[\begin{split}\begin{align}
p(\mathbf{y}_\mathcal{T} | \mathbf{x}_\mathcal{T}, \mathcal{c}) 
&amp;\approx q_{\boldsymbol\theta}(\mathbf{y}_\mathcal{T} | \mathbf{x}_\mathcal{T}, \mathcal{C}) &amp; \text{Parametrization}\\
&amp;= q_{\boldsymbol\theta}(\mathbf{y}_\mathcal{T} | \mathbf{x}_\mathcal{T}, R) &amp; \text{Sufficiency}  \\
&amp;= \prod_{t=1}^{T} q_{\boldsymbol\theta}(y^{(t)} |  x^{(t)}, R)  &amp; \text{Factorization}\\
&amp;= \prod_{t=1}^{T} \mathcal{N}\left( y^{(t)};  \mu^{(t)},
\sigma^{2(t)}
\right) &amp; \text{Gaussian} 
\end{align}\end{split}\]</div>
<p>Where:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align}
R_c 
&amp;:= h_{\boldsymbol\theta}(x^{(c)}, y^{(c)}) &amp; \text{Encoding} \\
R 
&amp;:= \mathrm{Agg}\left(\{R^{(c)}\}_{c=1}^{C} \right) &amp; \text{Aggregation} \\ 
(\mu^{(t)},\sigma^{2(t)}) 
&amp;:= g_{\boldsymbol\theta}(x^{(t)},R^{(t)}) &amp; \text{Decoding}  
\end{align}
\end{split}\]</div>
<p>The aggregator is the major difference across the NPFs. Intuitively it takes in the representation of each context points separately and it aggregates those in a representation for the entire context set. Given this representation, all the target values become independent (factorization assumption).
Importantly, the aggregator <span class="math notranslate nohighlight">\(\mathrm{Agg}\)</span> is invariant to all permutations <span class="math notranslate nohighlight">\(\pi\)</span> on <span class="math notranslate nohighlight">\(1, ..., C\)</span>:</p>
<div class="math notranslate nohighlight" id="equation-permut-inv">
<span class="eqno">(2)<a class="headerlink" href="#equation-permut-inv" title="Permalink to this equation">¶</a></span>\[\begin{align}
\mathrm{Agg}\left( \{R^{(c)}\}_{c=1}^{C} \right)=\mathrm{Agg}\left(\pi\left(\{R^{(c)}\}_{c=1}^{C} \right)\right) 
\end{align}\]</div>
</div>
<div class="section" id="training">
<h2>Training<a class="headerlink" href="#training" title="Permalink to this headline">¶</a></h2>
<p>The parameters are estimated by minimizing the expectation of the negative conditional conditional log likelihood (or approximation thereof):</p>
<div class="math notranslate nohighlight" id="equation-training">
<span class="eqno">(3)<a class="headerlink" href="#equation-training" title="Permalink to this equation">¶</a></span>\[\mathrm{NL}\mathcal{L}(\boldsymbol{\theta}) := -\mathbb{E}_{\mathrm{X}_\mathcal{T}} \left[ \mathbb{E}_{\mathrm{Y}_\mathcal{T}} \left[ \mathbb{E}_{\mathcal{C}} \left[ \log q_{\boldsymbol\theta} \left(\mathbf{y}_\mathcal{T} | \mathbf{x}_\mathcal{T}, \mathcal{C} \right)\right] \right]\right]\]</div>
<p>We optimize it using stochastic gradient descent:</p>
<ol class="simple">
<li><p>Sample size of context set <span class="math notranslate nohighlight">\(C \sim \mathrm{Unif}(0,|\mathcal{X}|)\)</span></p></li>
<li><p>Sample <span class="math notranslate nohighlight">\(C\)</span> context features <span class="math notranslate nohighlight">\(\mathbf{x}_{\mathcal{C}} \sim p(\mathbf{x}_{\mathcal{C}})\)</span>.</p></li>
<li><p>Sample associated values <span class="math notranslate nohighlight">\(\mathbf{y}_{\mathcal{C}} \sim p(\mathbf{y}_{\mathcal{C}} | \mathbf{x}_{\mathcal{C}})\)</span>.</p></li>
<li><p>Do the the same for the target set <span class="math notranslate nohighlight">\(\mathbf{x}_{\mathcal{T}},\mathbf{y}_{\mathcal{T}}\)</span> <a class="footnote-reference brackets" href="#sampletargets" id="id1">1</a>.</p></li>
<li><p>Compute the MC gradients of the negative log likelihood <span class="math notranslate nohighlight">\(- \nabla_{\pmb \theta} \sum_{t=1}^{T} \log p_{\boldsymbol \theta}(y^{(t)} | \mathbf{y}_\mathcal{C}; \mathbf{x}_\mathcal{C}, x^{(t)})\)</span></p></li>
<li><p>Backpropagate</p></li>
</ol>
<p>In practice, it means that training NPFs requires a dataset over sets of points.
This contrasts with usual neural network training which requires only a dataset of points.
In deep learning terms, we would say that it is trained through a <em>meta-learning</em> procedure.</p>
</div>
<div class="section" id="properties">
<h2>Properties<a class="headerlink" href="#properties" title="Permalink to this headline">¶</a></h2>
<p>NPFs generally have the following desirable properties :</p>
<ul class="simple">
<li><p>✓ <strong>Preserve permutation invariance</strong> as with stochastic processes. This comes from the permutation invariance of <span class="math notranslate nohighlight">\(\mathrm{Agg}\)</span> (Eq. <a class="reference internal" href="#equation-permut-inv">(2)</a>) and the factorization assumption (Eq. <a class="reference internal" href="#equation-formal">(1)</a>).
for all permutations <span class="math notranslate nohighlight">\(\pi_{\mathcal{T}}: |\mathbf{y}_{\mathcal{T}}| \to |\mathbf{y}_{\mathcal{T}}|\)</span> and <span class="math notranslate nohighlight">\(\pi_{\mathcal{C}}: |\mathbf{y}| \to |\mathbf{y}|\)</span>:</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
p_{\boldsymbol \theta}(\mathbf{y}_{\mathcal{T}}  | \mathbf{x}_{\mathcal{T}}, \mathcal{C}) = p_{\boldsymbol \theta}(\pi_{\mathcal{T}}(\mathbf{y})|\pi_{\mathcal{T}}(\mathbf{x}), \pi_{\mathcal{C}}(\mathcal{C}))
\]</div>
<ul class="simple">
<li><p>✓ <strong>Data Driven Expressivity</strong>. NPs require specification of prior knowledge through neural network architectures rather than a kernel function (like in GPs).
The former is usually less restrictive due to its large amount of parameters and removing the need of satisfying certain mathematical properties.
Intuitively, the NPs learn an “implicit kernel function” from the data.</p></li>
<li><p>✓ <strong>Test-Time Scalability</strong>. Although the computational complexity depends on the NPF they are usually more computationally efficient (at test time) than proper stochastic processes.
Typically they will be linear or quadratic in the context set instead of cubic as with GPs.</p></li>
</ul>
<p>These advantages come at the cost of the following disadvantages</p>
<ul class="simple">
<li><p>✗ <strong>Lack of consistency under marginalizaion</strong>. So NPFs are not proper stochastic processes.
This essentially means that even if you had infinite computational power (to be able to marginalize) and sampled points autoregressively, the order in which you do it would change the distribution over targets.
Formally, there exists <span class="math notranslate nohighlight">\(\mathbf{x}'\)</span> such that:</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
p(\mathbf{y}_{\mathcal{T}}|\mathbf{x}_{\mathcal{T}}, \mathcal{C}) \neq \int p(\mathbf{y}_{\mathcal{T}}|\mathbf{x}_{\mathcal{T}}, \mathcal{C},\mathbf{y}',\mathbf{x}')  
p(\mathbf{y}'|\mathbf{x}', \mathcal{C}) d\mathbf{y}'
\]</div>
<ul class="simple">
<li><p>✗ <strong>The need for large data</strong>. Learning requires collecting and training on a large dataset of target and context points sampled from different functions. I.e. a large dataset of datasets.</p></li>
<li><p>✗ <span class="math notranslate nohighlight">\(\sim\)</span> <strong>Lack of smoothness</strong>. Due to highly non linear behaviour of neural networks and the factorized form of the predictive distribution (Eq. <a class="reference internal" href="#equation-formal">(1)</a>), the output tends to be non-smooth compared to a GP.
This is less true in newer NPFs such as ConvCNP <a class="bibtex reference internal" href="zbibliography.html#gordon2019convolutional" id="id2">[GBF+19]</a>.</p></li>
</ul>
</div>
<div class="section" id="usecases">
<h2>Usecases<a class="headerlink" href="#usecases" title="Permalink to this headline">¶</a></h2>
<p>There are two major usecases that we will consider for the posterior predictive <span class="math notranslate nohighlight">\(p(\mathbf{y}_{\mathcal{T}}|\mathbf{x}_{\mathcal{T}}, \mathcal{C})\)</span> :</p>
<ul class="simple">
<li><p><strong>Sampling from the posterior predictive</strong> <span class="math notranslate nohighlight">\(\mathbf{y}_{\mathcal{T}} \sim p(\mathbf{y}_{\mathcal{T}}|\mathbf{x}_{\mathcal{T}}, \mathcal{C})\)</span>.
This can be used for conditional generation, for example in active learning or Bayesian optimization.</p></li>
<li><p><strong>Arg maximizing the posterior predictive</strong> <span class="math notranslate nohighlight">\(\mathbf{y}_\mathcal{T}^* = \arg \max_{\mathbf{y}_\mathcal{T}'} p( \mathbf{y}_\mathcal{T}' | \mathbf{y}_\mathcal{C}; \mathbf{x}_\mathcal{C}, \mathbf{x}_\mathcal{T})\)</span>.
This can be used for imputing missing values or meta-learning.</p></li>
</ul>
</div>
<div class="section" id="members-of-npfs">
<h2>Members of NPFs<a class="headerlink" href="#members-of-npfs" title="Permalink to this headline">¶</a></h2>
<p>NPFs can essentially be categorized into 2 sub-families depending on whether they aim at sampling (Conditional NPFs) or at arg maximizing (Latent NPFs) the posterior predictive.
LNPFs essentially have have a latent random variable, while CNPFs don’t.
Inside of each sub-family the main differences concerns the choice of aggregator <span class="math notranslate nohighlight">\(\mathrm{Agg}\)</span> and the form of <span class="math notranslate nohighlight">\(R\)</span>.</p>
<p>We will be talking about and experimenting with the following:</p>
<ul class="simple">
<li><p><a class="reference internal" href="CNPFs.html"><span class="doc">Conditional NPFs</span></a>(CNPFs)</p>
<ul>
<li><p><a class="reference internal" href="CNP.html"><span class="doc">Conditional Neural Process</span></a>(CNP) <a class="bibtex reference internal" href="zbibliography.html#garnelo2018conditional" id="id3">[GRM+18]</a>.</p></li>
<li><p><a class="reference internal" href="AttnCNP.html"><span class="doc">Attentive CNP</span></a> (AttnCNP)<a class="bibtex reference internal" href="zbibliography.html#kim2019attentive" id="id4">[KMS+19]</a> <a class="footnote-reference brackets" href="#attncnp" id="id5">2</a>.</p></li>
<li><p><span class="xref std std-doc">Convolutional CNP</span> (ConvCNP) <a class="bibtex reference internal" href="zbibliography.html#gordon2019convolutional" id="id6">[GBF+19]</a>.</p></li>
</ul>
</li>
<li><p><a class="reference internal" href="LNPFs.html"><span class="doc">Latent NPFs</span></a> (LNPFs) <a class="footnote-reference brackets" href="#lnps" id="id7">3</a></p>
<ul>
<li><p><a class="reference internal" href="LNP.html"><span class="doc">Latent Neural Process</span></a>(LNP) <a class="bibtex reference internal" href="zbibliography.html#garnelo2018neural" id="id8">[GSR+18]</a>.</p></li>
<li><p><a class="reference internal" href="AttnLNP.html"><span class="doc">Attentive LNP</span></a>(AttnLNP) <a class="bibtex reference internal" href="zbibliography.html#kim2019attentive" id="id9">[KMS+19]</a>.</p></li>
<li><p><a class="reference internal" href="ConvLNP.html"><span class="doc">Convolutional LNP</span></a>(ConvLNP) <a class="bibtex reference internal" href="zbibliography.html#foong2020convnp" id="id10">[FBG+20]</a>.</p></li>
</ul>
</li>
</ul>
<div class="toctree-wrapper compound">
</div>
<hr class="docutils" />
<dl class="footnote brackets">
<dt class="label" id="sampletargets"><span class="brackets"><a class="fn-backref" href="#id1">1</a></span></dt>
<dd><p>Instead of sampling targets, we will often use all the available targets <span class="math notranslate nohighlight">\(\mathbf{x}_\mathcal{T} = \mathcal{X}\)</span>. For example, in images the targets will usually be all pixels.</p>
</dd>
<dt class="label" id="attncnp"><span class="brackets"><a class="fn-backref" href="#id5">2</a></span></dt>
<dd><p><a class="bibtex reference internal" href="zbibliography.html#kim2019attentive" id="id11">[KMS+19]</a> only introduced the latent variable model, but one can easily drop the latent variable if not needed.</p>
</dd>
<dt class="label" id="lnps"><span class="brackets"><a class="fn-backref" href="#id7">3</a></span></dt>
<dd><p>In the literature the latent neural processes are just called neural processes. I use “latent” to distinguish them with the neural process family as a whole.</p>
</dd>
</dl>
</div>
</div>


              </div>
              
        </div>
    </div>
    
    
    <div class='prev-next-bottom'>
        
    <a class='right-next' id="next-link" href="CNPFs.html" title="next page">Conditional NPFs</a>

    </div>
    <footer class="footer mt-5 mt-md-0">
    <div class="container">
      <p>
        
          By Yann Dubois<br/>
        
            &copy; Copyright 2020.<br/>
      </p>
    </div>
  </footer>
</main>


      </div>
    </div>

    <script src="_static/js/index.js"></script>
    
  </body>
</html>