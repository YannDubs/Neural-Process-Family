{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"></ul></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from collections import OrderedDict\n",
    "import numpy as np\n",
    "\n",
    "def check_same(stride):\n",
    "    if isinstance(stride, (list, tuple)):\n",
    "        assert len(stride) == 2 and stride[0] == stride[1]\n",
    "        stride = stride[0]\n",
    "    return stride\n",
    "\n",
    "def receptive_field(model, input_size, batch_size=-1, device=\"cuda\"):\n",
    "    '''\n",
    "    :parameter\n",
    "    'input_size': tuple of (Channel, Height, Width)\n",
    "    :return  OrderedDict of `Layername`->OrderedDict of receptive field stats {'j':,'r':,'start':,'conv_stage':,'output_shape':,}\n",
    "    'j' for \"jump\" denotes how many pixels do the receptive fields of spatially neighboring units in the feature tensor\n",
    "        do not overlap in one direction.\n",
    "        i.e. shift one unit in this feature map == how many pixels shift in the input image in one direction.\n",
    "    'r' for \"receptive_field\" is the spatial range of the receptive field in one direction.\n",
    "    'start' denotes the center of the receptive field for the first unit (start) in on direction of the feature tensor.\n",
    "        Convention is to use half a pixel as the center for a range. center for `slice(0,5)` is 2.5.\n",
    "    '''\n",
    "    def register_hook(module):\n",
    "\n",
    "        def hook(module, input, output):\n",
    "            class_name = str(module.__class__).split(\".\")[-1].split(\"'\")[0]\n",
    "            module_idx = len(receptive_field)\n",
    "            m_key = \"%i\" % module_idx\n",
    "            p_key = \"%i\" % (module_idx - 1)\n",
    "            receptive_field[m_key] = OrderedDict()\n",
    "\n",
    "            if not receptive_field[\"0\"][\"conv_stage\"]:\n",
    "                print(\"Enter in deconv_stage\")\n",
    "                receptive_field[m_key][\"j\"] = 0\n",
    "                receptive_field[m_key][\"r\"] = 0\n",
    "                receptive_field[m_key][\"start\"] = 0\n",
    "            else:\n",
    "                p_j = receptive_field[p_key][\"j\"]\n",
    "                p_r = receptive_field[p_key][\"r\"]\n",
    "                p_start = receptive_field[p_key][\"start\"]\n",
    "                \n",
    "                if class_name == \"Conv2d\" or class_name == \"MaxPool2d\":\n",
    "                    kernel_size = module.kernel_size\n",
    "                    stride = module.stride\n",
    "                    padding = module.padding\n",
    "                    kernel_size, stride, padding = map(check_same, [kernel_size, stride, padding])\n",
    "                    receptive_field[m_key][\"j\"] = p_j * stride\n",
    "                    receptive_field[m_key][\"r\"] = p_r + (kernel_size - 1) * p_j\n",
    "                    receptive_field[m_key][\"start\"] = p_start + ((kernel_size - 1) / 2 - padding) * p_j\n",
    "                elif class_name == \"BatchNorm2d\" or class_name == \"ReLU\" or class_name == \"Bottleneck\":\n",
    "                    receptive_field[m_key][\"j\"] = p_j\n",
    "                    receptive_field[m_key][\"r\"] = p_r\n",
    "                    receptive_field[m_key][\"start\"] = p_start\n",
    "                elif class_name == \"ConvTranspose2d\":\n",
    "                    receptive_field[\"0\"][\"conv_stage\"] = False\n",
    "                    receptive_field[m_key][\"j\"] = 0\n",
    "                    receptive_field[m_key][\"r\"] = 0\n",
    "                    receptive_field[m_key][\"start\"] = 0\n",
    "                else:\n",
    "                    raise ValueError(\"module not ok\")\n",
    "                    pass\n",
    "            receptive_field[m_key][\"input_shape\"] = list(input[0].size()) # only one\n",
    "            receptive_field[m_key][\"input_shape\"][0] = batch_size\n",
    "            if isinstance(output, (list, tuple)):\n",
    "                # list/tuple\n",
    "                receptive_field[m_key][\"output_shape\"] = [\n",
    "                    [-1] + list(o.size())[1:] for o in output\n",
    "                ]\n",
    "            else:\n",
    "                # tensor\n",
    "                receptive_field[m_key][\"output_shape\"] = list(output.size())\n",
    "                receptive_field[m_key][\"output_shape\"][0] = batch_size\n",
    "\n",
    "        if (\n",
    "            not isinstance(module, nn.Sequential)\n",
    "            and not isinstance(module, nn.ModuleList)\n",
    "            and not (module == model)\n",
    "        ):\n",
    "            hooks.append(module.register_forward_hook(hook))\n",
    "\n",
    "    device = device.lower()\n",
    "    assert device in [\n",
    "        \"cuda\",\n",
    "        \"cpu\",\n",
    "    ], \"Input device is not valid, please specify 'cuda' or 'cpu'\"\n",
    "\n",
    "    if device == \"cuda\" and torch.cuda.is_available():\n",
    "        dtype = torch.cuda.FloatTensor\n",
    "    else:\n",
    "        dtype = torch.FloatTensor\n",
    "\n",
    "    # check if there are multiple inputs to the network\n",
    "    if isinstance(input_size[0], (list, tuple)):\n",
    "        x = [Variable(torch.rand(2, *in_size)).type(dtype) for in_size in input_size]\n",
    "    else:\n",
    "        x = Variable(torch.rand(2, *input_size)).type(dtype)\n",
    "\n",
    "    # create properties\n",
    "    receptive_field = OrderedDict()\n",
    "    receptive_field[\"0\"] = OrderedDict()\n",
    "    receptive_field[\"0\"][\"j\"] = 1.0\n",
    "    receptive_field[\"0\"][\"r\"] = 1.0\n",
    "    receptive_field[\"0\"][\"start\"] = 0.5\n",
    "    receptive_field[\"0\"][\"conv_stage\"] = True\n",
    "    receptive_field[\"0\"][\"output_shape\"] = list(x.size())\n",
    "    receptive_field[\"0\"][\"output_shape\"][0] = batch_size\n",
    "    hooks = []\n",
    "\n",
    "    # register hook\n",
    "    model.apply(register_hook)\n",
    "\n",
    "    # make a forward pass\n",
    "    model(x)\n",
    "\n",
    "    # remove these hooks\n",
    "    for h in hooks:\n",
    "        h.remove()\n",
    "\n",
    "    print(\"------------------------------------------------------------------------------\")\n",
    "    line_new = \"{:>20}  {:>10} {:>10} {:>10} {:>15} \".format(\"Layer (type)\", \"map size\", \"start\", \"jump\", \"receptive_field\")\n",
    "    print(line_new)\n",
    "    print(\"==============================================================================\")\n",
    "    total_params = 0\n",
    "    total_output = 0\n",
    "    trainable_params = 0\n",
    "    for layer in receptive_field:\n",
    "        # input_shape, output_shape, trainable, nb_params\n",
    "        assert \"start\" in receptive_field[layer], layer\n",
    "        assert len(receptive_field[layer][\"output_shape\"]) == 4\n",
    "        line_new = \"{:7} {:12}  {:>10} {:>10} {:>10} {:>15} \".format(\n",
    "            \"\",\n",
    "            layer,\n",
    "            str(receptive_field[layer][\"output_shape\"][2:]),\n",
    "            str(receptive_field[layer][\"start\"]),\n",
    "            str(receptive_field[layer][\"j\"]),\n",
    "            format(str(receptive_field[layer][\"r\"]))\n",
    "        )\n",
    "        print(line_new)\n",
    "\n",
    "    print(\"==============================================================================\")\n",
    "    # add input_shape\n",
    "    receptive_field[\"input_size\"] = input_size\n",
    "    return receptive_field\n",
    "\n",
    "\n",
    "def receptive_field_for_unit(receptive_field_dict, layer, unit_position):\n",
    "    \"\"\"Utility function to calculate the receptive field for a specific unit in a layer\n",
    "        using the dictionary calculated above\n",
    "    :parameter\n",
    "        'layer': layer name, should be a key in the result dictionary\n",
    "        'unit_position': spatial coordinate of the unit (H, W)\n",
    "    ```\n",
    "    alexnet = models.alexnet()\n",
    "    model = alexnet.features.to('cuda')\n",
    "    receptive_field_dict = receptive_field(model, (3, 224, 224))\n",
    "    receptive_field_for_unit(receptive_field_dict, \"8\", (6,6))\n",
    "    ```\n",
    "    Out: [(62.0, 161.0), (62.0, 161.0)]\n",
    "    \"\"\"\n",
    "    input_shape = receptive_field_dict[\"input_size\"]\n",
    "    if layer in receptive_field_dict:\n",
    "        rf_stats = receptive_field_dict[layer]\n",
    "        assert len(unit_position) == 2\n",
    "        feat_map_lim = rf_stats['output_shape'][2:]\n",
    "        if np.any([unit_position[idx] < 0 or\n",
    "                   unit_position[idx] >= feat_map_lim[idx]\n",
    "                   for idx in range(2)]):\n",
    "            raise Exception(\"Unit position outside spatial extent of the feature tensor ((H, W) = (%d, %d)) \" % tuple(feat_map_lim))\n",
    "        # X, Y = tuple(unit_position)\n",
    "        rf_range = [(rf_stats['start'] + idx * rf_stats['j'] - rf_stats['r'] / 2,\n",
    "            rf_stats['start'] + idx * rf_stats['j'] + rf_stats['r'] / 2) for idx in unit_position]\n",
    "        if len(input_shape) == 2:\n",
    "            limit = input_shape\n",
    "        else:  # input shape is (channel, H, W)\n",
    "            limit = input_shape[1:3]\n",
    "        rf_range = [(max(0, rf_range[axis][0]), min(limit[axis], rf_range[axis][1])) for axis in range(2)]\n",
    "        print(\"Receptive field size for layer %s, unit_position %s,  is \\n %s\" % (layer, unit_position, rf_range))\n",
    "        return rf_range\n",
    "    else:\n",
    "        raise KeyError(\"Layer name incorrect, or not included in the model.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter in deconv_stage\n",
      "Enter in deconv_stage\n",
      "Enter in deconv_stage\n",
      "Enter in deconv_stage\n",
      "Enter in deconv_stage\n",
      "------------------------------------------------------------------------------\n",
      "        Layer (type)    map size      start       jump receptive_field \n",
      "==============================================================================\n",
      "        0             [256, 256]        0.5        1.0             1.0 \n",
      "        1             [248, 248]        4.5        1.0             9.0 \n",
      "        2             [244, 244]        6.5        1.0            13.0 \n",
      "        3             [240, 240]        8.5        1.0            17.0 \n",
      "        4             [120, 120]        9.0        2.0            18.0 \n",
      "        5             [116, 116]       13.0        2.0            26.0 \n",
      "        6             [112, 112]       17.0        2.0            34.0 \n",
      "        7               [56, 56]       18.0        4.0            36.0 \n",
      "        8               [52, 52]       26.0        4.0            52.0 \n",
      "        9               [48, 48]       34.0        4.0            68.0 \n",
      "        10              [99, 99]          0          0               0 \n",
      "        11              [95, 95]          0          0               0 \n",
      "        12              [91, 91]          0          0               0 \n",
      "        13            [185, 185]          0          0               0 \n",
      "        14            [181, 181]          0          0               0 \n",
      "        15            [177, 177]          0          0               0 \n",
      "==============================================================================\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "k = 5\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.ms = nn.ModuleList([nn.Conv2d(3, 3, kernel_size=k+4), \n",
    "                                 \n",
    "                                nn.Conv2d(3, 3, kernel_size=k), \n",
    "                                nn.Conv2d(3, 3, kernel_size=k),\n",
    "                                 nn.MaxPool2d(2, stride=2),\n",
    "                                 \n",
    "                                nn.Conv2d(3, 3, kernel_size=k), \n",
    "                                nn.Conv2d(3, 3, kernel_size=k),\n",
    "                                 nn.MaxPool2d(2, stride=2),\n",
    "                                 \n",
    "                                 nn.Conv2d(3, 3, kernel_size=k), \n",
    "                                nn.Conv2d(3, 3, kernel_size=k),\n",
    "                                 \n",
    "                                 nn.ConvTranspose2d(3, 3, kernel_size=k, stride=2),\n",
    "                                 nn.Conv2d(3, 3, kernel_size=k), \n",
    "                                nn.Conv2d(3, 3, kernel_size=k),\n",
    "                                 \n",
    "                                 nn.ConvTranspose2d(3, 3, kernel_size=k, stride=2),\n",
    "                                 nn.Conv2d(3, 3, kernel_size=k), \n",
    "                                nn.Conv2d(3, 3, kernel_size=k),\n",
    "                                 \n",
    "                                \n",
    "                                ])\n",
    "\n",
    "    def forward(self, x):\n",
    "        for m in self.ms:\n",
    "            x = m(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") # PyTorch v0.4.0\n",
    "model = Net().to(device)\n",
    "\n",
    "receptive_field_dict = receptive_field(model, (3, 256, 256))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "32 // 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
