{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 2 : Neural Processes - 1D GP Data\n",
    "\n",
    "Last Update : 27 August 2019\n",
    "\n",
    "**Aim**: \n",
    "- Training a simple [Neural Process]\n",
    "- See how well a NP \"learns\" a GP kernel\n",
    "\n",
    "**Nota Bene:**\n",
    "- Much more details about the framework and dataset can be found in [Tutorial 1 - Conditional Neural Process].\n",
    "- Not much hyperparameter optimization done\n",
    "- Architecture differs slightly from the paper to be more general and modularizable for future work.\n",
    "\n",
    "\n",
    "[Neural Process]: https://arxiv.org/abs/1807.01622\n",
    "[Conditional Neural Process]: https://arxiv.org/pdf/1807.01613.pdf\n",
    "[Attentive Neural Process]: https://arxiv.org/abs/1901.05761\n",
    "[Image Transformer]: https://arxiv.org/abs/1802.05751\n",
    "[Tutorial 1 - Conditional Neural Process]: Tutorial%201%20-%20Conditional%20Neural%20Process.ipynb\n",
    "[Tutorial 2 - Neural Process]: Tutorial%202%20-%20Neural%20Process.ipynb\n",
    "[Tutorial 3 - Attentive Neural Process]: Tutorial%203%20-%20Attentive%20Neural%20Process.ipynb\n",
    "\n",
    "**Environment Hypermarameters:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_THREADS = 8\n",
    "# Nota Bene : notebooks don't deallocate GPU memory\n",
    "IS_FORCE_CPU = False # can also be set in the trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/private/home/yannd/projects/NPF\n"
     ]
    }
   ],
   "source": [
    "cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "IPython.notebook.set_autosave_interval(600000)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autosaving every 600 seconds\n"
     ]
    },
    {
     "data": {
      "text/html": [
       " <style> .output_png {display: table-cell; text-align: center; margin:auto; }\n",
       ".prompt display:none;}  </style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%autosave 600\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "# CENTER PLOTS\n",
    "from IPython.core.display import HTML\n",
    "display(HTML(\"\"\" <style> .output_png {display: table-cell; text-align: center; margin:auto; }\n",
    ".prompt display:none;}  </style>\"\"\"))\n",
    "\n",
    "import os\n",
    "if IS_FORCE_CPU:\n",
    "    os.environ['CUDA_VISIBLE_DEVICES'] = \"\"\n",
    "    \n",
    "import sys\n",
    "sys.path.append(\"notebooks\")\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "torch.set_num_threads(N_THREADS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "The dataset we will be using are simple functions sampled from different Gaussian kernel. See [Tutorial 1 - Conditional Neural Process] for more details.\n",
    "\n",
    "[Tutorial 1 - Conditional Neural Process]: Tutorial%201%20-%20Conditional%20Neural%20Process.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.visualize import plot_dataset_samples_1d, plot_prior_samples_1d, plot_posterior_samples_1d, plot_losses\n",
    "from ntbks_helpers import get_gp_datasets_varying # defined in first tutorial (CNP)\n",
    "\n",
    "X_DIM = 1  # 1D spatial input\n",
    "Y_DIM = 1  # 1D regression\n",
    "N_POINTS = 128\n",
    "N_SAMPLES = 50000 \n",
    "\n",
    "datasets, test_datasets = get_gp_datasets_varying(n_samples=N_SAMPLES, n_points=N_POINTS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The general model architecture is slightly different from the paper to make it modular and easy to extend. Refer to [Tutorial 1 - Conditional Neural Process] for an overview main parameters or the docstrings of `NeuralProcess` for important parameters. \n",
    "\n",
    "Main difference with [Tutorial 1 - Conditional Neural Process]:\n",
    "- `encoded_path=\"latent\"`\n",
    "\n",
    "[Tutorial 1 - Conditional Neural Process]: Tutorial%201%20-%20Conditional%20Neural%20Process.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "from neuralproc import LNP\n",
    "from ntbks_helpers import CNP_KWARGS # defined in first tutorial (CNP)\n",
    "from neuralproc.utils.helpers import LightTailPareto\n",
    "\n",
    "np_kwargs = CNP_KWARGS.copy()\n",
    "np_kwargs[\"encoded_path\"] = \"latent\" # use NP\n",
    "np_kwargs[\"is_q_zCct\"] = True\n",
    "\n",
    "model = partial(LNP, X_DIM, Y_DIM, **np_kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### N Param\n",
    "\n",
    "Number of parameters (note that I did not play around with this much, this depends a lot on the representation size):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N Param: 186434\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/private/home/yannd/projects/NPF/neuralproc/architectures/mlp.py:67: UserWarning: hidden_size=32 smaller than output=128 and input=128. Setting it to 128.\n",
      "  warnings.warn(txt.format(hidden_size, output_size, input_size, self.hidden_size))\n"
     ]
    }
   ],
   "source": [
    "from utils.helpers import count_parameters\n",
    "print(\"N Param:\", count_parameters(model()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More parameters than CNP because need a MLP that maps $R \\to Z$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training procedure is the same as in [Tutorial 1 - Conditional Neural Process], refer to it for more details.\n",
    "\n",
    "[Tutorial 1 - Conditional Neural Process]: Tutorial%201%20-%20Conditional%20Neural%20Process.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Loading RBF_Kernel/ISLNP/run_0 ---\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92496ec45cc943eb9fd2e3ffa4fc08ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1563), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RBF_Kernel/ISLNP/run_0 | best epoch: 20 | train loss: 82.3374 | valid loss: 104.723 | test log likelihood: -117.4098\n",
      "\n",
      "--- Loading Periodic_Kernel/ISLNP/run_0 ---\n",
      "\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'results/notebooks/neural_process/Periodic_Kernel/ISLNP/run_0/history.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-73d5ef27071e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m      \u001b[0mpatience\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mmax_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m      seed=123) \n\u001b[0m",
      "\u001b[0;32m~/projects/NPF/utils/train.py\u001b[0m in \u001b[0;36mtrain_models\u001b[0;34m(datasets, models, criterion, test_datasets, chckpnt_dirname, is_continue_train, is_retrain, runs, starting_run, train_split, device, max_epochs, batch_size, lr, optimizer, callbacks, patience, seed, datasets_kwargs, models_kwargs, **kwargs)\u001b[0m\n\u001b[1;32m    226\u001b[0m                 \u001b[0;31m# load in all case => even when training loads the best checkpoint\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    227\u001b[0m                 \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minitialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 228\u001b[0;31m                 \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheckpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mchckpt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    229\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    230\u001b[0m                 \u001b[0;31m# return the training rather than testing history as dflt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/neuralproc/lib/python3.7/site-packages/skorch/net.py\u001b[0m in \u001b[0;36mload_params\u001b[0;34m(self, f_params, f_optimizer, f_history, checkpoint)\u001b[0m\n\u001b[1;32m   1642\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minitialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1643\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mf_history\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mcheckpoint\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf_history\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1644\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mHistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheckpoint\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf_history_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1645\u001b[0m             \u001b[0mformatted_files\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheckpoint\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_formatted_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1646\u001b[0m             \u001b[0mf_params\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf_params\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mformatted_files\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'f_params'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/neuralproc/lib/python3.7/site-packages/skorch/history.py\u001b[0m in \u001b[0;36mfrom_file\u001b[0;34m(cls, f)\u001b[0m\n\u001b[1;32m    159\u001b[0m         \"\"\"\n\u001b[1;32m    160\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 161\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0mopen_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    162\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/neuralproc/lib/python3.7/contextlib.py\u001b[0m in \u001b[0;36m__enter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0;32mdel\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 112\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    113\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"generator didn't yield\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/neuralproc/lib/python3.7/site-packages/skorch/utils.py\u001b[0m in \u001b[0;36mopen_file_like\u001b[0;34m(f, mode)\u001b[0m\n\u001b[1;32m    431\u001b[0m     \u001b[0mnew_fd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpathlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    432\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mnew_fd\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 433\u001b[0;31m         \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    434\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    435\u001b[0m         \u001b[0;32myield\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'results/notebooks/neural_process/Periodic_Kernel/ISLNP/run_0/history.json'"
     ]
    }
   ],
   "source": [
    "from neuralproc import NLLLossLNPF\n",
    "from utils.train import train_models\n",
    "from utils.data.dataloader import cntxt_trgt_collate\n",
    "from ntbks_helpers import get_cntxt_trgt\n",
    "\n",
    "trainers = train_models(\n",
    "     datasets, \n",
    "     {\"ISLNP\":model},\n",
    "     partial(NLLLossLNPF, is_importance_sampling=True),\n",
    "     test_datasets=test_datasets,\n",
    "    device=\"cpu\",\n",
    "     chckpnt_dirname=\"results/notebooks/neural_process/\", \n",
    "     is_retrain=False, # load pretrained\n",
    "     iterator_train__collate_fn=cntxt_trgt_collate(get_cntxt_trgt), \n",
    "     iterator_valid__collate_fn=cntxt_trgt_collate(get_cntxt_trgt),\n",
    "     patience=10,\n",
    "    max_epochs=20,\n",
    "     seed=123) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_trainers = len(trainers)\n",
    "fig, axes = plt.subplots(n_trainers, 1, figsize=(11, 5*n_trainers), sharex=True, sharey=True)\n",
    "if n_trainers==1: axes = [axes]\n",
    "for i, (k, trainer) in enumerate(trainers.items()):\n",
    "    plot_losses(trainer.history, \n",
    "                title=\"Losses for {}\".format(\" \".join(k.split(\"/\")).replace(\"_\", \" \")), \n",
    "                ax=axes[i])\n",
    "    axes[i].set_ylim([-1, 500])\n",
    "    axes[i].set_xlim([0, 100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: the losses are not directly comparable to the first notebook as there also is the ELBO loss. Should run test only with NLL."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INTERPOLATION_RANGE = list(datasets.values())[0].min_max"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Posterior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from neuralproc.utils.datasplit import CntxtTrgtGetter, GetRandomIndcs, get_all_indcs\n",
    "\n",
    "def get_n_cntxt(n_cntxt):\n",
    "    return CntxtTrgtGetter(contexts_getter=GetRandomIndcs(min_n_indcs=n_cntxt, max_n_indcs=n_cntxt),\n",
    "                         targets_getter=get_all_indcs,\n",
    "                         is_add_cntxts_to_trgts=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_CNTXT = 10\n",
    "fig, axes = plt.subplots(n_trainers, 1, figsize=(11, 5*n_trainers), sharex=True)\n",
    "if n_trainers==1: axes = [axes]\n",
    "    \n",
    "for i, (k, trainer) in enumerate(trainers.items()):\n",
    "    data_name = k.split(\"/\")[0]\n",
    "    dataset = datasets[data_name]\n",
    "    \n",
    "    X, Y = dataset.get_samples(n_samples=1, \n",
    "                               n_points=3*N_POINTS) # use higher density for plotting \n",
    "        \n",
    "    plot_posterior_samples_1d(X, Y, get_n_cntxt(N_CNTXT), trainer.module_, \n",
    "                             generator=dataset.generator,\n",
    "                             n_samples=50,\n",
    "                             train_min_max=dataset.min_max,\n",
    "                             is_plot_std=True,\n",
    "                             title=\"NP Posterior Conditioned on {} Context Points : {}\".format(N_CNTXT, data_name.replace(\"_\", \" \")),\n",
    "                             ax=axes[i])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_CNTXT = 30\n",
    "fig, axes = plt.subplots(n_trainers, 1, figsize=(11, 5*n_trainers), sharex=True)\n",
    "if n_trainers==1: axes = [axes]\n",
    "    \n",
    "for i, (k, trainer) in enumerate(trainers.items()):\n",
    "    data_name = k.split(\"/\")[0]\n",
    "    dataset = datasets[data_name]\n",
    "    \n",
    "    X, Y = dataset.get_samples(n_samples=1, \n",
    "                               n_points=3*N_POINTS) # use higher density for plotting \n",
    "        \n",
    "    plot_posterior_samples_1d(X, Y, get_n_cntxt(N_CNTXT), trainer.module_, \n",
    "                             generator=dataset.generator,\n",
    "                             n_samples=50,\n",
    "                             train_min_max=dataset.min_max,\n",
    "                             is_plot_std=True,\n",
    "                             title=\"NP Posterior Conditioned on {} Context Points : {}\".format(N_CNTXT, data_name.replace(\"_\", \" \")),\n",
    "                             ax=axes[i])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the predictions are not great.\n",
    "\n",
    "\n",
    "**Bad**:\n",
    "- samples seem to mostly be shifted versions of the mean sample\n",
    "- the predictions don't seem better with more context points\n",
    "- badly underfits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
