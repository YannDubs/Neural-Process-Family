{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SUMO Convolutional Latent Neural Process\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_THREADS = 8\n",
    "# Nota Bene : notebooks don't deallocate GPU memory\n",
    "IS_FORCE_CPU = True # can also be set in the trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "IPython.notebook.set_autosave_interval(600000)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autosaving every 600 seconds\n"
     ]
    },
    {
     "data": {
      "text/html": [
       " <style> .output_png {display: table-cell; text-align: center; margin:auto; }\n",
       ".prompt display:none;}  </style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%autosave 600\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "# CENTER PLOTS\n",
    "from IPython.core.display import HTML\n",
    "display(HTML(\"\"\" <style> .output_png {display: table-cell; text-align: center; margin:auto; }\n",
    ".prompt display:none;}  </style>\"\"\"))\n",
    "\n",
    "import os\n",
    "if IS_FORCE_CPU:\n",
    "    os.environ['CUDA_VISIBLE_DEVICES'] = \"\"\n",
    "    \n",
    "import sys\n",
    "sys.path.append(\"notebooks\")\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "torch.set_num_threads(N_THREADS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "The dataset we will be using are simple functions sampled from different Gaussian kernel. See [Tutorial 1 - Conditional Neural Process] for more details.\n",
    "\n",
    "[Tutorial 1 - Conditional Neural Process]: Tutorial%201%20-%20Conditional%20Neural%20Process.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.visualize import plot_dataset_samples_1d, plot_prior_samples_1d, plot_posterior_samples_1d, plot_losses\n",
    "from ntbks_helpers import get_gp_datasets_varying # defined in first tutorial (CNP)\n",
    "\n",
    "X_DIM = 1  # 1D spatial input\n",
    "Y_DIM = 1  # 1D regression\n",
    "N_POINTS = 128\n",
    "N_SAMPLES = 50000\n",
    "\n",
    "datasets,test_datasets = get_gp_datasets_varying(n_samples=N_SAMPLES, n_points=N_POINTS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "from neuralproc import ConvLNP\n",
    "from neuralproc.utils.helpers import LightTailPareto\n",
    "\n",
    "p_n_z_samples = LightTailPareto(a=25).freeze(512)\n",
    "\n",
    "model = partial(ConvLNP, X_DIM, Y_DIM, is_q_zCct=False, encoded_path=\"latent\", \n",
    "                n_z_samples_train=p_n_z_samples, \n",
    "                n_z_samples_test=p_n_z_samples)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N Param: 175108\n"
     ]
    }
   ],
   "source": [
    "from utils.helpers import count_parameters\n",
    "print(\"N Param:\", count_parameters(model()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "The training procedure is the same as in [Tutorial 1 - Conditional Neural Process], refer to it for more details.\n",
    "\n",
    "[Tutorial 1 - Conditional Neural Process]: Tutorial%201%20-%20Conditional%20Neural%20Process.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Training RBF_Kernel/SUMOConvLNP/run_0 ---\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e619db96a914fbd8618c552aff8553f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=2970), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Re-initializing module.\n",
      "Re-initializing optimizer.\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for ConvLNP:\n\tMissing key(s) in state_dict: \"decoder.destination.weight\", \"decoder.destination.bias\". \n\tUnexpected key(s) in state_dict: \"p_y_scale\". ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-93235120012a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m      \u001b[0mpatience\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m      \u001b[0mmax_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m      seed=123) \n\u001b[0m",
      "\u001b[0;32m~/projects/NPF/utils/train.py\u001b[0m in \u001b[0;36mtrain_models\u001b[0;34m(datasets, models, criterion, test_datasets, chckpnt_dirname, is_continue_train, is_retrain, runs, starting_run, train_split, device, max_epochs, batch_size, lr, optimizer, callbacks, patience, seed, datasets_kwargs, models_kwargs, **kwargs)\u001b[0m\n\u001b[1;32m    226\u001b[0m                 \u001b[0;31m# load in all case => even when training loads the best checkpoint\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    227\u001b[0m                 \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minitialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 228\u001b[0;31m                 \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheckpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mchckpt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    229\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    230\u001b[0m                 \u001b[0;31m# return the training rather than testing history as dflt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/neuralproc/lib/python3.7/site-packages/skorch/net.py\u001b[0m in \u001b[0;36mload_params\u001b[0;34m(self, f_params, f_optimizer, f_history, checkpoint)\u001b[0m\n\u001b[1;32m   1654\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_is_fitted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1655\u001b[0m             \u001b[0mstate_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1656\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodule_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1657\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1658\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mf_optimizer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/neuralproc/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mload_state_dict\u001b[0;34m(self, state_dict, strict)\u001b[0m\n\u001b[1;32m    828\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_msgs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    829\u001b[0m             raise RuntimeError('Error(s) in loading state_dict for {}:\\n\\t{}'.format(\n\u001b[0;32m--> 830\u001b[0;31m                                self.__class__.__name__, \"\\n\\t\".join(error_msgs)))\n\u001b[0m\u001b[1;32m    831\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_IncompatibleKeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmissing_keys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munexpected_keys\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    832\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for ConvLNP:\n\tMissing key(s) in state_dict: \"decoder.destination.weight\", \"decoder.destination.bias\". \n\tUnexpected key(s) in state_dict: \"p_y_scale\". "
     ]
    }
   ],
   "source": [
    "from neuralproc import ELBOLossLNPF, NLLLossLNPF, SUMOLossLNPF\n",
    "from utils.train import train_models\n",
    "from utils.data.dataloader import cntxt_trgt_collate\n",
    "from ntbks_helpers import get_cntxt_trgt\n",
    "\n",
    "trainers = train_models(\n",
    "     datasets, \n",
    "     {\"SUMOConvLNP\":model},\n",
    "     partial(SUMOLossLNPF, is_importance_sampling=False, p_n_z_samples=p_n_z_samples),\n",
    "     chckpnt_dirname=\"results/notebooks/neural_process/\", \n",
    "     is_retrain=True, \n",
    "     iterator_train__collate_fn=cntxt_trgt_collate(get_cntxt_trgt), \n",
    "     iterator_valid__collate_fn=cntxt_trgt_collate(get_cntxt_trgt),\n",
    "     patience=10,\n",
    "     max_epochs=20,\n",
    "     seed=123) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_datasets = len(datasets)\n",
    "fig, axes = plt.subplots(n_datasets, 1, figsize=(11, 5*n_datasets), sharex=True, sharey=True)\n",
    "if len(trainers) == 1: axes = [axes]\n",
    "    \n",
    "i = 0\n",
    "for k, trainer in trainers.items():\n",
    "    plot_losses(trainer.history, \n",
    "                title=\"Losses for {}\".format(\" \".join(k.split(\"/\")).replace(\"_\", \" \")), \n",
    "                ax=axes[i])\n",
    "    axes[i].set_ylim([-1, 1.5])\n",
    "    axes[i].set_xlim([0, 100])\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXTRAP_DISTANCE = 4  \n",
    "dataset = list(datasets.values())[0]\n",
    "INTERPOLATION_RANGE = dataset.min_max\n",
    "EXTRAPOLATION_RANGE = (dataset.min_max[0], dataset.min_max[1]+EXTRAP_DISTANCE )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Posterior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from neuralproc.utils.datasplit import CntxtTrgtGetter, GetRandomIndcs, get_all_indcs\n",
    "\n",
    "def get_n_cntxt(n_cntxt):\n",
    "    return CntxtTrgtGetter(contexts_getter=GetRandomIndcs(min_n_indcs=n_cntxt, max_n_indcs=n_cntxt),\n",
    "                         targets_getter=get_all_indcs,\n",
    "                         is_add_cntxts_to_trgts=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_CNTXT = 10\n",
    "\n",
    "n_datasets = len(datasets)\n",
    "fig, axes = plt.subplots(n_datasets, 1, figsize=(11, 5*n_datasets), sharex=True)\n",
    "if len(trainers) == 1: axes = [axes]\n",
    "    \n",
    "\n",
    "for i, (k, trainer) in enumerate(trainers.items()):\n",
    "    data_name = k.split(\"/\")[0]\n",
    "    dataset = datasets[data_name]\n",
    "    \n",
    "    X, Y = dataset.get_samples(n_samples=1, \n",
    "                               n_points=3*N_POINTS) # use higher density for plotting \n",
    "        \n",
    "    plot_posterior_samples_1d(X, Y, get_n_cntxt(N_CNTXT), trainer.module_, \n",
    "                             generator=dataset.generator,\n",
    "                             n_samples=10,\n",
    "                             train_min_max=dataset.min_max,\n",
    "                             is_plot_std=True,\n",
    "                             title=\"CCP Posterior Conditioned on {} Context Points : {}\".format(N_CNTXT, data_name.replace(\"_\", \" \")),\n",
    "                             ax=axes[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_CNTXT = 30\n",
    "\n",
    "n_datasets = len(datasets)\n",
    "fig, axes = plt.subplots(n_datasets, 1, figsize=(11, 5*n_datasets), sharex=True)\n",
    "if len(trainers) == 1: axes = [axes]\n",
    "    \n",
    "for i, (k, trainer) in enumerate(trainers.items()):\n",
    "    data_name = k.split(\"/\")[0]\n",
    "    dataset = datasets[data_name]\n",
    "    \n",
    "    X, Y = dataset.get_samples(n_samples=1, \n",
    "                               n_points=3*N_POINTS) # use higher density for plotting \n",
    "        \n",
    "    plot_posterior_samples_1d(X, Y, get_n_cntxt(N_CNTXT), trainer.module_, \n",
    "                             generator=dataset.generator,\n",
    "                             n_samples=10,\n",
    "                             train_min_max=dataset.min_max,\n",
    "                             is_plot_std=True,\n",
    "                             title=\"CCP Posterior Conditioned on {} Context Points : {}\".format(N_CNTXT, data_name.replace(\"_\", \" \")),\n",
    "                             ax=axes[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the predictions are very good\n",
    "\n",
    "**Good**:\n",
    "- the model predictions follow closely the GP (besides for periodic kernel)\n",
    "- converges to good results very fast\n",
    "- can extrapolate\n",
    "\n",
    "**Bad**:\n",
    "- not good at periodicity\n",
    "\n",
    "[Tutorial 2 - Neural Process]: Tutorial%202%20-%20Neural%20Process.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model is translation equivariant and thus does not differentiate between the interpolation and extrapolation setting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_CNTXT = 30\n",
    "\n",
    "n_datasets = len(datasets)\n",
    "fig, axes = plt.subplots(n_datasets, 1, figsize=(11, 5*n_datasets), sharex=True)\n",
    "if len(trainers) == 1: axes = [axes]\n",
    "    \n",
    "for i, (k, trainer) in enumerate(trainers.items()):\n",
    "    data_name = k.split(\"/\")[0]\n",
    "    dataset = datasets[data_name]\n",
    "    \n",
    "    trainer.module_.set_extrapolation(EXTRAPOLATION_RANGE)\n",
    "    \n",
    "    X, Y = dataset.get_samples(n_samples=1, \n",
    "                               n_points=3*N_POINTS,\n",
    "                               test_min_max=EXTRAPOLATION_RANGE) \n",
    "        \n",
    "    plot_posterior_samples_1d(X, Y, get_n_cntxt(N_CNTXT), trainer.module_, \n",
    "                             generator=dataset.generator,\n",
    "                             n_samples=10,\n",
    "                             train_min_max=dataset.min_max,\n",
    "                             is_plot_std=True,\n",
    "                             title=\"CCP Posterior Conditioned on {} Context Points : {}\".format(N_CNTXT, data_name.replace(\"_\", \" \")),\n",
    "                             ax=axes[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
