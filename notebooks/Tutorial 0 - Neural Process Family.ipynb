{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 0 : Neural Process Family Framework\n",
    "\n",
    "Last Update : 18 June 2020\n",
    "\n",
    "**Aim**: \n",
    "- Brief overview of the framework (notation + computational perspective) I will use to try unifying different models from the neural process family\n",
    "- introduce the datasets I will be using\n",
    "\n",
    "## Preliminaries\n",
    "\n",
    "### Notation\n",
    "\n",
    "Let us denote a stochastic process as\n",
    "$\\mathrm{\\mathbf{Y}} := \\{ \\mathrm{Y}^{(x)} \\}_{x \\in \\mathcal{X}}$. $\\mathcal{X}$ denotes an index set --- as it can be arbitrarily complex, we will usually refer to $x$ as a feature instead of indices ---, each random variable (r.v.) $\\mathrm{Y}^{(x)}$ take value in (t.v.i) $\\mathcal{Y}$ known as the state space.\n",
    "$\\mathrm{Y}$ is often thought of as a distribution over function, indeed sampling one outcome from each r.v. $\\mathbf{y}:= \\{ y^{(x)} \\}_{x \\in \\mathcal{X}} \\sim  \\mathrm{\\mathbf{Y}}$ gives rise to a function $f : \\mathcal{X} \\to \\mathcal{Y}$ defined by the mapping $x \\mapsto y^{(x)}$.\n",
    "$\\mathrm{Z}$ will be used for latent variables, $R$ will be used for temporary representations.\n",
    "\n",
    "A key concept is the posterior predictive with density denoted as $p( \\mathbf{y}_\\mathcal{T} | \\mathbf{y}_\\mathcal{C}; \\mathbf{x}_\\mathcal{C}, \\mathbf{x}_\\mathcal{T})$.\n",
    "Where $\\mathrm{\\mathbf{Y}}_{\\mathcal{T}} := \\big( \\mathrm{Y}^{(x^{(t)})}\\big)_{t=1}^T$ (resp.  $\\mathrm{\\mathbf{Y}}_{\\mathcal{C}}$ ) is the collection of target (resp.) r.v. with possible outcome $\\mathbf{y}_\\mathcal{T}$ (resp. $\\mathbf{y}_\\mathcal{C}$) for the associated target features $\\mathbf{x}_{\\mathcal{T}} := \\big( x^{(t)} \\big)_{t=1}^T$(resp. context features  $\\mathbf{x}_{\\mathcal{C}}$).\n",
    "For simplicity we will use the shorthand $\\mathrm{Y}^{(t)} := \\mathrm{Y}^{(x^{(t)})}$.\n",
    "\n",
    "### Usecases of Stochastic Processes\n",
    "\n",
    "\n",
    "We will be focusing on the following usecases of  stochastic processes can be useful:\n",
    "\n",
    "* **Sampling from the prior predictive** $\\mathbf{y}_{\\mathcal{T}} \\sim \\mathrm{Y}_{\\mathcal{T}}$.\n",
    "This could be used for generating new functions (e.g. data augmentation, image generation ...).\n",
    "* **Arg maximizing the posterior predictive** $\\mathbf{y}_\\mathcal{T}^* = \\arg \\max_{\\mathbf{y}_\\mathcal{T}'} p( \\mathbf{y}_\\mathcal{T}' | \\mathbf{y}_\\mathcal{C}; \\mathbf{x}_\\mathcal{C}, \\mathbf{x}_\\mathcal{T})$.\n",
    "This can be used for imputing missing values, or meta-learning.\n",
    "* **Sampling from the posterior predictive** $\\mathbf{y}_{\\mathcal{T}}  \\sim \\mathrm{Y}_{\\mathcal{T}} | \\mathrm{Y}_{\\mathcal{C}}$.\n",
    "This can be used for conditional generation .\n",
    "\n",
    "### Gaussian Processes\n",
    "\n",
    "We say that $\\mathrm{\\mathbf{Y}}$ is a GP if and only if any finite collection of r.v. $\\{ \\mathrm{Y}^{(x)} \\}_{x \\in \\mathcal{X}' \\subset \\mathcal{X}}$ has a multivariate normal distribution.\n",
    "This is especially interesting as each of the aforementioned usecases can be computed with closed form solutions.\n",
    "\n",
    "\n",
    "\\Cref{fig:GP_graphical_model} shows the probabilistic graphical model for a subset of r.v. from the GP (without plates : \\cref{fig:GP_graphical_full_rasmussen}).\n",
    "Note that predictions $y^{(t)}$ depend only on the outcome of the corresponding latent r.v. $z^{(t)}$.\n",
    "The density of the posterior predictive is:\n",
    "\n",
    "\\[  p( \\mathbf{y}_\\mathcal{T} | \\mathbf{y}_\\mathcal{C}; \\mathbf{x}_\\mathcal{C}, \\mathbf{x}_\\mathcal{T}) = \\int \\prod_{t=1}^{T} p( y^{(t)} | z^{(t)}) \\int  p( \\mathbf{z}_\\mathcal{C} | \\mathbf{y}_\\mathcal{C}; \\mathbf{x}_\\mathcal{C}) p( \\mathbf{z}_\\mathcal{T} | \\mathbf{z}_\\mathcal{C}; \\mathbf{x}_\\mathcal{C}, \\mathbf{x}_\\mathcal{T})   d  \\mathbf{z}_\\mathcal{C} d \\mathbf{z}_\\mathcal{T}   \\]\n",
    "\n",
    "Each term in the integral is a Gaussian density. \n",
    "Moreover, we normally assume that the prior has a constant mean and that observations have additive i.i.d Gaussian noise $y^{(x)} = z^{(x)} + \\epsilon $.\n",
    "In such case, the ``only important'' term is $p( \\mathbf{z}_\\mathcal{T} | \\mathbf{z}_\\mathcal{C}; \\mathbf{x}_\\mathcal{C}, \\mathbf{x}_\\mathcal{T})$, whose mean is a function of  $\\mathbf{z}_\\mathcal{C}$ but not its covariance.\n",
    "Both the mean and the covariance are a function of the GP's kernel evaluated at combinations of $(\\mathbf{x}_\\mathcal{C}, \\mathbf{x}_\\mathcal{T})$.\n",
    "Although the posterior predictive has a Gaussian density with mean and covaraince that can be computed in closed form, this requires inverting $K(\\mathbf{x}_\\mathcal{C},\\mathbf{x}_\\mathcal{C})$ which is $\\mathcal{O}(C^3)$.\n",
    "\n",
    "### Sparse Gaussian Processes\n",
    "\n",
    "## Neural Process Family to Model Stochastic Processes\n",
    "\n",
    "**Neural Processes Family (NPFs)** are a family of models which aim to model stochastic processes (distributions over functions) using neural networks.\n",
    "If you think of neural networks as a way of approximating a function, then you can think of NPFs as a way of modeling a distribution over functions conditioned on a certain set of points (which we will call *context* set).\n",
    "As a result, during training NPFs require a dataset of context sets of points instead of a set of points.\n",
    "The links with meta-learning become immediately clear, but NPFs can also be used for generation, filling missing values, active learning ...\n",
    "\n",
    "\n",
    ", which uses a set neural network to estimate the conditional predictive distribution $p(\\mathrm{Y}_\\mathcal{T} | \\mathrm{X}_\\mathcal{T}, \\mathcal{C})$ over a set of *target* values $\\mathrm{Y}_\\mathcal{T} := \\{\\mathbf{y}^{(t)}\\}_{t=1}^T$\n",
    "conditioned on a set of corresponding target features $\\mathrm{X}_\\mathcal{T} := \\{\\mathbf{x}^{(t)}\\}_{t=1}^T$ and a *context* set of feature-value pairs $\\mathcal{C} := \\{(\\mathbf{x}^{(c)}, \\mathbf{y}^{(c)})\\}_{c=1}^C$ . \n",
    "Such conditional predictive distribution is usually modeled using stochastic process such as [Gaussian Processes (GPs)](https://en.wikipedia.org/wiki/Gaussian_process).\n",
    "The idea of NPFs is to use neural networks instead, essentially trading off some [\"well-behavedness\" properties](#Lacking-Properties-of-NPFs) for  [computation ones](#Properties-of-NPFs).\n",
    "In this series of notebooks, I will try to describe NPFs under a unifying framework. From a computation perspective, the family of models have three main components:\n",
    "\n",
    "* The **encoder** $h_{\\pmb{\\theta}}$ takes as input a context point $(\\mathbf{x}^{(c)}, \\mathbf{y}^{(c)})$ and embeds it as $\\mathbf{r}_c \\in \\mathbb{R}^d$. Typically, it is modeled as a multi-layer perceptron (MLP).\n",
    "\n",
    "* The **aggregator** outputs a representation $\\mathbf{r}_\\mathcal{C}^{(t)} \\in \\mathbb{R}^d$ (or a distribution over $\\mathbb{R}^d$), given all context embeddings $\\{\\mathbf{r}_c\\}_{c=1}^C$ and a target features $\\mathbf{x}^{(t)}$. As the input is an unordered set, the aggregator should be *permutation invariant* in $\\mathcal{C}$.\n",
    "A simple example of an aggregator is to use the average $\\mathbf{r}_\\mathcal{C}^{(t)} = \\frac{1}{C} \\sum_{c=1}^C \\mathbf{r}_c$.\n",
    "\n",
    "* The **decoder** predicts a distribution over target values $\\mathbf{y}^{(t)}$, conditioned on $\\mathbf{r}_\\mathcal{C}^{(t)}$ and $\\mathbf{x}^{(t)}$. This is achieved through a neural network $g_{\\pmb\\theta}$, which outputs sufficient statistics $\\pmb\\phi^{(t)}$ of a Gaussian probability density function (PDF).\n",
    "\n",
    "### Formally\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "p(\\mathrm{Y}_\\mathcal{T} | \\mathrm{X}_\\mathcal{T}, \\mathcal{C}) \n",
    "&\\approx Q_{\\pmb\\theta}(\\mathrm{Y}_\\mathcal{T} | \\mathrm{X}_\\mathcal{T}, \\mathcal{C}) & \\text{Parametrization} \\label{eq::cnp_approx}\\tag{1} \\\\\n",
    "&= \\int p(z | \\mathcal{C}) \\prod_{t=1}^{T} Q_{\\pmb\\theta}(\\mathbf{y}^{(t)} | \\mathbf{x}^{(t)}, \\mathcal{C}, \\mathbf{z}) & \\text{Exchangeability Assumption} \\label{eq::cnp_factor}\\tag{2} \\\\\n",
    "&= \\prod_{t=1}^{T} Q_{\\pmb\\theta}(\\mathbf{y}^{(t)} | \\mathbf{x}^{(t)}, \\mathcal{C}) & \\text{Factorization Assumption}  \\label{eq::cnp_facto}\\tag{3} \\\\\n",
    "&= \\prod_{t=1}^{T} Q_{\\pmb\\theta}(\\mathbf{y}^{(t)} | \\mathbf{x}^{(t)}, \\mathbf{r}_\\mathcal{C}^{(t)}) & \\text{Sufficiency Assumption} \\label{eq::cnp_rc}\\tag{4} \\\\\n",
    "&= \\prod_{t=1}^{T} p(\\mathbf{y}^{(t)} | \\pmb\\phi^{(t)})\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\mathbf{r}_c\n",
    "&:= h_{\\pmb\\theta}(\\mathbf{x}^{(c)}, \\mathbf{y}^{(c)}) \\label{eq::cnp_def_rc}\n",
    "\\\\\n",
    "\\mathbf{r}_\\mathcal{C}^{(t)}\n",
    "&:= \\mathrm{Agg}\\left( \\mathbf{x}^{(t)}, \\{\\mathbf{r}_c\\}_{c=1}^{C} \\right) \\label{eq:cnp_def_agg}\\\\\n",
    "\\pmb\\phi^{(t)} \n",
    "&:= g_{\\pmb\\theta}(  \\mathbf{x}^{(t)},\\mathbf{r}_\\mathcal{C}^{(t)}) \\label{eq::cnp_def_phi} \\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "with $\\mathrm{Agg}$ being invariant to any permutations $\\pi$ : $\\mathrm{Agg}\\left( \\mathbf{x}^{(t)}, \\{\\mathbf{r}_c\\}_{c=1}^{C} \\right)=\\mathrm{Agg}\\left( \\mathbf{x}^{(t)}, \\pi\\left(\\{\\mathbf{r}_{c}\\}_{c=1}^{C} \\right)\\right)$\n",
    "\n",
    "\n",
    "### Properties\n",
    "\n",
    "#### Properties of NPFs\n",
    "\n",
    "Properties that GPs also have:\n",
    "* **Permutation Invariance in $\\mathcal{C}$** : $Q_{\\pmb\\theta}(\\mathrm{Y}_\\mathcal{T} | \\mathrm{X}_\\mathcal{T}, \\mathcal{C}) = Q_{\\pmb\\theta}(\\mathrm{Y}_\\mathcal{T} | \\mathrm{X}_\\mathcal{T}, \\pi(\\mathcal{C})), \\forall \\pi$. The predictive distribution does not depend on the ordering of the set of context points due to the permutation invariance of $\\mathrm{Agg}$.\n",
    "* **Permutation Equivariance in $(\\mathrm{Y}_\\mathcal{T}, \\ \\mathrm{X}_\\mathcal{T})$** : $Q_{\\pmb\\theta}(\\mathrm{Y}_\\mathcal{T} | \\mathrm{X}_\\mathcal{T}, \\mathcal{C}) = Q_{\\pmb\\theta}(\\pi(\\mathrm{Y}_\\mathcal{T}) | \\pi(\\mathrm{X}_\\mathcal{T}), \\mathcal{C}), \\forall \\pi$. The predictive distribution does not depend on the ordering of the set of target points due to the conditional independence assumption $\\mathbf{y}^{(t)} \\perp \\mathbf{y}^{(t')} | \\{\\mathbf{r}_\\mathcal{C}^{(t)}\\}_{t=1}^T, \\ \\forall t,t'$ made in \\ref{eq::cnp_factor}. This is also known as (finite) **exchangeability**.\n",
    "\n",
    "Properties that GPs do not have:\n",
    "* **Data Driven Expressivity**. NPs require specification of prior knowledge through neural network architectures ($h_{\\pmb\\theta}$, $g_{\\pmb\\theta}$) rather than a kernel function (like in GPs).\n",
    "The former is usually less restrictive due to its large amount of parameters and removing the need of satisfying certain mathematical properties. \n",
    "Intuitively, the NPs learn an \"implicit kernel function\" from the data.\n",
    "* **Test-Time Scalability**. Although the computational complexity depends on which $\\mathrm{Agg}$ is used, NPFs are more computationally efficient (at test time) than proper stochastic processes. This is essentially due to the sufficiency assumptions made in \\ref{eq::cnp_rc}. I.e. the assumption that the context set $\\mathcal{C}$ can be summarized by a fixed size representation $\\mathbf{r}_\\mathcal{C}^{(t)}$.\n",
    "For example, inference in (C)NPs is $\\mathcal{O}(T+C)$, in Attn(C)NPs it is $\\mathcal{O}(C(T+C))$, in Conv(C)NPs it is $\\mathcal{O}(N_{pseudo}(T+C))$, while in GPs it is $\\mathcal{O}((T+C)^3)$.\n",
    "\n",
    "#### Lacking Properties of NPFs\n",
    "\n",
    "Important issues that NPFs have (at varying degrees depending on which model) but GPs don't:\n",
    "\n",
    "\n",
    "* **Does not Satisfy Consistency Conditions**. Predictions are not guaranteed to be consistent, meaning that for $M<T$, the equality\n",
    "\\begin{equation}\n",
    "Q_{\\pmb\\theta}(\\{\\mathbf{y}^{(t)}\\}_{t=1}^M | \\mathrm{X}_\\mathcal{T}, \\mathbf{r}_\\mathcal{C}) \n",
    "= \\int Q_{\\pmb\\theta}(\\{\\mathbf{y}^{(t)}\\}_{t=1}^M, \\{\\mathbf{y}^{(t)}\\}_{t=M+1}^T | \\mathrm{X}_\\mathcal{T}, \\mathbf{r}_\\mathcal{C}) d \\mathbf{y}^{(M+1:T)}\n",
    "\\end{equation}\n",
    "is not guaranteed to hold.\n",
    "So NPFs are not proper stochastic processes.\n",
    "This essentially means that even if you had infinite computational power and sampled points autoregressively, the order in which you do it would change the sampled set.\n",
    "Putting aside the theoretical and conceptual issues, it is unclear to me how bad this is in practice (it might be).\n",
    "\n",
    "* **The Need of Large Data**. Learning requires collecting and training on a large dataset of target and context points sampled from different functions. I.e. a large dataset of datasets.\n",
    "\n",
    "* **Lack of Smoothness** Due to highly non linear behaviour of neural networks and the factorised form of the predictive distribution (\\ref{eq::cnp_factor}), the output tends to be non-smooth compared to a GP.\n",
    "\n",
    "### Training\n",
    "\n",
    "The parameters are estimated by minimizing the expectation of the negative conditional conditional log likelihood .\n",
    "\n",
    "$$\\mathrm{NL}\\mathcal{L}(\\pmb{\\theta}) := -\\mathbb{E}_{\\mathrm{X}_\\mathcal{T}} \\left[ \\mathbb{E}_{\\mathrm{Y}_\\mathcal{T}} \\left[ \\mathbb{E}_{\\mathcal{C}} \\left[ \\log Q_{\\pmb\\theta} \\left(\\mathrm{Y}_\\mathcal{T} | \\mathrm{X}_\\mathcal{T}, \\mathcal{C} \\right)\\right] \\right]\\right]$$\n",
    "\n",
    "In practice we optimize it using stochastic gradient descent:\n",
    "\n",
    "1. You sample a function $\\{(\\mathbf{x}^{(i)}, \\mathbf{y}^{(i)})\\}_{i} \\sim P$ (abusing notation : $f \\sim P$ ). E.g., a time serie, an image.\n",
    "2. You sample some target and context set $\\mathcal{T},\\mathcal{C} \\subset \\{(\\mathbf{x}^{(i)}, \\mathbf{y}^{(i)})\\}_{i}$ (abusing notation : $\\mathcal{T},\\mathcal{C} \\sim f^C$ ). E.g., set of values (time serie) or set of pixels (image).\n",
    "3. Compute the MC gradients of the negative (conditional) log likelihood $- \\nabla_{\\pmb \\theta} \\sum_{t=1}^T Q_{\\pmb\\theta}(\\mathbf{y}^{(t)} | \\mathbf{x}^{(t)}, \\mathcal{C})$ \n",
    "4. Backpropagate\n",
    "\n",
    "\n",
    "In practice, it means that training NPFs requires a dataset over datasets.\n",
    "\n",
    "\n",
    "### NPFs Members\n",
    "\n",
    "Let's quickly summarize some of the NPFs that we will be investigating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(False)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.0473,  0.4335, -0.0693],\n",
       "        [ 0.2483, -0.2876,  1.0110],\n",
       "        [-0.0888, -0.0102,  0.3140]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn(3, 3) * 0.5\n",
    "print(isin_range(x))\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Swallow:\n",
    "    @property\n",
    "    def airspeed(self):\n",
    "        \"\"\"Returns the airspeed (unladen)\"\"\"\n",
    "        return 1\n",
    "\n",
    "class AfricanSwallow:\n",
    "    airspeed = Swallow.airspeed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "af=AfricanSwallow()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.6475, 0.3834, 0.4815],\n",
       "         [0.0320, 0.1933, 0.6603]]])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t=torch.rand(1,2,3)\n",
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.6475, 0.3834, 0.4815],\n",
       "         [0.0320, 0.1933, 0.6603]]])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.expand(1,2,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "test(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def greeting(name: str) -> str:\n",
    "    return 'Hello ' + name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "can only concatenate str (not \"int\") to str",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-70f534de3be9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mgreeting\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-25-311aff60a9bd>\u001b[0m in \u001b[0;36mgreeting\u001b[0;34m(name)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mgreeting\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m'Hello '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: can only concatenate str (not \"int\") to str"
     ]
    }
   ],
   "source": [
    "greeting(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc-autonumbering": true,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
